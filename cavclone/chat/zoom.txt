You know. Do we know how to turn the spotlight off?
It's like Super bright. I did all these
love all the apps, all the modern apples.
I'll tell you this for my high school football. I drove. My wife drove. I was not feeling great, but we drove to an hour 15 to Leander
Friday. and then 2 and a half or 2 15 back.
So the good is we won that game console, whereas one of our opponents lost put us into a 3 way tie for second tie breakers gave us second meaning this Friday. The game is here in town instead of San Antonio. So
great. Great result for us. Great result.
3 h
3 h back. Yeah, that's rough. Okay.
Llnds. Continue it for us or for use
for us, for us, like always homework. 3. Right now, I believe we have about
maybe 35 submitted
much.
So some of you are still cooking a lot of activity on slack. Check it out, Alan. Super helpful in there. People comparing Alan is really putting in work. I told Alan just to be clear.
I said. You need to shut it down, I said. Anyone who's posting on slack over the weekend, I said, it's too late for them. Don't help. Alan's a cab I got to help.
So really, I was like, it's the weekend they haven't figured it out by now.
Shame on that spirit.
anyway. Get it done, don't you know? Don't blow it? Okay, there is. Gonna be a homework for like I mentioned. Yay, it will be like kind of final exam preparation. It will be like a handwritten one. It will not be a killer.
Okay, for the final covering. Some of these concepts we've talked about that you did not see on the midterm have not shown up on a homework, so kind of give you practice to get ready. Yay, whoop, etcetera. Project. You should be cooking. Yeah, we're cooking
a
you're kind of going downhill right now, like, and you're sort of picking up steam, and you can't stop, because soon after Thanksgiving, and we're gonna have a fun time and still working on details of how we're gonna handle the kind of demos that you know that kind of stuff. I'll kinda update that. And then, okay, any other issues worries folks
speak now.
infrastructure.
Terry Swift reference.
See that
any other comments we're good.
So if you're here, you just you come to class. I'm wondering. Are the people here like who here has already submitted their homework?
Have you already submitted your homework.
Seems like most of you interesting.
It's a correlation there.
I don't know what's going on, anyway. People up there in Zoom Land
get it done. Okay?
Also, II say that the point is like in Zoom land. Right now, there's like a couple of people respect. But, like my view counts of the videos.
I was like, Oh, yeah, you know, we have like 35 people in class. There's 90 people in the class. So my view counts must fill it in the view. Counts are like 5.
So anyway, I'm I'm speaking. I'm now speaking to people who aren't even ever going to hear these words, but I don't know. Okay.
thanks for sticking with this. Give it up to yourselves. Appreciate it. Little bit of a grind, a little bit of a different topic than normal right search recommendation. Now, we're doing all the time language models
like we, we kind of see like, why are we excited? We spend a lot of time on basic Ingram language model and perplexity. How do we measure the quality? Right? This week? We're gonna continue, we're gonna kinda like, turn it up a little bit, turn it up a little bit.
We're gonna get into prompting. We're gonna get into. How do we connect Llms and recommendation like, why are we doing all this hard work on Ln. And Llm's like, What's the whole point? So it's going to pay off hopefully, not today. But if I pay off on Wednesday, and we're going to finally square the circle. Okay?
Now remind you, we always have to sort of censor ourselves. Large language models. These are transformer based.
Both loads of parameters train on lots of data, can do amazing things.
And so, as always, you know, the fundamental of the language models is basic thing. We can calculate probabilities of sentences we can calculate probably is words that's kind of all. It is at its core over time. We've seen this development. So
we've done the Ingram models. You know this
right? Today, we'll talk about a simple neural language model. Okay?
Once we had the transformer. In 2018,
we had to see, you know, these pre-trained language models. Then we're gonna jump to, how do we scale those up to be really, really large. And so we're gonna do is we're gonna kind of do like a a speed run.
And we're gonna get to Lms today hopefully. And we're gonna talk about, how do we use them? Okay? And then depending on how things go. Maybe next week we may rewind and go back into some more details here. Unclear. Okay? So
when we think about a language model. large, small doesn't matter. We typically think we have lots of texts.
article by Texas, Dana. But, like all of Wikipedia, all of stack overflow, all of the web yadda, yadda billions and billions of
words.
we're gonna train a model. And then the result is, we're gonna end up with a model. Okay? And so I know, I keep talking about models. And so I want to be real clear about when I talk about model
what I mean.
what's eating there?
Speaking of fruits, assay
very good super fruit. Don't know it. Very good. Let me tell you what
you go in front of someone.
And you said, man, I had the best bowl, and this has happened. I've seen this happen in my life, and they scold you, and they say it's like Hi, Bro.
and you're like.
actually, it's not. It's Usai. Look it up. But you can't say that you just smile. Oh, did you guys eat those?
Sai? Obviously, A/C AI, obviously. okay. we we know how to train a model. We know how to train a Vibran language model.
So the literal training is just. We count words. We count migrants. That's training.
The model that we learn is division.
We take the 2 words, we divide by the one word. And so when we talk about a model, I literally mean, like.
this is the model that's the model.
It's just what is the probability of A. And M. Given the previous word was Texas with some numbers. What is the probability of flagship given? The previous word is the it's some number.
How do we get that? We just calculate by taking some counts dividing by counts? That's it. That was our sophisticated training. But that is creating a model here.
That is the model. Okay, we can do a trigram or a 3 Gram model. We count the triples and the doubles. And then instead. We count. We divide.
you know. So we get what's probably the university given the last 2 words, we're Texas, and we'll treat E. And M. As a word.
what does flags have? In the last few words were, became the.
And again, the model is just those, just the probabilities. That's the model.
Okay, that's it.
And we know, given the model, we can now generate text with the model.
Okay, just rewind the reward. Last time we can say if the last few words were today the
what is the probability of word given we just saw today the and feed words, these are all conditions on. I just saw today, though.
right? So I'm gonna keep track of every arrow word and the distribution of all words that come after it.
And then we talked about. Now I gotta flip a coin or roll a die, or generate a random number, and I pick one of these, so we call it sample one of wars
and notice like company and bank have higher probability. But through chance this time I happen to think
price right.
So I rolled my dye. Whatever I pick price. It's now, today's the price.
So now we condition on just the price. So again, I have to have
this set of probabilities
stored somewhere based on the the price. So again you're seeing. Oh, this is a lot we talked about this last time. So lots of sort of accounting we have to do here we sample, we can sample up
by chance. So today, the price of now is conditional on price of
price, of the price of 18 price of oil prices. We sample, we pick gold for whatever reason, today, the price of gold dot dot. Okay?
Ingram language model problems. Number one
storage. We got to store all this stuff
right, every word, everywhere that comes after it. All these conditional probabilities. Okay.
if we store longer Ingrams
more, if our
collection gets bigger.
The quote, unquote model gets bigger. Okay.
also know the quality. We only know about the past in words. Everything's conditioned on this. We saw this before. We looked at all these examples of generating text where it's like, it doesn't really make sense right
in Graham language model. There's also another one we didn't talk about which is sparsity. Okay, so check this out and zoom in a little bit for you.
We're trying to find what is the probability that the next word given. I just saw students opens there.
That means in my data. I need to have words. Students open their
blank. But if students open their
mouths, shoes open their box wallets.
open their books if it never occurred.
We got zeroes right?
This is a big problem.
There's tricks. And we're not gonna get into the details. But there's a trick which is like you could basically do what's called smoothing. And you sort of pretend every word could occur.
Okay, so there's little tricks people thought of many years ago.
So you would just sort of add in a little delta, you say? Well.
I never saw it. I'll just pretend it did for some very, very small fraction accounts. Okay.
another issue is like, yeah, on the division side. What if students open there never occurred.
It's like dividing by 0, right? And so again, over the years, people thought of lots of tricks. And so one is called back off, which is, you say students open, there never occurred. Well, let's consider open there. Maybe that did occur, and use whatever that is, or if that occurs back off to there
that's called back off. Okay? But again, these are like tricks people are trying to come up with to fix the fundamental problems of these Ingram Linux models.
Okay, now.
philosophical point here. this is also Ingram. Models are basically memorizing
bye.
So I said that we will be most often. And it's just like some problem, right? Like, really, like the deep model of what's happening.
So in my mind, like the integrand models are first memorized, these words happened, what comes next.
we do much different things. We calculate some probabilities done. Okay. But again, it's not really learning relationships. It's just counting some stuff and divide it.
Instead, we want to build models that really understand relationships. Right? And the idea is a model could be more compact.
So it's not gonna explode with the in or explode with the collection size.
and hope they'll be more robust. Let me give you an example. You may here play chess. You're familiar with the game of chess.
No, this seems more like a checkers crowd, but that's a joke. Come on, come on, Whoa! Whoa! It's a reaction now. Okay, alright, that was a joke.
Okay.
is a chessboard pieces. Memorization. We see why memorization is brittle. Memorization would be okay. The
help me, how do I move? Like, if you're starting from the beginning, what's the notation? Like? I move my pond forward.
E, 4 means that there was a pond here, and I moved it to there. Okay.
And then then Black has to go.
And black maybe says I don't know what they did in this case. Maybe they went E, 5.
So Black moved. E, 5. And then I went. D, 4. And then black did something. Okay.
Memorization. What I'm getting at is like. I literally just memorize sequences of moves orders right? And then chess. How many, how many
pieces we have? We have a lot of pieces that can do lots of different things.
And so the idea that I could memorize consecutive move orders.
And so, if I got to this position, what do I do next?
Well, I would have had to have memorized all of the move orders that got me here. which you know has this sort of exponential slow blow up! You can't do it.
and then I would for count and see what is my next move? Right? Because I had seen this in my training data before pure memorization.
Okay? As I mean, I'm not doing great job explaining. I'm hoping you're seeing this stuff doesn't really work. What is modeling in a chess perspective modeling to me would be like rules of thumb. What are some rules of thumb in chess?
Take center. Take the center. What's another rule of thumb.
We want to do that, develop pieces, develop pieces means get them off their home squares. you know. Take take free pieces, maybe.
What are some other rules
yeah. Get get king safe King safety
protect pieces. The point is, these are kind of like rules of thumb and and like ideas we have. So if I gave you like a brand new board.
You've never seen it before. The memorization approach is very brittle, and it basically says, I've never seen this before.
I could do maybe an Ingram model, which is what were the last 4 moves. Maybe I've seen some of the previous moves. but I lose the context of the whole board that memorized. whereas if I have a model, I have rules of thumb. If I look at the board.
I can now say things like, this piece is kind of stuck in the corner.
I probably want to get it out of the corner. So maybe I should move upon okay.
or this black King looks pretty safe. Okay.
maybe I want to say, well, this piece is kind of hanging out, like. you know, sort of hang out like all by himself. Maybe I can move it, or whatever. But the idea that I have rules that can guide my thinking.
Okay, does anyone know this this game? By the way, this is a real game. Any cheese any chess heads in here.
I don't know if that means this is Bobby Fisher's game of the century.
He's black in this case. Go go! Read about it. What
he spoke out in the states of our Asians
we don't see race in this class hyperspear.
Okay, all of this is to say, remember, we're building up. We go from statistical language model counting
to the models we're building models, but they're kind of very naive. We want to build models, that kind of learn something more robust. So they don't scale
with our data. Okay? And so, for example, we could take all of the text. Now we can use a neural network to train a model.
And so what is the model? It's just a bunch of parameters of the neural network. So when we did room, we did you think about auto? Rec, as an example.
you guys remember auto? Rec. What were the parameters of Autorec?
Here's aside
like autorec parameters.
remember, there was like there was like matrix.
Then there's another matrix. And there's a couple of bias. Vectors remember that.
So the the auto rec parameters are like, I don't know what they what we call like a WV matrix. some like v, one and V, 2 vectors.
Those are that's like, that's the model
in MF.
What were our model parameters in matrix vectorization?
P and Q, right, the P. Matrix and the Q matrix.
That was a model. So same deal when we talk about training a model.
it's just the parameters of those matrices vectors whatever. That's though, that is the model. Okay? And notice, it's not necessarily encoding
probabilities of specific words like in the Ingram model. Okay. so imagine we did this. Okay.
what's neat is.
people tried this in the early 2,000. Okay.
some. And I'm not gonna go into great detail. But you're gonna you're gonna know what's going on.
Here's the idea. Let's use a fixed window kind of like an Ingram model. Okay?
Okay?
And but the point is, he's not going to have to memorize all the observed ingredients.
It's not a memorization. So check it out. This is what happens here. This is very neat. Okay?
And again, I'm gonna emphasize what I need you to know about this like, for example, for the final. Okay.
so what I want you to see is this, the students open there the sequence of words of students open there. Okay, this would be like in our sequence of words we saw. And we're trying to figure out what would come next.
Okay.
So when you take the word we didn't get an embedding for each word.
Do we know about word embeddings. Little bit little bit. We did like word to vex. The point is, they're dense
in this case, like the is that word embedding students. Is that opened.
So the point is, there are some word and things.
Okay, we put them all together. We multiply them by W.
What's W. Sh! We don't care
is a matrix
I didn't
hmm! Just hang on. Hang on here.
Repeat this.
So we did this before kind of autoreck. So if you were here for auto. Rec.
now, we're we're doing kind of the basics of neural networks. What I'm trying to avoid as a class. Okay?
But I'm trying to get you to understand is like, Oh, okay. So there's a hidden layer which is just like A. W. Matrix times the embeddings plus a bias factor.
And autoreg. It was a little different. But it's something similar. And then you could have, for example, some activation function.
You need some hidden layer. Vector. H,
which is this thing.
Well, then, we just take that thing. multiply it by U, what's U? Well, again, it's the second layer of the neural network. We add on some bias, vector and we softmax it, which we know how to do.
And the idea to give us a distribution over all of our words.
So
auto was input
w
you.
So it's like a 2 layer neural network. Okay, so W is making a big thing smashing it down. And then we're standing in background. Okay?
And again, like, How do you really do this? Well, you know, we would do some kind of gradient dissent to learn. The best.
Yes, he wasn't
WUV. One and v. 2. I'm not asking you to know that. But I want you to, when you look at this to feel quite comfortable of like, yeah, you just take some words. You get their embedding. You multiply by some matrix, you add a bias. Vector, you do some kind of activation. You multiply that by some other matrix, a vector into a probability distribution.
And that's it.
So to me, like the mechanics, I understand the details are a little grey, but the mechanics are quite straightforward. We've done this already. Okay.
so what I want you to think about when you do this is notice. It's like the students opened their and it tries to learn what comes next.
The idea use a neural network to do this prediction. Okay.
now. you can replace that neural network with a different one.
And again, this is where you got to go. Take AI class. You got to go. Take Ml. Class. But, for example, you could train a different model like a recurrent neural network.
same idea, just different model structure. And again, all I want you to get out of this thing
is that in a recurrent neural network again looks fancy.
But I want you to understand. This key fact is, notice this. we're reading the words, Bob. this is just an embedding for the word Bob.
You know what that is. It's a dense factor. Okay.
protect this out.
We multiply by some matrix.
we did some hidden states. But notice the hidden State also learns from the previous in the State. So this is how you're kind of passing history. So, for example, by the time I get to this is open, there.
there is learning from this embedding of like, what do I output next? Is learning from the fact that I just saw the word there.
but I'm also inheriting some knowledge from the previous state which knew about opened, which inherited from students which inherited from the. So the idea is, I'm I'm passing forward the knowledge of all the things that came before me.
And so when you get to this last state and say, I now want to predict a word.
Notice what happened in this
this version. You kind of lost the word order in a sense. right? It's just like we can tap. Maybe there is sort of in order of the embeddings, but you can't just take it. You do some multiplication here.
It's basically saying, I now see the students open there. What's the next thing that you know? What's the next word that I could admit? What's the next thing that could come out? Well, you just said there, but I know you just said previously, and that may change the distribution of what I output.
Okay? So again, do you need to know all this?
Not really.
But what I do want you to just understand is like, Oh, this seems like a more sophisticated model, but somehow is modeling history. So this is the key to me. It models history.
When we get into and we'll we're gonna fast forward. Through this we'd be like a transformer. These other kind of fancy models
they can consider of both sides
the right answer, notice. It's all left to right.
because the way we speak language is like
words come after one another. but from the modeling perspective you could also consider the words that came after it to guess the word in the middle cool. Okay? Again, big takeaway. Here is just a more sophisticated model considers history.
and then other examples of generating text, this is like you train over Harry Potter. You get text like this.
The mouth always, said Hermione. Harriet was watching him. He looked like mad, and Maxine, and she strode up the wrong staircase to visit themselves.
I mean again. It kind of looks by Harry Potter
timed it if you want to. This is trained over recipes. These are called country dip cookies.
You need 2 pounds potatoes, one. Yeah, it's good. You're not gonna make this.
Why not?
These are called country. See, the joke here is this wasn't generated by an Rna. This is from my family recipe.
Okay, we also did character level. And I'm just putting this in here. So it's in the slides we did this last time.
This is the same deal we did Shakespeare. We did Wikipedia. We did geometry. We did code. We did baby names. Okay.
all of that
got dark.
All of that is to say, is, that's like that original idea of using neural networks. That's early. 2 thousands. Okay.
you notice the name on that. That was Bingio. That's part of the deep learning crew.
So you also understand the history here, which is neural networks, were basically dead
right.
like they were kind of hot. Then they died off. And there was these kind of nuts still working on them when everyone else thought they were. It was stupid. And then they were right, you know. So then it all came back.
Okay, okay.
early, 2,000 s. We now get into the 2010 s. We saw word to Vec, it was like, Oh, yeah, that was like kind of like a 2 layer, simple neural network.
We see. Auto. Rec this kind of stuff.
But what you start seeing in terms of this language modeling is, people start replacing.
You know the simple Rnns with fancier ones, and so check out.
this is a Facebook reporting. This is around, I think, 2016. Yeah. they're measuring perplexity and saying, we can do an Ingram model. Notice, they're using 5 grams.
Okay? Or we can use different variations of phone if you just want to evaluate it
fancier R. And is called Lsdm's. you know. So fancy will be your model architecture.
And then whatever Facebook proposed and basically they're saying is like Ingram model perplexity.
You know you could do some bad voices. But notice, it goes way down
by choosing these neural, network-based models. So the idea is these are models that are better able to explain the text that we see. So you see, errors dropping again. This is only 2016
circle, that 2016. This is quite recent. Okay? But again, this is all pre-transformer.
Okay. cool. This stuff seems to be working.
Transformer hits 2018.
Replace. Rnn replace Lstm with a different kind of model architecture. But the same idea you give me a lot of.
And I have some transformer based model. What is the output? All these model parameters. Okay.
this led to Bert. I show you this spawn hurt and a whole muppet family.
There was Ernie.
They're Elmo. I think Elmo actually predates.
That's a teamwa. This is a vibrant.
this actually bringing in
what this is. Actually.
this is a hundred percent real.
Elmo el el Elmo predates. But all these papers, all these talks around this time everyone has to have a picture of Bert.
And that was like that that was like, cute
your background.
Yeah, it's we love birds. We love our needs.
This goes crazy. Okay, we're skipping over this right now. There's also Gpt
Openai. What I want you to see here is again.
notice, fancy, fancy neural network. We're not talking about that transformer in here. This is Gpt, this is 28 team.
Okay, the architectures are different.
This is like a decoder model with maybe come back to that stuff next week. Okay. But for now turning for 2,018 is like
we had
basic neural network models. Then we move to R and ends. They got fancier and fancier, then transformer hits. We all switch over to this transformer world. Then you have kind of the Burt world happening. And you have the Gpt world happening.
Okay. fast forward to today.
We have Openai. There's Var. Meta has llama open source. Stanford has alpaca.
which is like another kind of open source version and dot dot dot many other super super big models. Okay?
So what I want to get into now is as we went from
Gpt Burt to today.
What are the cool things that are happening? And how are they gonna inform our search and recommendation.
He concerns Western words.
Can you explain? One
this, 30 or 40 will become 20 will become 10.
Okay.
so Gp, 2 comes out 2019. Now, it's gonna get fun. You're gonna enjoy this part. Now, you got here with me. Okay.
this is Openai
language models are unsupervised multitask learners. What does that mean? You start to see people talking about quote emergent abilities.
This is the exciting bit
I put it in quotes, because I don't know how I feel about all this, but the idea is, the models can do things beyond how we thought we kind of designed them.
So g, vt, 2, basically is just much larger. Then GPT. One more parameters. Okay.
trained on lots more data. They also have some quality signals. So this is kind of funny. They have, for example, links posted on on Reddit with at least real votes, is like a proxy of human quality, dubious, to say the least.
Okay, one of the key key points they make here is this ability to do what's called 0 shot learning. So this is a big big idea oops.
Welcome back.
It's the ability to do some tasks with no examples.
and they emphasize no gradient updates. And so just just freeze it for a second. So what I'm talking about here is basically
training these big models. We see training in Signal's parameters. It's like you did matrix factorization. You learned your P. And Q.
Now you freeze it. You don't change the engine. They're frozen.
Now. You can use that model to do stuff right? You can guess ratings and blah blah blah.
But then we said, Well, what if there's like a new user?
Well, we might have to go in there and update all the model parameters to get the new user. So that would be great and update for changing the parameters. What they're trying to say is, you know, what is it my like? P. And Q for matrix factorization? I could say, Hey, matrix factorization? What's the you know, who's who's the President of the United States.
And matrix recognition was like, the Yeah, I don't know, but like it tries to answer us right? We've froze the model. We can ask him questions and it can answer us.
We don't have to change it. And what the quality is
this is what's happening here. When we say there's no gradient updates, you can just ask the thing something and it can do it.
So 0 shot a shot is a example.
So example, we train a model on pictures of animals and text descriptions.
The training data has forces that know zebras.
Okay? So it has never seen a picture of a zebra at test time. I basically say, can you identify a zebra from a picture
that would be 0 shot learning? It's never seen a zebra, but it kind of learns from the from the text like, Oh, zebras have stripes. I've seen horses. I can do a thing I was never trained to do. That's cool, right? So
here are
the most confident answers from GPT. 2,
from questions that they never saw in their training data.
So the point is, you can say who wrote the book, The Origin of Species, Darwin correct, who's the founder of Ubuntu, project our shuttle warriors?
These are all questions the language, model and theory has never seen before, but it's able to answer them. Notice.
it does make mistakes
right? There are X's in here, but the point is for lots of questions. It had never seen before. It is able to answer them.
This we're like, Oh, this is like amazing.
We're looking
largest state of the Us. By land mass, but not California. It is
very good.
Alaska. Where have you lived your whole life?
You think Texas is better than Singapore?
Oh.
anyone who's been to Singapore is like. It's like the greatest place they've ever been.
It's like a city of the future or something
I've never been.
One year later we get GPT. 3.
Let's see some more stuff it can do.
Here is Gpt. Generated Text. This is the article humans had the greatest difficulty distinguishing from a human written article. and just notice it compared to the Rnn. Stuff we saw before.
The post knows that the denomination is claiming 12.5 million members, literally considering the largest possible the deuce split will be.
It's like
so at the bottom relief that is up at the top. Unlike Ingram.
it makes sense, people think it looks legit.
So this is like holy Moly.
The model quality is really good. This is in 2020.
So
emergent ability, they're coming out of Gpt. 3. Again, parameter size must be mobile, 5 billion to 175 billion.
much bigger training data.
Okay, it can do what's called fue shot learning.
So this is cool instead of like just asking it things it doesn't know about. What if I gave it an example of a thing, and it can learn just from my example to do the thing.
Okay, sometimes you'll see this call in context learning again, there's no gradient updates. I'm just asking the model.
So you may have seen this kind of stuff. 0 shot would be this
model translate. I tell it, translate English to French. And I say, cheese.
Okay, this is 0 shot. What is the output I want here?
Fromage guys fromage. Oh, you wanted me.
Okay, this is 0 shot. One shot would be. Translate English and French to you.
Here's an example. Sea Otter. Here's a correct outlet.
Now, if you cheat and it's a fromage.
you understand, when I say, one shot is, I literally mean, like, that's the one example.
We can also do few shot. which is.
I give you multiple examples. Translate English to French. Here are 3 examples of English, the French. Now I give you cheese model. Do it for me.
Okay. And again they keep emphasizing this bit, which is notice. No gradient updates like the model is frozen.
We're just interacting with it through English.
does it work?
So let's look at this. Let's try to understand this figure. This is the number of examples to give it.
Okay.
this is their few stock model. Here is a fine tuned vert model we know about Bert a little bit. Here's human.
and here's some fine tuned state of the art model. So
that that is 0 shot.
I get a no example, I ask you to do a task, and it's better than guessing
metadata. I give it one example.
I give it one example. I get a big improvement. and I get a 2. Let's get 3, 4, 5.
I'm sorry. That's not the 16. Sorry
I give it 16 examples.
You know we're beating some fine tuned, Bert, but there's still a big gap here right big gap.
It was kind of exciting that, like
it had never seen the thing, and it does pretty well over a model that had been fine tuned like the model gradients. Sorry the gradient updates have been applied to the model parameters.
Okay.
but here's the here's the the real selling point.
Get 0 shot. I give you no examples. One shot. I give you one few shot. In this case I give you 32.
This is why people like kind of start losing their minds around this time is notice.
notice this bit.
Wait a minute. As I scaled the model up.
It's doing better.
And it seems to be like, well, what if I had going?
Could I be human just because the model is so big.
Also notice a really big 0 thought model
is better than a like smaller
model that has, like lots of examples. So again, compare that to like those 2.
So the model is way way bigger. But it's able to do stuff.
Smaller models can't do
with more and more kind of task specific evidence. So the dream again. That's why I drew it like, this is like, if you kind of.
you know.
could we? Could we beat human if we just have a big enough model, or is it maybe going to
go off? I don't know
but you can see the excitement of Wait a minute if we just train bigger and bigger models. We can really do a lot of awesome stuff. This is why people are kind of going like, Whoa, this is crazy. Okay.
so this is what we call prompting. I think everyone, this class is probably a world-class, prompt engineer.
That right.
your degree should say, I have a bachelor in science.
You've gone from
you. Think anyone here has written an essay in the last year? No.
it gives you the essay back, and you say
down the back room, pretend your
a model that determines if this is written by Chat gpt or not rewrite this essay. so they don't pass such a detector. Good. But chat gpt, your intro could be better.
I need a few more examples for my thesis paragraph in the middle.
Do better jet gpt.
so you guys already, I presume, no prompting. It's just asking the language model things.
Remind yourself when we have no examples. In the prompt
0. We give one example, we call it one shot, several. We call it few shot.
So while you have all been writing your own prompts. there's a whole literature trying to understand prompt good.
So stay with me.
We know. Prompting sometimes doesn't work. You know this from again, from your own experience. Right? I'd try it sometimes too hard. Sometimes there's multi-step reasoning. and if you find it like, if you try this on Openai, our attention, it'll work
it like historically, like sort of if you give it examples of math, you face right because it doesn't really understand what's going on.
So this is what like blows. My mind
was the idea of changing the prompt to what's called chain of thought prompting
this is this is when we entered to me a pure Star Trek land. Okay.
chain of thought that says 23. But it came out in 22 at least. Like an updated version.
Okay.
chain of thought. Yeah.
Standard prompting.
Roger has 5 min falls, he buys 2 more Pms. Me. Shannon has 3 tennis balls. How many tennis balls that we have? The answer is, load. So this is I give the model example. Right? So this is
like a one shot.
I give it an example. Then I ask a new question.
They have screws up
to me. Okay, as Bot says.
same
original input but now they get the logic. Rogers started with 5, 2 cans has 6, 5 plus 6 and 11. The answer is 11. So the idea here is.
give the reasoning.
So then, when I ask this question, the model outputs has to. Then reason
it doesn't just give you an answer. It says, the category had 23 apples. They used it down. They had this minus this. The answer is 9.
That's how they fix the 17
essentially Khan Academy. It was using a version of theirs to build their own thing, and they built a unique model for Khan Academy that had to say, hey, think through what you're about to say. Do it in like several steps, and that's what fixed math that they moved out to check it. So you see what I mean by this feels like I feel like we're back in the what was it? The dark ages like alchemy.
Right? I have lead. I want it to be gold. So what do I do?
right. I like. I mix some stuff in there. I turn the heat up.
But watch what happens. Same thing here is like these incantations. Model.
try it this way. And suddenly it works. There's a question here.
So this is right. So if I don't know about you
computer science people were just like, I have algorithm, I have, input, I have output makes sense I can have randomization, but I know what the output is going to look like.
And now, all of a sudden you say, Hey, chat, Gp, rethink yourself.
check yourself.
And it's like, Oh, you're actually right.
I got it. Okay.
this is called China thought prompted.
Just this idea hugely impactful. Let me show you all these examples they give. These are like templates.
They say, if we're gonna do a math word problem, make sure you write out the steps
before you give the example? The answer here.
if you have like a yes or no, when a pair of sinking water
you don't just say, though you say, well, the density is this, the density is that blah blah blah!
If you have a question like Sammy wanted to go to where the people were, where might he go? This is common sense, the reason.
So you tell them it must be a place of lots of people racetracks.
desert but unpopulated.
I maybe I disagree with this a little bit? Yeah, Mike, right?
Is the following.
okay, now, this is actually a soccer player.
N, no. So the idea these are sort of templates you can generate
as your shots. Okay.
now, here's here's again why people are again losing their minds.
So what I'm trying to show you here is, 3 different language models.
These are all different map map problem collections. Okay.
the supervised best model
versus. So in other words, a model that knows about all these questions, and is trained on them versus just a language model. and you have standard prompting and chain of thought
and kind of the argument here is is, check this out, which is notice as models get bigger
as the models get bigger.
You see this big jump. And again, you see these big gaps right?
Not always, sometimes quite small.
But the idea is one. There tends some of these cases. You have a big gap from using chain of thought.
Okay. But as the models get bigger.
success goes through the roof.
So what's the answer? The answer is, just build bigger models. That's why. And this is why
everyone is just like more Gpus, more machine, more data, bigger model.
Because maybe it'll shoot through.
Yeah.
very exciting.
Didn't JD. 4
regular.
We're done. We'll see you all Wednesday.
This is exciting. I don't know about you
that you know.

It's working.
Okay?
Moment.
we're good
related by people.
I gotta do a quick introduction.
Former student, I mean Pushman class of great it 2019 computer science major. So yeah, so I'm a HD, I research with cab
from 2,017 to 2019 to 3 bit of our 3 years. I went from here, started my career as a software engineer. Capital one. And now the company called Self. We we're startup and credit building and we recently hit a a a billion dollar valuation for a uniform startup. So that's little bit cool thing to be a part of
my biggest chance in all my opportunities really came from. Let's hear it. Let's hear it from from.
So
yeah, life is different. And our basic wasn't as popular doing great. So we're glad to have you back. Houston. Yup, pretty cool. So maybe this is a little message to you guys.
5 years from now you come back.
I'll get you a T-shirt. We're gonna have T-shirts this year. Still working on it. I'm still working on it. We'll see how the T-shirts go.
All right. Gang. Yeah. So anyway, I am dragging.
Got my, got my shots. My arm really hurts. Didn't sleep. Great.
I really. I'd like to go home and take a nap right now. You, too. I hate to talk about football. We have a game this weekend where we going. Oh, mess, we're gonna win.
Not bad. He's tough. Right handed.
I was having these dreams about embeddings seriously.
I really was.
and I was trying to unify these 2 different embedding spaces and so sleep. I gotta get some more golf.
Yeah, I was in California last week, so I played some golf.
Alright gang questions, comments, concerns homework homework. 3 is due this weekend.
this weekend project.
Anything else. We good
check out slack lot of good activity on there. Please do your homework. etc., etc. Okay, here we are gang. We were talking about models. Basic definition. One is a large model small models.
We said, basically, I wanna know what is the probability of a sentence occurring? And or what is the probability, like the next word given the words that I've spoken previously. If we could do that as a language model
we were talking about last time was very simple model. We said, we're gonna estimate the Ingram probabilities. Right? So we said, Look.
we're gonna take. What is the probability of a word given? I just observed a word.
We're just going to count all the sentences that have other words. and then we're going to buy by the word before
right? So we did this example last time where we said.
we have sort of sentence tag to open the sentence. I am Sam, Sam. I am. This is how we're gonna estimate those probabilities. If we can do that.
we have a Bygram language model. And so we said, the probability of I given S.
Samuel Smith
I'll take that.
Somebody's talking.
Am I talking.
I'm gonna mute. Everybody.
Okay. yeah. The probability of I given S is
2.
But we have 3 s's. So that was the count of.
So there's 3 of those.
and then how many times do we see? S. Then I
2 of those. And then we said. we have 3 s's. Then how many times do we see Sam? One. We have 3 eyes.
and then we see am 2 times due. One time, dot, I thought we fill it in. But the point is, once we do this.
this is a language model. This is a simple diagram language model. Okay.
pretty sick.
More examples. This is from a bunch of sentences talking about restaurants in Berkeley. Can you tell me any good Cantonese restaurants?
Tell me about all these kinds of sentences. So you have like 9,000 sentences. You can go count all of the vigram probabilities. This word, then that word. Okay. So we'd read this, basically, like.
if I say, I,
sometimes I say I again 5 times I would have the eye here. This like
I miss Pipe I miss spoke, I stuttered. III but let's see if you can say well, want
I want. We kind of kind of follow the most kind of like
kind of math, like, I want to
eat Chinese food lunch spin. Okay? But as you can count all of these right
from these 9,000 sentences.
and then you can go get the raw diagram probabilities. So the idea is like, if I just said I,
I'm very. There's many more words I'm not showing want right cause we see want given. I just said I is quite probable. whereas typically I don't say I 2.
You know, this is 0 right here all those options. This is going to be a problem that we're gonna have to deal with a little bit later on.
Okay? So that is, you can take any collection of documents. count all the diagrams or trigrams. Right now, we're just doing like this. Given that.
And we got a bunch of words, we got a bunch of probabilities. So from the example. There's also some other ones you needed
which are not in the, in the, in the table. But given all of these
raw diagram probabilities, we can now figure out what is the probability of a sentence.
You're gonna see why that's important in just a second. So, for example, the sentences, I want English food.
We need to know what is the probability of. I given the sentence. We're only conditioning on the previous thing. And what was that called when we only condition on the previous thing
dark.
and the the covid for mark off
wherever you are. There you are, man.
you are so deep it is deep
something only previous.
But yeah, if you don't consider the whole history. So I don't consider if I'm trying to figure out what is the probability of food given. I want English. I don't go all the way back infinitely to figure out that. But if I go back, one word is background. Mark on something like 2 words, 3 words, etc. But the idea is, you block it. You only consider a fixed history. Okay, great point.
But the point here is, we now have a sentence. We can go look up in that previous table.
and we can find all the probabilities at the end of the day. We say, what is the probability of that sentence? Point 00 O. 3. One.
Okay. Well, why is that interesting? What if we don't say I want English food. I want Chinese food.
Okay.
same deal. But if you look closely you'll notice
point 0 0 6 5 is way higher than 0 1 1.
Right.
So the probability that I say want English is much less than want Chinese.
And so when we do this whole thing out, we get point 00190,
much higher. What does that tell us?
Who's in the English food? Are you mad? Right? Right? But like the idea is just in terms of the probabilities. This is way more probable because people are more likely to want Chinese food. Okay.
point is, these are now language models. We can do by gram estimates of sentence probabilities. So I want you to pull back. And just look at these Bygram probabilities. And I was sort of arguing last time that there's not a lot in here right?
But if you look at this, what do you learn about English and language?
Do you learn any structure here. What kinds of knowledge is embedded in these probabilities here?
Grammatical stuff, nouns and verbs and stuff so like if I say
to eat. it's quite fun.
So it's telling me, like verbs right to eat, to run to whatever. So I'm kind of learning about.
But this is telling me something about or to want or want as I want to. So I'm kind of learning about verbs. I'm learning about grammar.
I'm learning about kind of relative popularity.
Want English want Chinese want Chinese? What?
How about what does this one mean?
2 food given? I just said 2
lobbying dramatically to food.
Private account.
I wouldn't say 2 food right? I wouldn't have that now, right weird, you know. No noun after my preposition.
And then, you see, like this one, most of the sentences start with, I
a lot 25%.
So my point here is, even if you just look at these some random numbers, they're actually inventing a lot of structure just in like pairs of words about like, okay, nouns, verbs, grammar
propositions.
something like this one and spend, or both verbs. So typically we don't go verb verge. Right? So this is like 2 verbs.
Why would you say, spend, want? Okay?
Okay.
Another key thing here, practical issues, everything we've been doing, we're multiplying these probabilities.
And so if you go back here, imagine, we added in a few more words, this number can be really small. Yeah. the longer sentence, it's going to be point. Okay. And so you can end up having
under flow, you can kind of have big big problems. So typically we're gonna do is we're gonna log everything. And it becomes addition.
So you remember, this log of multiplication is just summation of logs. Remember that. Yep.
so example.
So here's the point, though, if I log these things the same thing, I want English food. Now I'm just logging and adding, so it's again the log of this thing, plus the log of this thing, infinity and negative number negative 4.5 one. If you compare it to Chinese food, it's negative, 3.7 2. So Chinese food is higher. Okay, it's all good login.
Second.
alright.
And
you want it to be higher, as high as possible. Yeah, yeah. okay.
I'm dragging
yeah. Hey? The Rangers? One world series, your Rangers guy, are you from you? From Houston?
Where are my Houston people at
you guys got yours recently, right? I heard supposedly like in the locker room, someone said.
we're going to state
for something like this.
Is that true? I heard some something like on the recording of the of the Ranger locker room.
It's like we're going to stay, Alice.
Was that a big deal, or need you guys like in band or choir football or sports, they were going to state, yeah, what was your instrument drumline?
Okay?
Last time we talked about Google Ingram release. Has anyone played with that? Or you guys just watched me do it, and it was like fun. But no one will look that again.
You should do it. It's fun. It's kinda cool. Yeah. Go check out the Ingram release. We talked about this last time.
super, super, cool, super helpful. When this came out in 2,006.
I bought the. They mailed me Dvds of the Ingram, so I think in my office somewhere. I have Dvds
of the Ingrams, because it was that was much faster than like trying to download them.
Think about that. Okay.
Example that showed this to you last time. But the idea is now we have lots of these sentences serve as the. So now, if we wanted to figure out like, what is the
what is the probability of like incoming given
serve as the
we would just say, Oh, it's, you know, 92
over whatever that you know. Wherever the total, not the total number of times we ever seen this. But the idea, the individual
indicator independence, is like by far the most right. So you could get a sense of like language. And what matters the most? Okay, I showed you this last time. Go play. This is fun.
It's fun. You can go learn about history.
You guys don't care about history. We only care about the future.
And listening. If you don't know your past.
Okay. how do we evaluate a language model. So right now, we've learned some estimates of probabilities. Okay.
remember, we did this before in our ir world and in our recommender world. Okay, we're basically.
we're trying to figure out like, do we have good parameters? Do we have good probabilities? Okay? So typically we're going to test a model on data. We've never seen. Right? So remember, this is our setup. We did this before, like in our MF. World. All this stuff, Tessa, is totally unseen, separate from our training set.
we all run. We all agree on some evaluation metrics. And now we can say what were the best parameters
that give me the best results. Okay. so classically, you can do what's called extrinsic evaluation.
We talked about this in the context of recommendation and search as well. Right? So extrinsic is basically like, go put the models to to work for us on some downstream task.
So I have language model A, you have language model B,
go use them for self correction, autocomplete, etc., etc. system A and system. B, see which one performs the best.
That's the better language model. Okay.
we know how to do the Savi boom.
for example, how many misspelled words were corrected.
how many words were translated correctly, and then we can compare the accuracy. Okay. In practice. We don't do this a lot. Why don't we do this a lot?
Because there's a lot of different
ways to tell accuracy. There's not a whole lot more ways to tell accuracy.
We can do this. Why would we not want to do this? This works? We can measure accuracy. It tells us how well it works on the downstream application.
Oh, nominates.
it's always it's always like 2 answers to everything. It's expensive.
We're slow. This is, give me both
right? So the idea is like this can be expensive, and it can be slow.
because I've got to go like, do all of this stuff.
I got a good deploy to real like users. Okay, go back to cabs algorithms. You know, we're going to go test all of our users on this stuff.
Maybe not.
Okay. Tabs algorithms is the name of our record store that we've been building all semester algorithms. You get it. You get it pretty good, pretty good. Who came up with that name. By the way, thank you.
It all comes back also. We have a hyper sphere of trust in this class. I was trying to explain that to my girlfriend.
says the call guy, yeah, exactly. Exactly. Just like the Aggie spirit.
Okay?
Instead of we're gonna do intrinsic evaluation. Okay?
The idea here is something easy, measurable. I can measure it really fast. I don't need end users. I don't have to do AV test.
Okay, we have to always remind ourselves that this intrinsic evaluation is kind of a bad approximation. We always care about our real users. We care about our downstream tasks.
Okay, so keep that in mind that sometimes we use these sheep
fast intrinsic evaluations because they give us quick results. They help us think about the problems we're facing. So in practice, we're gonna use something called perplexity. I'll give you some examples. Okay.
yeah, your perplexity is trying to do.
Let's think about how well we predict predict the next word. I always order pizza with cheese and
rice. I always order pizza with cheese and pepperoni pepperoni. I always wear pizza with cheese and
bell peppers. The 33 President, Us. Was Pepperoni.
I saw.
Okay, so the idea is.
and the
we could try to use unogram language model. Remember, we only guess a single word. Why would be, why would a unogram model be terrible at this task?
The whole rest of the context doesn't know any context.
Sorry assign probability to them.
And here we basically now randomly sample from that.
And so we would be picking more probable words. But your point is right, we're not considering it. The context, we would just say, the
okay. And
you know, all those popular words have a lot of probability. Mass is very likely to be fit soon. Terribly okay. So instead, like, we want to like, somehow, model sentences do a better job. Okay?
And so perplexity is, gonna allow us to measure sort of the goodness with which we make our guesses. Okay, so here's the formula. And I'm gonna do example to make it clear for you. Okay.
the best language model is one that best predicts an unseen test set with.
So here's the highest probability of a sentence. Right? So here's the definition. Perplexity is the inverse probability of the test set normalized by the number of words.
This is perplexity of a sentence. It's the end root of the inverse of the probability of the words we observed in the sentence.
if you see that you're just like I don't know what that means.
What is that all about? I'll show you some examples. Make it clear.
It's perplexing.
The idea is you want our perplexity to be small. Okay, let me show you an example.
We have 4 recipes, chicken, butter, pears, chicken, butter, linen, chicken, lemon pears.
I don't recommend these.
so I was in the Msc Wednesday. Wednesday. I want to vote
money for the schools.
But when I was in there in the Msc.
I saw Chick-fil-a has kiosks. Yeah. Yeah.
And the line there was like 1,000 people.
but the line went all the way down that hallway
I stand. You don't.
However, the web page is a little
okay. We have 4 recipes. This is not a language model. These are just recipes. So let's do a language model. Let's call it model A. This would be very simple.
and we'll just assume every word has equal probability. 161, 6, 1, 6. This is our model. Okay.
cool. This is a model language model.
Given our recipes, given our model, we can now calculate the perplexity of these sentences
using our model.
So in this case, you'll notice, it's like that first edges is 1, 6,
one over, 1, 6, one over, 1, 6 times, one over, 1 6 times, one over, 1 6. That's the first sentence.
the second sentence. one over the probability of chickens one sixth.
All the way down to the twelfth route. You do that. You get 6.
This is the perplexity.
Okay, that's the perplexity. Remember, lower is better.
right? We have a perplexity of 6, as it tells us how well this language model did at basically guessing the actual sentences we have.
It's cool. I like extremes. So let's take a different model. Call this model B.
This one has the probability of chicken is 95. Every other word is only one.
Okay, yes.
So let's go. Do the perplexity of this model.
How do we do that?
The same? What is it
chicken, butter, pears, chicken, butter. So what is the probability? What is one over? The probability of chicken
one over point 9 5
times butter one over point 0 1 pairs I wanted over point one.
So just super super clear. That's chicken.
chicken, chicken, chicken, chicken.
chicken, chicken.
You guys have you guys seen the chicken paper 2 and a 2. Do you know about the chicken paper?
Do you guys know about the chicken paper? Talk?
Yeah.
Oh, my God! Our next speaker is Doug Zanker.
Doug wrote an article published in the Annals not too long.
But you're gonna have. I know, I feel. Although I asked why? I asked. Doug
presented himself.
The aye.
the whole the whole talk is. He just says, chicken, chicken, chicken, chicken, chicken, butter, pears, chicken, butter, lemon. But notice here in this case, because we're thinking chicken is so super probable.
and everything else is so rare. When we do see these non chicken words, we get super penalized, we have super high perplexity.
So the idea is, this is a worse model. Then our one sixth model. according to perplexity. So it turns out, there's a third way.
The third way is, let's actually train the model based on the actual probabilities of words in the data set itself.
And so in this case you would say, Well, there's 12 words. 3 of them are chicken.
so our best guess of chicken would be 3, 12 or one fourth.
This is called the maximum likelihood estimate. This is basically given the data. What's the like best guess? Given the data we have now, it may turn out that's not the right guess, but it's the best one we can do, considering the 4 recipes.
Similarly, we said, what is the probability of shrimp one out of the 12.
So our probability of shrimp will be 1 12. It's cool. So this is model C, this is derived from the actual words we have.
Okay, guess what if we tried this in our complexity
Cablermo?
It's smaller than 6. It's the best one. Okay? And you can work this out on your own.
So here are 3 models. the one that's actually trained on the actual words we have gives us the lowest perplexity and would be our best model. Okay
is the best one.
Cool. It's easy, right? That's perplexity.
That's how we're going to do simple evaluation of language model. And you may think, well, this cab. This is only for silly little baby language model.
You go reading all these papers now about Bard p. 5. Chetchypt somewhere in there is going to be a perplexity number.
You're still measuring stuff, even with fancy models.
Okay?
Oh, man.
we do.
Whew! Having a hard time here, folks.
okay, thanks. Appreciate that when you're in the hypersphere of trust, you feed off the energy for everyone else in the hypersphere. You should remember that. Remember that, too, when you're having a downtime.
Get on, get on slack and be hypersphere. Show out. I need you right now for real, hey? I'm serious. If you guys ever have any issues
get me on. I think if you were ever in trouble
I'm serious. Real trouble. If it was Saturday, 4 in the morning
and you needed someone hit me on slack, I will be there. How's this? Who? Anywhere?
What is the division level here? That is the apple.
You say. You know, something bad's happening. Stupid stuff is happening. You need help. You don't know who to call. That's this.
Your Bros have let you down
try this day. So we're trying to finish your homework. Another example. This is Wall Street journal
training over millions of words, different kinds of models. We can measure the perplexity you notice more sophisticated model trigram is this, lower perplexity is better.
Okay? So typically you'll see these kinds of things happening as you drive this
plus perplexity. Now.
we talked a little bit about this last time generating text. Right? So now, like, we have a language model. Okay, we have
our Lem. Remember, all that means is, we have, like, you know.
word given word like this is a diagram.
Lm.
or it could be probably a word, you know, given
word
word. This would be trigram. But the point is, we have some language model. Okay.
Now, given that language model where we talked about last time was, now I want to go, use it to go generate new sentences. Okay, this is very like,
but in the mind, when we think about chat, Gp.
etc., etc. And so we said that you know, we could basically define. Like. you know, I said, it's like from 0 to one.
And you know. this is not really 2, 3, 4.
Is that right? 12345678910. That's 10 cool.
We don't really do it like this. But this is the idea of how we would sample from this distribution. Right? And so the idea would be. There are different words. We roll this, like many cited die. And we start picking words to generate. Okay, so examples of that like, if you consider that Wall Street journal.
the top one is a unogram model that generated a sentence. So if you look at this like months of my end issue of year foreign, these are just words that show up in the Wall Street Journal. But again, notice, there's no connection between words.
The diagram, though. Now you start to see connections between words last December
through the way to preserve the Hudson Corporation
in the Ec. Taylor in the scene. To complete the major central planners, it kind of
it's not really doesn't make sense, but it's kind of something's happening.
Now go to the Tribram model. They also point to 99.6 billion dollars from 204063% of the rates of interest stores as Mexico and Brazil on market conditions. I mean.
it kind of smells a little bit like language a little bit, almost almost
same deal with. This is Shakespeare.
Now you have to say it like you're you're you're performing a Shakespearean play
of the watch.
Will you not tell me who I am? He's a noble leopard.
Okay? So
when we think about
when you think about like Chet Gbt, okay. it's model is more sophisticated. But it's still doing the same thing.
Architectures that the ideas are going to understand these long distance relationships. But you're not actually just counting
strings of words. Okay.
we're gonna get there. It's gonna be cool.
I think I have one more of these. Okay, so actually
wanted to show you this the bad exit?
Where can I get this for you?
state journalism?
Tell you what? Let me. I think we'll look at this. So
it's basically
would say
so where we're headed. Just so, you know, is like, right now, you have the basics of a language model. We're doing what's called like those 1990 S era, statistical language model, like, literally just count words. Okay, we know. And we in the 2010 s, we have this big, like deep learning revolution.
This is a very famous law in 2,015. In fact, Andre Carpathy. He just tweeted about this yesterday took a photo of it.
he said. It's highly amusing historical Quirk, that I was very excited about language models. In 2015
he started. He was one of the founders of Openai. He says, we started Openai a few months later. The thought hadn't crossed my mind to work on them. I was very interested in reinforcement learning lol side.
So
this guy was thinking about generating text using a more sophisticated art architecture. And Rnn, okay, so we're gonna maybe talk about Rnn is like, imagine like a baby version of transformer which we know powers, modern language model. But I just wanna show you here is
when he uses R. And N. Now a more sophisticated model to generate text. He's he's doing what's called a character like leveling language model. So he's literally generating every character, not by words. Right now we have probability of words.
He's not doing that. He's saying like, I sample a T,
an h and E, and a space. I generated the word love.
Okay, so this character level. But he's using a more sophisticated architecture to do it. I just want to show you
just the results. And next week we'll come back and try to understand how some of the stuff works.
Okay, so check it out. So these are character level. So if you know Paul Graham, he's like a a startup like Guy, who talks a lot about like how to be a good startup guy.
so this is a bit of degenerated character level.
the surprise investors. We're going to raise money. I'm not the company with the time. There are all interesting quickly. Don't have to get off the same programmers.
But what I want you to understand is like.
this is character levels, not word level is learning like starts and sentences.
Learning, like quotes. is learning what references to generate all of this stuff.
So then, he tries Shakespeare.
This is his Rnn. Generating Shakespeare. And notice again, it's learning
like as all cast is like a speaker. the model. No one told the model that it's just saying I believe this is should come here right.
Alas! I think he shall be calm approach in the day when little strain would be attained into never being, into being, never fed, and who is but a chain and subjects of his death. I should not sleep.
That kind of sounds like Shakespeare. Right?
Okay, let's train it on Shakespeare.
Oh, it's not gonna make sense, because Shakespeare doesn't make sense.
be all. I'll drink it. You also get something longer
again. This is all character. Love it. Why, Salisbury must find his flesh and thought that which I am not apps, not a man and his fire. Okay, so he keeps going. But check this out.
He then said, I'm gonna go try Wikipedia.
And so notice, I want to see in this. If you know markup or markdown, it's basically generating the markdown
like this is how you you put in like links and stuff. So it's learning that.
And again, so it looks kind of like a Wikipedia article.
Naturalism and decisions in the majority of Arab countries, capital times grounded by the Irish language. By John Claire.
I mean, it kind of sounds reasonable.
Okay? It says, even it can generate this kind of like, header stuff. Code. Okay.
yeah. So how about code? Right? So then, he's like
they did algebraic geometry.
And they got to generate latex. So if you go look at this kind of stuff, this is all it's generating.
Hence we can find a quote subset H. And H. It means that F on XU is a closed immersion of S and U to T in separate algebraic space proof that that dot. So again, like, just superficially, I think we all agree. This doesn't really make sense.
but from kind of like 10 feet away, it's like that looks kind of like.
But be right.
yeah, generates. Limos
wasn't.
And it says, proof omitted. I'm not gonna show you the proof.
Yeah, yeah. that's the last of usability.
Okay? And then to come down. So then you guys know all about using Chat Gp for code generation, this is again 2,050. He took some some he got to generate some
some C code, right?
And again, this is all a character based. Rnn language generator. And again, if you look at this. It kind of looks kind of looks alright. This is 2,015.
Oh, you don't like his code. Why is there go 2 here
you can see the in insufficiency of our original ingrained model. Right? It doesn't have any intelligence. What this is doing, though, is in the Rnn. It. Basically, you have these model parameters that help guide the generation. So you're trying to learn something, not just output
from a random sample of these word probabilities. Okay, so what the Rnn is doing is it basically is maintaining some history.
And it's also considering the current state. So it's basically saying, Where am I now
in the sentence, okay, it has some history about where I was. and all of that is encoded in these model parameters that it's using to figure out what character to generate next.
Okay, so we're gonna come back to this next week. So it should be a little fuzzy for you. But the idea is, we're moving away from simply, randomly generating to sort of imposing some kind of structure
which is encoded in the model parameters of the R. And M. Which is helping decide what to generate. That's what's happening. Okay, that's why this is so cool. And so again, this is 2015. I think if you go down.
This is occasionally right. Model will recite the like. The new license character by character. Right? So there's like, Here's the general public license, all this stuff.
but it knows how to generate this stuff. It has all the standard includes here, which like.
And then he gets into more code. Right? Oh, yeah, he did one with baby names.
Yeah, these these are baby names, I thought was kind of fun.
Is it
coffee? Say, sales? Sales?
at least yeah, we find our team robuster.
and then also
go on headline Casina.
Some of some of you were named after this. It's like your parents got this from from.
But if you look at those I mean again. Those names look kind of like names.
Marlise. Johnny.
Oh, my God! If you look at this, he said, some of the other baby needs generated were Baby
Kelly.
And so, just to give a little more insight. So back to like the model kind of learning structure. So again, our our everything we talked about so far, there's no structure. Again, we count words. We just generate probabilities.
This he's going to train this model.
And so he's trying to show you here is like, what does the model look like as you train it?
And so it's showing you here in the beginning. This is character level.
right? It's just for random characters, because the model has no knowledge. It doesn't know anything.
because by 300 iter iterations. You start to see some quotes.
So we're like here. It's starting to kind of form a little bit better by 500.
It's like by 700 iterations, 700 iterations that I mean is, it's being fed in text. And it's learning from real text and the model trying to understand what does real text look like.
So by whatever by 1,200, it starts to actually look pretty good.
And by iteration 2,000, you know, it's why I do what that day we plan to talk to.
So the idea is the model. The beginning is totally nonsense. But over time it's more sophisticated as you train it. As so my thought would be. This is again we're trying to kind of balance like, because again, we've not. I'm not assuming you've taken a neural nets class. Okay?
But the idea is like, I wanna kind of next week, get into a little bit of like, how does this stuff really work
to sort of give us a sort of a sketch of how this transformer works to really do like chat, gpt style. Okay, just to see like from where we went, where we started out, which was like Count Ingrams, generate to R. And N. 8 years ago. Doing this to today.
You can see the like sort of the progression. Right?
Okay? So we'll try to get a little bit more into the details of all this stuff, and then we'll look at how we can incorporate this into cahps algorithms. Okay?
Because that's what people are doing right now, how do you take these Llms and improve your search. Improve your recommendation. Okay.
cool. Okay. Gang we'll see you all on Monday. Have a great weekend.
Don't forget.

Second
on
cool
is the A.
No. Oh.
howdy. hey? Nice to see you, friends! Welcome back through waivers.
breaks you off.
Anything happening in the world. You got yours.
You might stay up last night and watch the spurs play.
I did. I want to see this guy, Victor, one by me, Winbunyama. Did you see his Halloween costume?
He was the slender man. It's very creepy. Find the video, he, this guy's like 7 foot 6 inches 7 5 extremely rail thin, and he's wearing like the white like body suit, but and then wearing like a dark suit super creepy.
Anyway, wanna check that guy out French superstar. If you don't know. He's like 18 or 19, I think 19. He's the future.
and that was your sports Update in the weather.
It's chilly
in class language. Notice the word that's missing there. We'll get large later. We're gonna start old school large as always, questions, concerns, comments.
homework 3. I'm seeing a lot more activity on slack good. Someone's doing something. Yeah.
Oh, yeah. Oh, wow!
Make sure you're on it.
projects. I think we asked you some question, how's the project going? Yeah. No one no one has started.
Yeah.
it is what it is. You're grown-ups title
a little bit. You'll figure it out. Nothing else. We're good.
Okay, cool. Last time. Lolms exclamation, wow! Exciting! Exciting
kind of trying to hype it up. And then we ended by talking about basically kind of going back to our kind of original ideas, which were, we've done a lot of what I call kind of surface level text representations. Vector, space, even like word to Vec, is not like super sophisticated and trying to do like, if we're running a query do match.
Or we're doing recommendation. You can do like text match and recommendation space. But the idea, like very superficial level, where it's basically matching words, or even in the embedding space is kind of like a fairly like, not sophisticated model. Okay?
So then, we try to say, like, how would you actually help people
using just those surface level. We said, well, the Pre machine learning world
would be. We build some kind of system, kind of bake in some rules. And so we talked about this. Ask Msr. Where they're like. Well, we'll query a search engine which has lots of repetition, and then we'll do some like Ingram mining, and then we'll do all these little tricks, and we'll hopefully get a result. Okay? And the issue, of course, is like, it's very brittle.
And you can imagine those kinds of techniques screw up. And so you would see that at least in the last few years, like, if you went into Google and you searched for, like, you know.
occasionally, you know, they would basically put in those little info boxes at the top. And occasionally people would post these funny ones cause someone asked for something.
and some like some sort of naive model of the web within, like a post. You have an example, something like dinosaurs. And this kind of stuff. Anyway. The idea is like this kind of style has been around for a long, long time, and what we're trying to get to is more of this modern like Ml. World, Llm. World.
where we can do very like deep representations. And really I don't want to call it understand, but have much deeper representations and modeling. So we can actually answer questions. So that's what we're trying to get to. Okay.
fundamentally, when I think about this.
we have kind of like our traditional
search and recommendation. And then we have kind of this Llm world.
And the way I've kind of been thinking about it is basically, like, you know, we have our users here each. That's kind of a sad looking user. But the idea is, typically, we have, like
all of this stuff
right? Those could be web pages.
It could be albums.
And basically, you know, II can't draw the gear in between. But the idea is typically like our models that live in between
are kind of not sophisticated in that they don't really understand the stuff they're just trying to bridge the user and the stuff.
And so in our traditional search is like what we index some words. And then we do like we build a positional index, or we do some ranking with cosign. And that's all we do or in recommendation. Maybe we have some like ratings data, and we do some kind of matrix factorization at the end of the day. We don't really understand the stuff.
The Llm world, right is basically
you take all the stuff.
You then basically, you know, you train your Llm. So it learns
kind of learns about the world.
So then, when we have the user, come along
and they they talk to it. The Lm. Has already learned about the world. Okay, so it's kind of like philosophically, what's going on. And so if you think about it in terms of you know, we talk about those parameters.
and I showed you that slide last time, like how many parameters are there in this plan, or model? Or Gpt. 3. And it's like billions and billions of parameters. Right?
The way to think about that is like in our traditional world, like how many parameters are in your like, vector space, retrieval world.
how many parameters do you learn?
Like kind of nothing, you know, maybe one or 2.
Okay, how about in our makes factorization role. Maybe you learn some parameters right? The idea. But there's not much there.
But in this new world the idea of these Lms have, like billions and billions of parameters that kind of encode, all that knowledge, the big transformation.
in any event.
along the way we've been talking about language models. So I gave you this definition last time, said Lms. Or transformer based language models. And they have all these characteristics, and they're trained in certain ways.
But kind of the thing we haven't really talked about right is like that very important phrase, like language model.
We sort of assume we know what a language model is. So today, we're gonna kind of go back to basics and just build up what is the language model? How can we begin to build a language model towards understanding large ones.
So we're gonna do our background. We're gonna go back in time. Okay. way to think about it is today, we're gonna kind of cover.
Traditional kind of statistical language model have started in the 90 S. All the way into the 2,000 s. People still use this stuff. And then what you saw again, deep learning, revolution.
We kind of saw the beginning of like kind of word to vec. It's like right after this
kind of word to Vec and others.
Then we're gonna get these pre-trained language models. This is gonna be like Bert.
And then today, we have kind of what we call the Llms which are scaling up these big, pre-trained language models. This is kind of like the history. So today we're gonna do is basically try to understand classic language models like, what are they? How do they work? How good are they?
Okay, then we're going to elide a little bit of this to get to our big big language models, our large ones.
So here's our definition.
Okay? And these slides and the other readings are called often. Dendraski was like kind of one of the top Nlp researchers
in the world. So what is the language model?
Keep using this work so well. Look.
imagine you have a sentence or a sequence of words. It's just the probability of the sequence of words. Word one word, 2, word, 3, word, 4. Or
you might consider. If you remember your conditional probability, what is the probability of word? 5 given? I just observe word 1, 2, 3, 4.
Can I sort of do the next word and put an estimate on that.
Okay, we'll do some examples in a second. Make this concrete. But the idea is that a model that can either tell you like, what is the overall probability like, here is the next sentence. I'm gonna say, everyone in your mind.
In fact, let's do. Let's do the next word, okay? And that's everyone make in your mental model. Right now.
I'm going to get to a point where you're gonna have to guess the next word. Okay?
So and then we're going. I'm gonna ask you what those with that word is. Okay. Okay?
Everyone. Don't forget. Your homework is due
point blank. I just okay, do. Okay. So now.
where that word was to use
and your homework is due in. I heard Sunday any other guesses soon
any other guesses tomorrow
within the week. What did I actually say? I think I said.
Duh, okay.
But it's the idea of like, Hey, guys, don't forget your homework is due
banana. Your homework is due running.
your homework is due. So there's some words in your mind, or less likely to appear in some words that are more likely to appear. Okay. So we talked about what is the probability of an upcoming word? A language model allows us to make an S. And the point is, there are many possible words that could be next, which is why we need probabilities. We're not just saying the next word is
some. It wasn't just Sunday.
There's lots of possible words I could have said next. It's like a probability distribution of all words. Some are more likely. Some are less likely.
Yeah, that's that's basically a language model. Okay?
So if you scale this up and you think about large language models.
you know, basically, they have models of the world that are super super rich
and can do not just sort of make sure, can guess and sort of generate the next word, but like lots and lots of words in a row that are kind of sensible and can convince us that something cool is happening.
Okay? So we're gonna do some basic probabilities. So how do we compute the probability of a sequence of words?
Right? This is a joint probability. So the sense it's going to be. It's water is so transparent that
dot dot dot
your homework is due. Don't forget, guys, your homework is due next Sunday.
so we have a sequence. So we want to figure out, what is the probability of that full sequence? Okay? And so we're going to go use the chain rule.
It's really simple. You're gonna if you forgot this from high school, it'll come back.
Okay. You guys remember conditional probabilities
and a little bit
somewhat a little bit. So the probability of B given A is A and B divided by probably a
more like base rule. And all this stuff. Okay? So the probability of A and B is probably an A and then B given that A has happened.
So let's do an example like
If the events, you know I don't want to do coin toss, because so boring right?
But it's like the probability of if I if I go heads heads.
it's probably of heads on the first one, and then heads so heads on the first heads, on the second. Is it probably if heads on the first heads on the second given, I gave heads on the first. Well, these
you'll recollect, these don't. The the coin tosses don't matter. They don't depend on each other.
So the problem is that I go heads heads is what
1? 4. So the probability of heads is a half.
and what's the probability of heads given? I just flipped the heads.
Well, if you're in Vegas, you're like, what matters? It doesn't matter right? So it's a half times a half.
because they're independent. So it's just a quarter.
But let's change that and say, instead of like, that's like an independent example.
It's like. What is the probability? Like the aggies win
heroes.
Okay? And then no. And I need another event. That also happens.
That depends on the aggies winning
and the aggies go to a bowl.
Right? So you can say what is the probability the aggies win?
And what is the probability that the aggies
go to a bowl given? We win
right? So there's some probability that the agencies win this next game.
60%, 50%, 30%. We don't know.
What is it? Probably that the aggies go to a bowl given that they win is that higher than if the aggies go to a bowl given, they don't win. Yeah.
So the idea is, okay, this is sort of the idea they're sort of conditioned on each other.
Now, the point here is, if you have lots of variables, what's it called? If a happen and B and C and D, it's basically well, if a happens.
and then probably the B given A happens. C, given A and B have happened. D, given A, B and C have happened, or, more generally, you can write it out like this.
Okay? And so hopefully, you kind of remember this from high school. The point is when we think about
words, what is the probability of us observing? You know my name is Joe.
Your homework is due Sunday. Okay.
we're just going to multiply out, probably. Each word given the words before it in this example, it's water is so transparent.
It's a probability that I say it's probably that's a water given. I just said, it's the problem that I say is given. I just said, it's water.
The problem is, it's so good. I just said, it's water is, and so on and so forth. Right? Probably a transparent given. I just said, it's water is. So
this cool ish.
it's cool. This is cool. Okay. Now.
now, my question to you is, we want to build a model. We want to build our first language model that can estimate those probabilities. So we can build our own
Gp.
so one idea.
if we had a big collection of sentences.
we can go counter.
What is the probability that I see the word Bob given, I just said it's water is so transparent that
I would go count every sentence that has. It's water is so transparent that the
and divide over all the sentences that begin. It's water is so transparent that.
or in the exam, like just gave earlier. You take all the sentences which are, Hey, guys, don't forget your homework is June.
That would be the denominator you kind of all those sentences, and the numerator would be
if you're trying to get. Sunday
was probably that I'm gonna say, Sunday. Given. That would be all the sentences are, hey, guys, don't forget your homework is due Sunday.
So let's do that.
So I mean, I guess that's why, whenever you're like typing stuff on like messages, you'll see like those like 3 suggested words after extension. So all of that like, exactly like, it's any kind of like,
kind of like query, suggestion or autocomplete is based on language model.
Okay? And you can see some are more sophisticated. Some are more personalized than others. Right? So I'm typing out an email. And it's like, Hey guys, don't forget your homework is due
next week, tomorrow, Sunday. But they knew more about me. If I know. Sunday, yeah. Auto complete is definitely like a very like tactical application of Microsoft.
My question to you is this. if we want to estimate those probabilities, can we do this? Do we do this?
It seems reasonable. We just count it. And we divide
the problem. That's different
getting a bunch of probabilities. It's not actually.
let me ask you this, where did
I need to count how many times I've seen this phrase.
So imagine I had.
you know every file on your on your device. How many times does this break occurred.
probably 0. So instead, imagine I went through all of Wikipedia. How many times has this phrase occurred on Wikipedia? Probably very little teeny. Maybe
maybe 0, maybe once.
So then how do I find? How many times did the fall of this thing? Well, never, ever happen.
You get me.
This is the hard thing about language is, we can say things so many different ways
that we don't have this like this nice set of like all the possible sentences that we can go count the words and do it. Yeah.
when you were to just start typing the sentence. So let's say you didn't get to water just like, yes.
hold! This is a great question. Hold this thought.
My first point is this, can we just count
no too many possible sentences?
We don't have the data. These sentences don't occur right, like.
you know. Just imagine. you know, how many times is this sentence.
you know, like on Wikipedia
or on the web.
you know, 0 kind of close to 0.
We just don't have it right. But to your point, which is.
imagine we were saying. what is the probability of water given its? I've just said, it's what do you think the problem of water is?
It's quite low, right? But also, what is the probability of Joe Joe given his.
What is the probability of
banana? I had a banana for breakfast, so it's
what is the probability of Sunday given its right. So it's water. It's Joe, it's banana. It's Sunday. something.
Do you think you could calculate these probabilities?
Using the same little trick? Let's find every sentence. Let's count.
It's
water
divided by the count of it's that's the probability of water
given its.
Can we do that?
Yeah, I think so. Right. That happens a lot. and we can do it for all the other words. So which you've kind of gotten to, which is, I agree. This is a very small number. but we could calculate
we could estimate these probabilities, if we only considered, like the word right before it.
that make sense. These longer chains of sentences are harder to model. So
okay, so what you're getting at is the Markov assumption. Do you wanna know the Markov assumption?
I like to describe it like this. The Markov rule.
The Markov rule is this.
wherever you are.
there you are, man.
What what I mean to say is this.
here
I don't know.
That's it. That's Mark. What I need is this.
when we're trying to figure out, what am I gonna imagine this? What am I gonna do next.
The idea of Markov assumptions basically don't consider everything I've done my whole life
all my actions to decide my next action.
No, the Markov assumption says, basically use a very limited history, or even know history, or like one history. And basically say, the fact that I'm standing here. What I'm gonna do next is totally divorce from everything behind me, everything I've ever done
for this moment.
What could I do? What was my next thing? I'm gonna do?
What you've got here in class. You're teaching a class you're probably gonna stay on.
Is that
same deal? Oh, I'm reaching my wallet or going money into the ground. Yeah.
can't go into bush.
And I think that was well.
unlikely.
The Markov assumptions, when I say, But
wherever you are, that's where you are, is, the the next thing you do is kinda independent of what you've done before the long history of you. Okay. So, for example, a simplifying assumption would be what's probably given. I've just said it's water is so transparent that
ignore all those other words I said and say, what is the probability of the given. I just said that
you know they only consider the one word where I am right now to make my next decision.
or maybe the last choose. I have a little bit of history. What's probably the given. It's water so transparent that well, it's just the same as thing with the probably the given. I've just said transparent that, and I forget
it's water in his study
wherever you are. That's where you are. You only consider, like your immediate circumstance. You don't consider your whole history.
This is the Markov assumption.
This comes up a lot. It makes life. There's some. Okay.
yeah.
let's put that in math.
You love it, if only that works at the top.
What is it? Probably all these words in a row, hey, guys, don't forget your I'm working zoom Sunday.
Oh, it's the probability of each word given some little calendar
window behind it.
Okay.
so imagine I think of an example here.
Yeah. So imagine like,
you know, you said like, if it was just one.
it would be saying like, I'm only gonna consider the one behind me
or the 2 behind me.
Okay. So, for example. what if we considered no history?
So what is the probability that you know? Hey, guys, don't forget your homework is Sunday.
this, probably, of hey? Probably, guys, hey? Don't
forget. There's no connection from word to word.
If we were going to generate sentences using unogram, there's no connection between the words
you would be like if I spoke.
I never consider the history. Notice. There's no given. I just said something.
So, for example. we have think about our vocabulary.
Okay, how big is our vocabulary?
It's large.
Okay. Now imagine every word you've ever spoken.
How many words have you spoken in your life, you think at least 3. So you've surprised you've uttered millions and millions of words
that was good, though that'd be strange.
It's good. You guys are good.
So if we were going to sample from your brain
a word. So we're gonna flip a coin
like a million sided coin that's weighted by how often the words you said, what are some likely words you might say? This is a class. This be cool. Yo.
Now II sample another word. Farmer. Hi, gay.
sitting.
transcendental
but existential movie
hypersphere. Okay.
so notice these. These are sentences.
They don't mean anything because I was randomly generating words. It's called unogram model. You already know ungrams. Right? So we also know by gram
vigram says.
remember, I don't consider my whole history. Just consider the word I just said.
right? So this is like, you know, whole history.
This is just the last word.
Wherever you are.
wherever you are.
there you are. man.
what am I gonna say next, I don't care about where I was before.
It's the next word boom, boom! So notice in this case.
this is again a diagram model. And so the hope is you're going to see a little bit of connection between pairs of words. So the first word was sexico.
Let's it, generated Rose. Mexico. Okay, Rose. 1 one in kind of makes sense. In this makes sense. This issue kind of locally issue is is pursuing.
But each pair kind of makes some sense pursuing growth growth in in a boiler with their house.
But when you put it all together, nonsense.
right? Yeah.
Very good question which is like this is a good point.
Let me
me add a add a page. So let's think about it like this.
We have our ho! Vocabulary.
I need some vocabulary. Words
hyper, scared
swimming.
Here let me see
A and and
the zebra
to add gig
a vocation. So imagine we have our 100,000 words. Okay, this is our vocabulary.
And so of those 100,000 words we go into like a collection like those lyrics datasets. Okay? And so so suppose in our lyrics data set.
we see these
100,000 words. let's see, we see 500,000 times.
So we have 5,000 500,000 words that occur. So Taylor can happen. Multiple times all happens multiple times. Okay.
so what you can do is you can go for a. You can sort
count the number of times a
over 500,000.
How many times did we count hypersphere divided by 500 1,000 and vocation? For whatever reason why I do that
vocation
divided by 500,000,
this is going to give me some probability, some 0 point 0 0. I don't know 0 1 0 point 0 0 0 0 1,
you know.dot.so the idea is.
we have a probability of all these things, and if we add them all up.
it sums to one you cool with that.
So what I'm saying in the unogram model
is, we now imagine we have all of these 100,000 words
we generate a random number to pick one according to its probability.
Let me make it ultra simple. So let's look at a simple vocab of 4 words.
Okay.
cow, thank you.
Swept. Thank you. And
like relationships.
Like, is your wireless dreams coming through.
Yeah, it's funny. That's wild and food.
Okay, imagine we had some sentences, and I counted these. And we said, the probability of Calv is like.
probably of each of these words, let's say it's like
adds up to one cool. With this we only have 4 words. They have some probability associated with them. just like these are probabilities I found on the 100,000. Okay, so I'm saying now is to generate a word.
So now we're gonna do is we're gonna generate a sentence.
generate a sentence
from our unogram model.
Okay, so we're gonna flip a coin, or we're gonna generate a random number. So you can imagine we take.
I mean one way to think about it is like this.
can I? Can I draw an exact?
That's just selection. How do I draw like a
there's like a ruler. This is too much. I'm gonna do it like this.
Think about like this. That's like 5.
Well, this is fun for me. By the way.
oh, this is, this is really fun. By the way.
what's your other color? Orange?
Okay, beautiful.
So we're gonna do is we? Basically, you know, you can imagine this is from 0 to one. This is I laid out all of my words in terms of order
from 0 to one. And now I can just generate a random number. So let's let's one generate
a rand
between 0 and one. So let's say it's point 3 6, 2.
So I go. 1, 2, 3, that's right there. Oh, Swift.
swift is the first word I generate.
Okay, does that make sense?
I generate another one.
And this time, what's my number?
0 point 0 0 1? Do, do, do, do I generate pink? That's Cav that's here. I generate another one 0 point 0
430,
I generate Cav again.
and on and on and on. So the sentence I generated is now swift path path.
Okay, that's my sentence, using a very dumb unogram model where I generate words.
So in the case where we have 100,000 words, you can imagine something similar where it's 0 to one, and I've subdivided it to the 100,000 times, and assigned the probabilities to each word. In there, flip a coin or roll a random number generator and pick the word that comes out
is that cool? Because again, there's no relationship. The fact that I generated Tab had no idea that I just done swift. It just said, Pick it, go, pick it, go
cool.
This is really good question. This is how we're gonna do. Generation. so what's happening here is when you see Texaco.
Texaco was being generated from like, you know. Imagine we have like a
you can imagine there's like a sentence, starter like a token. What is it? Probably a Texaco is the very first word in a sentence.
With some probability it could be low. It got generated. We don't know just randomly get generated. Then
what is it? Probably if Rose given Texaco well again, it's a probably of every vocabulary word given Texaco.
So what I just showed you this example.
where'd it go?
This is this, distribution is the probability of a word.
I can also generate the same thing here, for the probability of a word given Texico
is, I'd have a different distribution for all my words given. I just saw Texaco, and I sample from that distribution
is very dumb.
right? It doesn't understand. But you could imagine, like you did, a very rich model. It could really understand how to do this. Yeah, question
adds additional.
yeah, so basically, yeah, what you're getting at here is like in this unagram world. we're trying to simplify. And so all we need to do is know
what is the probability of? Well.
we don't want to know that if we do, the probability of like a word given the previous word.
That's easy. I would say. That's easy, I agree. If we have a hundred 1,000 words you now have, like
100,000, by 100,000 probabilities, you need to track right. but compare that to if you do it for every 2 words. So now I have to do a hundred 1,000 words, followed by a hundred 1,000 words, different combinations.
and the condition on that to get a district.
Now, what if I have 3 words? It's 100,000 by 100,000 by 100,000 people.
So keep blowing up
right. And what you're getting at is the intuition as to like how you know
as that thing blows up. That's why
this doesn't work, because these sequences become rarer and rarer.
So I'm now tracking like trillions and trillions of combinations of words, and only a few of them ever occur.
Okay.
these are good questions. I'm loving this. I love it.
Okay.
we did Unagram. you know, unogram. This is all Mark Hobby, an assumption unogram says. I'm a total random actor. I am the joker. What am I going to do next? I flip a coin, and I just do. It
makes no sense.
This is just published. Well, he's only choosing like being good or bad. The joke that can do anything right
here
which which you guys are like waking Phoenix joker people right? Equally. Your heath ledger. Okay, interesting, interesting.
I'm a Cesar Romero joker guy.
That's from us from the sixties you guys are watching sixties. Batman.
real stupid. You only climb up. There's also a clip of Adam West is Batman.
He's doing. He's doing batman voice and
Bruce Wayne voice on the phone. And he's like slipping between them. And they're 2 different. You realize they're 2 different characters the way he does it
very good. anyway.
Model, I don't care where I was. I'm a random actor. I flip a coin, just generate a word, have what was our sentence we just gave
half swift, Swift.
or Swiss swift cab. Swift have makes no sense. Okay. by grant model says the word I'm going to speak is condition on the word. I just said.
Okay.
and you can extend that out now to a 3 Gram model, a 4 Gram model, a 5 Gram model. The word I'm going to say next is conditioned on the last 4 words I just said.
Okay. you see, even if we kept doing this out to 10 grand. This is still insufficient to really model language. Why, we have long-term dependency in the sentence. We say.
right? So the computer which I had just put in
into the machine room on the fifth floor crashed. So the crash is like telling me about the computer and way way way way ago. It's like, however, many words that go. And again, in our 3 grand 4 grand model
we know about very localized part of the Marconi assumption makes it easier. We don't have to calculate as much. We don't store as much. But the point is that we know this is kind of insufficient.
but it's something okay. But it's something. So we call these Ingram models
and come in a second. What I want to show you is.
I got a lot of cool stuff. What I want to show you is this.
2,006 Google released Ingrams? All our Ingram
belong to you. Do you know about that? You guys know all, all, all our base long. Whatever old school
you can go play with this.
but they don't for books. All the books they digitized. They literally just count in grams.
And so I put in aggies longhorns and blue devils just to see. And so this is, if you're doing like this is just counting words. And
why is? And it appears to be when people really sad
when people are really sad, they said, like you were in. You kind of had the blues like the blue devils are getting you. and so some phrase it must have popped it back. But I had no idea back then. But
you can change the time, so we can just do like.
And so what I want you notice here is
they just counted up these words in these books, and they calculated, how likely is it I'm going to see these words.
You have other words you'd like to see they did. One was like
Lincolnshire.
Frankenstein.
So here's Frankenstein Dracula and go space
a screen of movies. Try putting an emoji
then. Okay.
certified emergency. Right now. But you can do things like
one.
Peggy's yeah
compound
as a possible.
But you can do like, you know. Love me.
love you auto business.
You know it's something right you can play with. This is really quite cool.
You have all the different kind of eras you can look into.
Yeah, Einstein, Sherlock Holmes, Frankenstein. Yes, you kind of see, like over time, what's going on
right? And presumably like this is corresponding with like, move these.
anyway, Google and Greg Viewer. But I want to show you. This, though, is again, as you're seeing on this side is, these are probabilities they're estimating for just counting words
in in all these books.
so would you check this out, though. So if you say like, let's look at Albert Einstein
whoops.
So this is just Einstein. If you do something like
Albert
Einstein
said.
And you understand why this is so much smaller.
Right?
So you're basically saying, Albert Einstein. Now we have Albert Einstein said.
So I can figure out what is the probability of said given Albert Einstein. I can divide this by that and also given what brand? Yeah, what are they looking at? This is some collection of books, real quick. I think they go up to 5.
yeah. And there's all this stuff is really cool. So yeah, here's some fun examples like childcare. There's kindergarten different names for things. People use this. Just so, you know.
people will use this to do like a kind of
digital humanities and kind of historical analysis.
So people want to know, how do we talk about things in the past? Let's go look into the books and see how people talked about things. And so there's this whole like, if you're if you are a historian type person.
or you're like a literature type person. You take these computational tools and it opens up all these like cool avenues to study. Right? Very interesting. You can really study society this way, a lot of people do this.
Okay, I just want to show you that
like, that's A, that's a concrete way that's called Google Ingrams. So the meme is from a video game where there was like some broken English, and it said, all our base.
Oh, I know what you're talking about. Someone will find it is. This is a good old Internet. Me, yeah.
all our base belong, all your base belong to us or something. And it's something like this. And then people like ha, ha, ha! Ha! Ha! And it became a big deal. Okay.
before we get out of here. I wanted to. We're gonna do it together. Now. okay.
I give you 3 sentences.
I give you a sentence kind of sentence. Begin, marker sentence end marker.
I am Sam. I am. I do not like Renee's and Danny.
how are we gonna estimate the probability of I given? I just saw the sentence beginning, Marker. Well, we're gonna count every time I saw both of them, and we're gonna divide by the number of times I saw the sentence starter.
So
these this is our language model. This is this is a language model.
It's very simple. So what is the probability that I see the word? I? You know I just saw starting simple.
Well, 3 times I saw a starting school. How many times I saw.
did you? So we said, What is the probability that I'll see? I given the starting symbol. 2 thirds. Sam is only one-third. What is it? Probably an M. Given? I just said I.
Editor said, aye, aye.
I do so 2 out of 3 times.
Was it probably that I see? Do given? I just, said I,
one third, and you can also do the probability of
not
give and do, and the probability you can do all the other pairs.
But if you understand this, you know how to estimate
the probabilities mean you can build a language model.
There's a language mark.
Okay?
You had, yes, sentences.
paragraphs, documents.
right? If you write a report for class. The very last sentence, which is hundreds and hundreds of words away, systematically related by very versatile long distance relationships.
So what you're getting at is again
one of the foundational challenges that sort of face, this traditional language modeling perspective. This is really good for auto complete.
I'm on my phone.
A guess is the next word simple. To calculate this stuff. And to just fill in the word.
okay.
how do we get to modern era where Chat Gpt can generate long coherent sentences? Right? So basically through the
kind of 90 s. 2,000 s. We couldn't.
We need to have more sophisticated ways to model this stuff. That's what we're trying to get to right now, we can't do it.
We're gonna need to have some architectural changes.
And part of this is going to be the adoption of neural networks that have lots and lots of parameters. And so it's not just guessing this. Given that
we're gonna have these models that have these hierarchical structures that can ma model these long range dependencies. This is very good point. And like the you're, you're getting at the critical question on how to do this? Yeah, does that mean that
other spoken languages like mandarin or something, aren't going to have large models for them because they English words like, so
what about other languages. So number one my understanding is the, because, first of all, so much the web is in English, so much content is in English. It's high quality, the English language model. They're really good.
So you necessarily have the result. But you talk about there are many Chinese language models.
You need totally different like organization, right? Cause? In my understanding, at least till now, is, those models are not as good
thing that is, Chinese still has lots of speakers, lots of different content now go down to languages that are like not orphan languages, but 8 languages that have very few speakers.
They have an even bigger data sparsity problem. And so what you're kind of getting at is like there are different ideas about like one.
Let's translate everything into English and then translate it back right? Or can we learn something from an English model and then transfer the knowledge to the other language?
Don't know. These are big questions. That's it. I avoid this great questions. It's exciting. Come back Friday, and we're gonna bring this. We're gonna land this ship. Okay?
See? You all done.
and
this there's only this
share.

Okay, hey? Hey? Howdy started? I know it's cold, and I'm gonna send a shout out to all my friends in the zoom smash that subscribe button.
etc., etc. I know it's cold. it's cold. What happened? What? The heck
now freezing. But by Friday
someone.
Texas, hey? I'm gonna give a shout out. I'm beginning to throw my heart to all my homies online.
It's a big day. I'm glad you're here. Oh, hey! There are people in the room, too. Thanks for coming. It's a big day. Y'all. It's a big day. Get in the chat. I won't be responding, but I appreciate it. I'm glad you're here.
no, no, it's funny, because my son has started streaming again. I will tell you his user. But he's also on Youtube as well, posting shorts.
And I think, like he posted some video in one day, got like 1,000 views on shorts. I thought, that's kind of amazing.
So anyway, that's the only way we interact anymore. He doesn't like me. So I I'm kidding. I'm kidding. I just had to watch. I had to watch the screens know what's going on.
Anyway, let's get started we missed a class last week, so we actually missed the class. But I've gone ahead and retitled it back to 29, which is where we're supposed to be so 28 I in a previous semester I missed a class. I thought we could add 5 min to every other class, and we'd make up those 50 min we missed.
But we won't do that because you'll do it on your own time.
but I'm so excited. I know we haven't talked about this, but you know the projects are out.
I love them. I kind of tried to organize them with food and drink.
art and anime, a lot of in the educational space productivity. A lot of music I call miscellaneous recommendations of different kind of topics cars, I think a Youtube clothing, etc., etc.
Lot of really great projects love the feedback people gave really appreciate that we went through and posted our feedback as well. You don't have to respond. You don't have to listen to us at all. Just some thoughts. Okay, the main thing is, we're cooking right now. I'm very excited in terms of in this. Basically, every team, if you're on a team, which everyone is, you got a hundred for your proposal.
and we haven't counted the feedback. Presumably if you gave feedback, you got a hundred feedback. Okay? So you know, it doesn't show from canvas that will soon have your back pocket. But again, that's a gimme everyone gets that now we're into, you know, project time. So I get it. It's it's not even Halloween yet, right?
But basically you're like, in a month-long sprint. or, as we like to say. 3 weeks of hibernation.
So that
one week, or maybe even one weekend of spring.
if that is for teams approach, I think we don't do that.
My other recommendation is this. If you have a team of 3 or 4, don't divide the pieces and say, Hey, you do that piece you do. That piece will integrate later. Don't do that
will not work right. Instead, I suggest, have an end to end thing that works as soon as possible.
Okay, so you're doing
car recommendation.
Okay, we need this data. We have this algorithm. We have this interface. Just have a really dump, have a small set of data. A 100 cars
have assembled. 2 features have a simple text output, but build the end to end pipeline. So you know, you have something that works.
Okay? Then iteratively Update. Okay, I'm spending you.
Okay. But I don't want to do. And this has happened, which is, hey, cab, here's our demo. Well, actually, it doesn't really work. But here's my code for the demo. Let me walk you through my code.
That's real bad. Okay, I want to see a thing that does the thing works. That's cool
questions, concerns, comments about anything folks
chat. No one's even chatting. I said you could chat, and I wouldn't read it, but now I may am reading it. There's nothing there.
any comments, questions, concerns, homework out. I've seen a lot of kind of the answers being posted. Great love that, or it's kind of the final output. Great.
Well, there's a chat, hey? Hi.
hey, Rohan, give him
cool any other questions. We good. Are we freaking out still about operating systems.
photography. Why would you forget about that stuff? Okay. this is my, this is our image that guides us from the very beginning.
Okay, it's happening.
It's happening.
I don't know if you saw this, but today I don't have a link to it. White House put out their guidance on AI
big deal. We'll cut, maybe talk about that later in the week, basically saying, if you're building a big language model. you have to basically consider safety
and like don't kill us all. Don't build the terminator. Yeah, yeah, I don't know how they're gonna actually like make it concrete. But there's like a big thing that came out today we'll read it. We'll get back to you on that. Maybe someone can post that on slack we can discuss.
But anyway, Llms are happening. So our plan this week. And basically for the next 3 weeks. we're gonna go into Llm world. Okay, to start out. Let's get hyped. Let's get excited.
This is very exciting. Topic
number 2. Whoa, let's chill out. Relax. Slow down. Okay, Whoa! Whoa! Whoa! Let's build some foundations.
We're also going to preview a little bit how Ellen's can help us directly in our search and recommendation tasks.
Okay? And a number of projects are using them, please. And if you're not using it. You don't have to, but you may get inspired, and you want to use it as well.
Okay.
let's start with a few examples. I assume everyone here has used Chat Gp.
some of you have used barred. Maybe maybe maybe those are the 2 big ones that people know about.
And I want to compare it to kind of our traditional search.
This is like we, we built CAD's algorithms. You come to our remember that I'm kind of drop the ball on them a little bit. Remember, you come to our website where we sell you the albums.
You can do search. You can get recommendation. Okay.
sort of compare like, what's going on there and the differences and why people are so excited. Okay, so
consider just a simple factoid query. Right?
Where is the Loos screenshot from Google. I didn't show you there's a map over here.
but it just tells me the answer. This is where the Louvre is. Okay. And this is the Wikipedia link. Okay, if there's a map to the right.
this is just Google. Search nothing. Fancy no bar, no. Llm. I go to Chat. Tpt. Where is the Louvre? Blue is in Paris.
and it tells me famous renown. Blah blah! Blah! Where is it? It's in Paris
which one is better.
I don't know. I don't know. I mean, well, this comes back to the user intent.
which is like, do I care about where the Louvre is? Because, just like in a like. I don't have no idea. Where is it? What's Paris versus? Where is the Louvre?
I'm in Paris? I want to go to the Louvre. Right? So one issue is, I mean.
presumably Google knows where I am. Since you're in college station. right? So the Matt, I don't think it gave me like directions.
But my hunches. If I was in like Leon, or some city in Paris or city in France, it might tell me like, here are the directions. Okay, I tried this on Google barred, and they've integrated the map. So I can say, Where is it? Says this is exactly where it is facts about it. And here's a map of it. It's a little bit of integration. Okay? Fine. Thanks for fine.
I would say about equivalent, right? Simple factoid. What if we consider slightly more difficult queries? Right? So I say, describe the style of the news.
And so what Google is doing here is right. It's finding a page. It's previewing kind of the key snippets. And it says.
and so it's telling me about okay, it must be Renaissance.
But it's basically I found a page that mentioned style, and it highlights it for me. Done. Okay.
But when I go to this is bar. It's a mix of architectural styles. Okay. The oldest section was in the French Renaissance style.
but then we have the East Colony, which was the bomb, and it's a massive piece of French baroque architecture. So it's giving me, I would say, this is way better, right way, more complex way more subtle, I think. I asked the same thing. I didn't.
How about a creative task? I asked Google to write a one stanza poem in the style of Ogden Mash about information retrieval.
and it gave me
10 of the best August mesh owns
everyone should read it.
But this is like a major fail. Right? Why does this fail
on Google?
Right? It's not creating anything. It's not generating anything new. I'm asking for something that doesn't exist right. All it does have to do is point me to things that do exist and hope.
What I want is somewhere in the things that do exist. But presumably there is no poem in the Silo of Aug didn't ask about information. Retrieval. However.
I ask, this is like, it's barred
information, retrieval or wonders thing. It helps us find the data we sing from books to websites. It's all the same information. Retrieval is a game. So it's actually one and a half scans, as I would say, didn't really understand.
So let's raise a glass information retrieval tool that's essential to our digital spiel.
Okay, do you guys know Ogunesch?
So he's one of the great poets. I'll give you an example. The cow
right? See? I mean, people are smiling and laughing
when you read this. No one smiled, no one laughs. Right? So it's missing the essence of Ogden Nash wit.
Okay, Google for Ogden Mash Mini poems like this. Very short. Funny.
very good. So again, I would say, like
a couple of things are happening here. One. It's not really one stanza that has a trouble counting.
It does generate something, but it lacks in this case kind of lacks the wind. That's the fun of of augmentation. But it's definitely like, pretty amazing compared to traditional search.
Okay?
I passed it. The the text of part of our readings. And I said, this is Chet Gpt, I said, please summarize. And I gave it like a big
chunk of text. And it basically says the text of stuff would be important to problems, models it highlights, models. This is pretty amazing. You cannot do this on Google. Of course.
I have to summarize football history, but integrate. How delicious barbecue is! Okay. It says, the team has a fan base
throughout their history. The ages have consistently displayed a commitment to excellence and hunger for success, much like
the irresistible aroma, delicious barbecue waffling through the air on game day. This blend of savory flavors mirrors the team's dedication to savoring every triumph and enduring the challenges. Okay.
pretty good. What? You will notice, however, maybe if you scroll down a little bit, I don't see much specific yet, but this is from Jetg about actual Texas A. And M. This could be any
school, any football team. But maybe if we regenerate it. If we scroll down we would find some more details. code understanding. Right? You guys have probably tried this. I passed in some of our homework code from doing? Some of the
data transformations. I basically said, What does this code do? I just do it all in there. And it says, Yeah, it reads this file
right? It creates dictionaries. It updates these columns by reint doing re indexing. Well, it's like, that's what it does right? It's pretty good. very helpful trip can't get that out of Google
code generation. So II passed in the MF code. And I basically said, How would you train the MF model. And it was like, Okay, you need to initialize it. And it generated some code and it tried to fill in
with the learning range and regularization. So it kind of is doing the right thing. It understood what the code looked like. It's kind of helpful. Okay. 0. Have you guys used this for cogeneration.
One's like, yes, but I'm not gonna say it's a mix you had to for class. It was an assignment for class. See Kev. It was. It was an assignment where I had to use chat. Gvt.
so it can work. Okay, can work. Some of you are like, actually works by. Well, okay.
okay, then, I tried recommendation. Remember our other task on cabs algorithms.
And I said, I enjoy Taylor swift albums. Could you recommend album albums within the artists that have a similar vibe. I said. Certainly.
And I said, these have elements of pop, country and singer, songwriter. So
bonnie, I think it's Bony Bear. Did I do that right?
Midski suck and see them? So maybe if you read these cases they make sense to you. maybe they're good.
Okay? So maybe we don't need all that matrix factorization.
We don't need autoencoder. We just ask the language model
could be, don't know.
Oh, and then I went to Google, and I said, I enjoy 10%. Could you recommend? And it first of all, it tried to correct me and say, I would. You recommend, like I have to be more polite. But then, what is it doing? I can't create. But it found a red thread where someone mentioned similar vibe to a particular album
and then have a bunch of results. People talking about specifically, right? So it's again, it's able to take advantage of the fact that someone has already talked about it
to do, to try to help me find it?
Other examples that you guys have tried or encountered in this space of Llm. Magic. Anything you've done. It's been amazing. Or you've been disappointed in
yeah, limitation.
So is it hallucinating? Or is it just? They're they don't exist like they were there. But they were gone.
Okay.
so there's something which is yeah, like, you know, definitely, links die.
And so if it's old, you've lost it. It's also the case that, like they will hallucinate links. So it'll say, here's a link for you that has the data. And you're like great. And it's like that, data doesn't exist. The link is not real. So we call that hallucination. That's a big.
that's a big problem. Any other examples where you guys have been using a large language models. Gbt, for anything.
I'm just kind of curious, like,
there's kind of like plagiarism. Kind of bot detection.
Yeah, and there are these dummies. There are these dummies who know just enough to be scared of of large language models, but also dumb enough to think that they can rely on them for this stuff. So they ask it.
did you know? Didn't AI generate this? And it says yes, and then you failed the kid and the kids like. But I wrote it.
And the guy's like, Blah blah, the Chad Gpt told me. And it's like you're basically outsourcing your like actual expertise to the machine that you claim shouldn't be relied on makes no sense.
Any other examples of you guys have had fun with.
does anyone here try to jailbreak any of these things. Some people are nodding, you know, about this.
It's like, what's what is this whole jail break business. I mean. It's like.
I guess I can use to be like way back early, way back early.
How to do the these are not real people.
Those examples are very interesting. There was when a
it was when barred kind of the barred plus Tedgt came out. There was a big I think a New York Times reporter had this long, long conversation before they had a lot of these constraints in it. And he basically like
the, You know Bard, or whatever was like, I love you. Yeah, he's like your wife doesn't love you. You need to be with me. We need to go live together. It was really wild, I mean, really wild.
And I believe also he got it to.
he said, like, well, like, who created you? And he's like well, like engineers, and like well, who exactly. And he was trying to like reveal names, like private information about engineers and stuff.
Very scary.
so another sort of just by by hands. How many of you have used Chat Gp, like in the last week?
So what are you guys using it for?
He's straight with me. I'm just curious.
I was messing around with the Api.
That's around the Api. Here for the project. Be great answer. Hey? What else you got to do this.
Better. Explanations. Documentation. Yeah, you saw this like stack exchange. And all those sites like have fallen off a cliff because people aren't using them. They're just asking Chat Gpt, which is trained on all of that data. Other reasons. You guys, I'm just curious.
Yeah. So with.
how would it be possible to get some good ideas.
I mean, were they reasonable? It just weren't good for you. Not personalized. Yeah, yeah, that's interesting, you know. Create a task like that, or.
yeah.
kind of thing. Help me get started.
And to be honest, like
one of the challenges that you know, I understand we're pedagogically like we want you to understand, like how to build that stuff. But for lots of things it makes sense. It's like that I'm not interested in, just like, get me started. And now fill in all the stuff right?
I don't know. It's very curious. I don't know what the right right thing to do is this is great great examples. We also have, like Halloween
costume ideas.
We have like kind of code starters. and we'll say other things. Cool cool. That's really great.
So
what are Llm, so I grab, there's a really great survey I posted. I think I posted it in canvas.
basically like to give us a definition. They say, Lms are transformer based language models that have lots of parameters train them lots of data. They can understand language. And they can solve complex tasks. Okay. So we're gonna try to do
today's I'm gonna try to kind of get you hyped and like, what is this all about
again? Then we're gonna roll it back to the kind of Pre Llm, and then kind of build our way back up to it
the best we can. Okay, so a couple of things. First of all, transformer.
What
transformer basically proposed, you can argue about like the there were
some of the precursors to transformers, some attention. Ideas have been around earlier. But essentially 2017. We have the transformer.
Okay.
which is basically using multi-headed attention. If you don't know if this is, don't worry, we're gonna try to kind of explain it a little bit later. But for now I just want you to get in your mind that just 5 years ago.
6 years ago.
We have this paper come out. Attention is all you need. Okay? And again, you guys aren't sort of super like oriented, but like, that's like 100,000 citations in just a few years. This thing is like again, like an atom bomb. Right? This is attention. This is basically transformer.
that kind of changed everything. Okay, we had neural networks. People understood them. They were using them
transformer hits. And that basically launches the large language model kind of revolution. Interestingly, I didn't. I think I posted this link. But there's a really interesting article from the Financial Times, and these are the authors
the Google scientists who pioneered an AI revolution. And if you go down and read the fine print.
their paper paved the way for the rise of large new models, but all have since left the Silicon Valley Giant. They've all left Google to go do startups and go do other stuff.
It's kind of interesting. This is part of the reason why I always say, all these companies open AI, Google, etc., etc. like, is this is Google like saying, Hey, we invented this cool thing here world, check it out.
and then everyone who invented it left the ideas get out there, everyone copies the idea. And so now, I would just say, all those big companies are kind of nervous about publishing their secrets.
Okay, they still do. You can still find papers from Google doing this kind of stuff.
Openai publishes a little bit.
But basically the idea is not. People are very nervous. Companies are very nervous. They're gonna publish like the secrets.
because the idea is there's so many good people at all these places. If you publish something like this, people can figure it out and then get it implemented and go. Okay.
very interesting times these days. But anyway, they've all left. They're all gone. If you can go. Someone has like there's some mean that went around a point where they all went, some to start up, some to other companies, etc., etc. Very interesting stuff.
I got this from Wikipedia. But just to kind of put it in context. I just want you to see where we are and like what's happening in this class. There's transformer. That's bag of words. This is like.
tf, I df. vector, space
word embeddings. That's like word to Vec.
all of this, these are just neural network methods.
And so I'm just trying to give you a sense of like, how do we attack problems in this space? The cool thing is, you know, you know this, you know this.
Okay? Again, this is not a neural networks class. So I've tried to kind of elide some of the details. But the idea is like this is just current neural network. Lsdm long term, short term memory to different Rnn architecture and transformers is the variation of all of that. But the idea there's basically a straight line from the stuff we already know how to do to this transformer stuff. Okay? And if you'll remember word embeddings like word to effect. This is all like 2,012,
2013 bag of words. As you know, you know, sixties, seventies
to present transformers are, basically, you know, 2017, 20, 18. So all of this. I just wanted to sort of the scale of things like kind of had this
long, long time. Where do that exist? Then? For the neural network, deep learning revolution happens. See how compressed all of this is. Lots of innovations happening very, very rapidly. And we've kind of arrived at kind of like the transformer, which is kind of like the thing that takes us all the way. Okay.
so what I'm gonna try to do is like we're gonna kind of skim some of this. And I'm gonna give you
a little insight
into how this works. But again. the way I'm trying to structure the classes, we're not we really. We would need to spend weeks on R and ends
all the basics. So what I'm going to try to do is kind of like juggle. So you can kind of at least appreciate what's going on. Okay.
so yeah, this is really great article. Again, notice how fast things are moving. This just came out in September.
It's really cool survey of large language model kind of summarizing what's going on.
And again, just to put in context what we're talking about, there's really great figure of just like how rapid things are happening. So again, you have transformer paper
attention is all you need. 2,017, like December 2,017. So basically 2,018, this is basically walking you through 2,019 to basically
August 2023, all of the Ll ins that have been like launched and kind of all these, it's like Google, T. 5.
Here's Gp, t, 3. This is just like January to April. You have. Boom, boom! Boom!
But what I want you to notice is like 22.
All these things come out. All this income out.
Verse 23. Sorry. Yeah, this is all like in the later 22,
just to kind of give you a sense of like this explosion of this ecosystem.
what's happening right now. So again, it's not just us being excited. And you're thinking bard and jet. Gp, but look up here, there's like a million of these things. Okay? You can see some of them like our way, this is a Meta. Google has a bunch of different ones. By do. Chinese search engine has a bunch. Amazon is in here and many, many others. The idea is, this stuff is kind of like blowing up. Okay.
it's kind of nuts.
And then I grab this from the paper. This. These are the publicly available ones.
So these are these are ones that you can get. And again, just to show, like.
how many of their been so so fast. Once you guys may have, you know, once you could really use or have heard about our llama and llama to those are very popular
from from Facebook slash inventor.
And again, it's trying to show you here kind of the size and parameters.
what kind of data they're training on. But you know, like trillion tokens, hundreds of tokens, big big training data. again, a lot of places don't report how long it took to train. But like hours, 2 months, 3 months.
and so on, and so forth.
These are some of the closed source ones. It's like we know about them, but we can't necessarily get access to them. Same deal. It's like
really, really gigantic.
you know, huge, pre-training.
insane amount of hardware to to do these things.
It's just totally nuts. And just to give you an example, here, this is llama. So for a lot of I don't know like, for if you're doing a a lot of the projects are just gonna use chat. Gp, because you're just gonna interface with it through like an Api.
But if you're actually trying to do any kind of like fine-tuning, or actually instruction tuning or doing anything with the actual model parameters
you need like a public one. So Llama is the one this is begins to show you the whole ecosystem. This is just like in the last, like, you know, basically 18 months right? It's like you take llama, which is just a big one. And you now trained over some Chinese data, you always variations. Right? What if you took chat data chat data. Okay, but
what do you have? There's this one
that that
anyway, you can get down to like open flamingo. There's a lot of yeah
for the grade. Yeah. So basically.
I think that they're. I think some of these are training from scratch using the model. And then some of them, I'm not exactly sure on how they define this, if they're just reusing the parameters and then finding the parameters. So I'm not sure
but just to give you a sense, too. It's like there's also there's for this push
and sort of philosophically you can try to think about which is. do you try to make the models bigger and bigger and bigger.
So again, it's like.
here's here's plan. Here's when new palm.
Okay. So the idea is like, if you're Google.
you want to build just one really big language model, if you're open on AI, do you want a big one really big language model? Okay? Or
this sort of llama idea is I'm going to take this pretty big one and then make it super personalized and customized in different domain.
Okay? And so there's sort of this tension here, which is, some people believe this is the way to go. Okay, all you need is a pretty good language model. That, then, is like super focused on a particular topic
alternatively. There's sort of this sort of agi like if we just build the really really biggest one, it can do everything anyway.
So we don't even need to specialize it, because inherently, it knows so much about the world. It can do anything. So it's like superhuman.
I'm not gonna answer that. I don't know the answer. But just so you know what's going on right now. It's like totally insane.
Oh, yeah, I put this in there, just to show you kind of like kinds of data. We're talking about large language model. They're trained on lots of data. And just to show you again, like, there are these existing data sources like, there's the pile.
right? There's book Corp, but this is like 41 Gb, there's book Corpus. There's people, you know, Archive, that has a bunch of github. They take all of github as training data.
is totally nuts. If you know pubmed scientific word. You take all the science, you take all Wikipedia.
So we're kind of getting them to this world where it's like.
if you have high quality, good data, you get all of it, you can. And then you train on that. Okay, so if you wonder like, how can it understand my code, or how can it like design something for me. It's because it's like trained on, like all of all you know, for example.
right?
oh, yeah. Yeah. And if you're wondering like why, like it answered a lot of my questions like, I have, like all of stack exchange. right?
So it's like, learn from all of staff exchange as part of its training.
Okay.
awkward.
I know right now, like, your heart's like going. This is awesome. I'm excited. I'm excited. Okay.
so let's slow down for just a second. We're gonna go back to the old days.
So my hope is right. Now, you're like, okay, I've used. It's cool. There's clearly, a lot of activity happening. Kind of don't really know what's going on. Train over lots of data.
We can do some cool stuff. What I want you to do now is, be like me. This is now my lifetime.
and you get in your in your time machine. We go back to the yielded days. Okay?
And so we said, I can go and ask Google today, where is the leave?
It answers, Okay. but if you go to Google 20 years ago.
Is this where you can't believe this? You go to Google 20 years ago, and you ask, Where is the Louvre?
This is what Google looked like.
The first result is a Pdf. Of a scientific document
that happened to have this example about where the Lewis Museum is located. The second one is hotel hotel in the heart of Paris.
walking distance to the looph.
And then the third one is the paper we're gonna talk about right now. All the scientific people talking about the need
just want to give you a sense of like 20 years ago. if you search for where is the Louvre? This is using lots of the tricks that we already know. There's no word embedding. We're doing some tf-idf tricks like this.
And it's basically saying, it's like, Here's a document that has those words in it. Here's a document that has those words in it. It must be a good match.
right? But notice what it does. It doesn't do.
It doesn't answer. Notice what else it doesn't do. There's no math. Do you know why there's no math.
there's no Google Maps.
And I think to be the old guy.
I'm gonna tell you this, like Google Maps is, and you could literally drag them out.
It broke people's minds.
They're like from here to there and give you basically like a static image of the route.
you know, like high school. the Google Maps comes out. I'm like.
I can zoom in. I can zoom out. I can drag it in my browser. There's not some like standalone, like applications that in my browser people's brains. And then you guys are all like
and chance, you can see, like I mean.
I had to revise the code for my demo project. It wasn't great. so again doesn't work.
So I want you to go back in time. Pre, large language, model. pre word embedding.
And we're trying to build a better Google that can answer this question. It's very simple. Where is the Louvs located like we should be able to do that. Okay.
I still show you this is coming from this paper called Web question answering is more, always better from 2,002. And again, I like to highlight things. So like, you guys know, this Guy Andrew Ing.
He like he's a was a Stanford professor, but he did coursera
he has all these online videos about machine learning. You may have watched. Jimmy Lynn is the guy. This is when he was a Phd. Student. He was the one who wrote that
the neural hype.
hype thing recanted. That's him.
Eric Rill is like a super deputy at Nsr.
Suda. May is like kind of a goddess in this area at Microsoft for many, many years, who also did some early work on kind of like matrix factorization. But for text.
though back a long, long time ago, very cool stuff. Okay.
But I want to show you these are like people, Berkeley and Microsoft and Mit, thinking about this problem 20 years ago.
I'm trying to show you like how people thought about it and how much we've changed. Because this is very much like what I call like a data mining Pre machine learning world.
Okay.
So their argument is, where is the Lu located? You want? Say, Paris, or give me like the actual like address. We don't want links. Okay?
And so this is their style. I wanted to show you this is something that we would have been doing in this class as a project 10 years ago. Okay.
we would say, where is the move located? We're going to rewrite that query. The Blue Museum is located. Blue Museum is in Louisiana is near. We're gonna rewrite the query and then go send it to a search engine.
Okay? So we have no knowledge here. We're just going to send the queries. We're gonna mine some text out of the results. We're gonna do some way to kind of combine them.
And then we're eventually we're gonna hopefully find in Paris. France is the top result. And so where is the Museum Lou located? It's in Paris.
But if we screw that up, our second answer would have been, where is Lou Museum? Located museums?
Where's Lucy Zee located hostels? So I want you to see how kind of fragile this is. Okay. So the hope is.
the web knows a lot about belief. Just people say it a lot.
And it's in Paris. And so we can find that relationship. We can tell people. But notice, we're not actually modeling like the deep semantics of this. Right? It's very like superficial.
Okay, let me show you how this would work today is very simple idea, but I think everyone can appreciate it. So so the idea is, the hope is that the answer is like very close to the question.
So the question is, is, where is the Louvre Museum located? Notice that, like all of these words, the Louvre Museum is located, and that sentence, we hope, exists out there.
So the idea is that answers may have lots of words in common with the question.
who created the character of Scooch?
Charles Dickens created the character of Scrooge. All these words line up.
Okay?
So the idea is, if you rewrite the query.
and then you go issue that query. The hope is, you find matches
that will then have kind of the answer right? Next to it. That's kind of the dream.
Okay? So they do some more kind of notice. This is all like kind of hand engineered. Oh.
we had different kind of who queries when queries, where queries? So we have special rules for different kinds of queries.
So, for example, for a weird question. the rule might be, movie is to all possible locations.
where is the Web Museum located? Is the Museum located. So it is
because again, they don't understand. They don't understand. Kind of grammar. Here. The Lou is Museum located. The Lou Museum is located. Let's go. So the point is, some of these are kind of nonsensical.
but they're saying we don't care. We don't know. We're not trying to model English, but the hope is, one of them will work. Okay? And they also kind of say, well, it's a where question. We want to have a location. It's a win question, we want to get it digged out. So you kind of have these rules about what they're trying to find.
Okay? So now, oh, yeah, also notice this. We know that, like
some query, rewrites are better. So where is the Lou Museum located?
There's one period which is the Louvre Museum is located, which is all in parentheses. So we need that exact match that's really good, because maybe it's gonna have in Paris next to it. So this might be like a super high weight.
whereas the query it has the word moved. It had the word museum and had the word located is like not. It's okay. Queer admission count as much.
We have kind of different weights. And this is all kind of hand tuned. Yeah? Question.
yeah. So I'll be getting a little bit of myself.
Let's do it
because it's like the bread, because if you type it like the breadth revolution laws, you're probably just gonna get linked with Pdr. Or Bitcoin.
So it the
when was the French?
You can't. Oh, you can't read that.
I don't think they'll have those. It was it really was.
This is how I, this is how I sign my name.
What's that even means.
actually, yeah, that's that's yeah. No, I can.
Okay, yeah. When was the French Revolution question mark.
And so you can imagine, like, ideally, we're gonna have. The French Revolution was kind of in. You can imagine that something like this. So there's a version of it that maybe works.
But what they're gonna do is they're gonna say something like, Okay, if it's a win, we don't actually say we could say
the French Revolution was when something. So you can imagine we're gonna generate all these citizens that don't make sense.
And what they're gonna do is they're gonna use some of their like kind of human intuitions to say, like the French Revolution was. Oh, this is this is like
the high quality one. This one's okay. And so then they're gonna just have rules, though.
But the point again, I think what you're getting at is, you can always kind of construct sentences that will break this, because again, they're not really understanding English. They're just moving tokens around.
That's the key thing to keep in mind. This is again. It's very cool, but it's not actually understanding.
So now they do. Is, you say, like, where? Where was the question now like, Where where is the friend? Where is the Louvre?
But but we rewrite it. They go within a web search engine.
This is kind of like that paper we read, remember about
a web kernel.
It's on your homework. Remember that one. you say. How close is this word to that word.
we query the search engine, we get the results. Back. We compare those same idea.
So we give back the results.
Then we go mind the Ingrams out of the results. Remember, Ingrams, like the one Gram, they're just like each word the migrant or pairs of words. The trigrams are the threes of words, etc., etc. So again, you take all these queries. You send them, you get results. Back.
You pull out the Ingrams. Okay?
And then you wait. The Ingrams, and you weight them by like. How many times you see them, maybe where you also consider how good the query was.
So you can imagine who created the character of Scrooge.
Christmas Carol again. 72. Carlisle's
what he did.
Someone's googling it. Who did Carl Banks invent.
Okay.
okay, there's a different, I think I think Carl Bates invented Scrooge Mcduck. I believe
so. I check that out. Tell me if I'm right or wrong, or animated Scrooge Mcduck.
But again. you get a bunch of these are just Ingrams. Okay?
So then, what they do is so don't worry about this, do they? Do, is they say, well, I found Charles Dickens. I found Dickens. I found Mr. Charles.
Then they have it, because again they don't understand that these are related.
but they just know. I found 4, 15. Mr. Charles has 10. They have kind of like this iterative algorithm to combine things. So they say, Oh.
Charles Dickens and Dickens. They have a lot in common. We'll merge them and get the 35 score.
Charles Dickens and Mr. Charles have a lot in common will merge them. So would it be Mr. Charles Dickens? So the idea is, yeah, by, as they call it, timely, we put them together, and the idea is, then you get the highest score. Okay?
So the hope is. we end up finding this. Mr. Charles Dickens is the right answer.
So they actually tried this on some trek competition.
They tried it over like,
this track content. They had a million documents, and they kind of did okay.
it like rings like 9 out of 30. Okay, but the point is, it wasn't taking advantage of the scale of the web.
So instead of they did, they said, then they tried to use web
as a whole, they end up scoring about second or third, because they have to get all this sort of repeated evidence from the web itself to give them better results. Okay, so what I want to turn back to you now is to think about
like Number one. This is like the olden days
when you were just little tots. How we would approach these problems. Notice what we're missing.
There's no machine learning here.
This is all kind of like untuned rules. We don't learn anything. Okay, it's kind of heuristic based. It's kind of like hand-tuned based.
Notice, we don't have any notion of like entities and entity types. So, for example. I don't know, like I had no knowledge that the Louvre is a museum
like just hidden somewhere.
I don't know that dates. I don't even know what dates are. I don't know what locations are. You see what I mean like. I don't have any knowledge. It's just surface words. And I'm doing matching on.
There's nothing deep there. There's no like understanding. So what's happened
in kind of 20 years. Since then one thing is like the development of knowledge graphs. It's very popular. So think about like on. If you go on, Wikipedia.
it'll tell you. Charles Dickens was born in this year died. In this year. He was born in this town. He wrote these books.
people have taken all of that and said, Let's just that's knowledge, and so do things like this. We say, like
Charles Dickens. he's a node
he has. He was born in. Does anyone know what year he was born in. I'll not do. I'll let's get rid of Dickens. He's boring
there.
Let's do the lose.
is in
Harris.
Paris is a let's say, a part of
France. France. You can imagine neighbors. a Spain. a
Raphael on the doll is from Spain. a. He plays tennis
A
pickleball is like a
competing sport or something.
And it's not just these pairwise. And because pairs has lots of all these things have lots and lots of edges.
What I'm trying to show you is people realized
you can go build these big knowledge graphs. So everything we've done in this class so far, like when we go search for tailors without ever. We're kind of doing it on the surface. We're like hoping the words match, or even if the Embeddings match and hope they match. But we don't really know. Fair Smith is like a human
who was born, and you know, blah blah, we don't know that like these are albums by right. We don't know those things.
And so one of the big advances in the last 20 years
is development of knowledge. Graph, build these big knowledge graphs. So if someone says, Where is the Louvre?
I already know it. I can just go look it up, I can say.
and if it has, a relationship, is in or located in. I just look, I go to Paris, dummy
or I say, what are some Taylor swift albums?
Our style right now is we have to kind of like, do some matching.
I can just go look at my knowledge graph. This is Taylor Swift. These are the albums
done. Okay.
that's pretty cool. Second thing.
do you guys know what these are?
Yeah. Stackoverflow quora?
I use. I know a lot about Yahoo answers.
Those kinds of question answer sites gave us lots of data of the kinds of questions people ask and what are good answers. What do they look like?
It's part of the issue we don't need. We don't have training data. So now I have. Here is a question.
There was a hundred answers. This is the best answer. Why? Because the community all uploaded it. So you take that. Take these 2 things. plus Ml. Models. Now, I can learn. What does a good answer look like?
I can now learn to answer a lot of these things.
So this is basically like the last 20 years, I would say
in one slide, apologies
to get there
now that we have that come back on Wednesday.
and we'll take that to start talking about language models and how they can solve the problem. See them.
Bye. Goodbye. Online people. Thanks for joining us today.

Yeah.
Yeah.
hey, man, great to see you all. I'm back. I'll share photos of my golfing next time
I play actually in the 2 rounds. Oh, wow!
The first round was that Moon Bay, ocean corridor. and great vistas. Beautiful is rainy.
I got to hold it up a few times and some good shots.
It was pretty sick, and then I played shoreline.
it's all good.
not great, not great. It's good to be back. I missed you all.
let's go to the important stuff. So we had a vote. thank you. We had these. These are our swag that you guys all got to give your opinions on
pads algorithm with kind of like, some kind of like waveform
hypersphere.
You guys get there worried about you kind of seventies cabs, algorithms. And then this one here votes are in.
So it was A and C, you guys, you guys like the algorithms looks like blue is on top.
so
something along these lines, I'll send out a revision and then we'll finalize.
Thank you.
Thank you. It's exciting. It's exciting. Okay.
since I've been gone. Anything troubling you?
Math 4, 70 is probably the muted okay.
Anything else going on in the world? Any other things worrying you?
Are you guys looking for jobs. So I'll say this and talking to people in industry, they said they felt right now was a very tough time for new grabs. You guys feel that recognize that basically
despite and hiring is going for more experienced people.
And it's time. So they were wondering how everyone is doing? Not good. So what is what is the plan like? Try to like intern again, or
if you had any questions or concerns, we could talk about that as well. Because it is troubling, you know it's not good.
I would say historically, like our majors are very respected in industry in particular in comparison to some other schools in Texas. The feeling is that we have like
they're very strong.
So if you're suffering in OS, it's there is a you know there's a benefit to it.
Okay. alright, how about homeworks projects? Any other concerns?
It is
all right. We're all supposed so I'll give you. I can't tell you secrets about what I've been doing.
but I will tell you this, these Llms that Richard talking about next week.
That's all. Any one of these companies are talking about, you know that.
But I mean, it's like to the next level. So even like, let's say, 3 months. So I visit 3 months ago.
It was like, Yeah, we're excited. There's a lot of enthusiasm. 3 months later. It's like turned up even more. Okay. So right now it's super super exciting time.
We'll get to that next week. I'm very excited. That's how we're gonna end the semester, maybe okay
to the moon, etc., etc.
Okay, so let me say, let's get back into. We're basically grabbing up additional recommendation, right?
And when I say traditional recommendation, I mean, like, how do we take advantage of.
you know, sort of smarter architectures to still take advantage of like user item, interaction behavioral data. Okay, so kind of open this up as like a thought question to take with you for the weekend would be
if you had a big enough language model.
Could we throw away all of that human behavioral interaction.
What I mean is this, take an example platform.
Let's say, what's your favorite platform?
Youtube, Tiktok Instagram, Youtube, for example.
Youtube, right now imagine how their models are built. They have every time you click on a thing you subscribe what you smash that white button every time you do that stuff comment, whatever
all of that stuff goes in. And somewhere there may be doing some sophisticated neural networks that are basically trying to figure out like, what are your interests? What are these items all about? And somehow matching you to items right
thought question, could we throw all of that away, and instead, purely rely on
tbt, 4 or 5, or whatever
I basically just say, Hey, here's me. Here's some videos that I subscribe to. Hey, lol, recommend to me
what I watch.
Think about that. We'll be talking about that in the next couple of weeks.
And let me tell you what that's the kind of questions people are asking. Right now. can we do this?
Let's go back to reality. For a second.
last time. Autoregs, auto encoder based recommended. We take, input, we smash it down.
we inflate it, we recover ratings. Awesome.
Can we talk through how this works
and my hope? All of that is again, we never really have done neural networks in this class. But if you understand this, you understand, basically.
I take rows of this.
I multiply. I do a vector matrix multiplication. Maybe I add in another vector maybe I do an activation function.
Then I multiply by another matrix. So Armi is just a couple of matrix multiplications. So it's just multiplication and addition.
And you understand, like the basics, the rudiments of this sort of neural transformation. Okay? So that's the cool thing I hope when you walk out of here, you're like, if someone says, Hey, are you an expert in deep learning and neural networks.
you can say no. so don't lie.
don't lie. Don't say I watched a video on Youtube, and I know what's going on. You say, no.
see? However, I do understand the basics of like an autoencoder, which is basically a very simple 2 layer neural network which, when you get down to it, is basically just some matrix multiplications and some additions. And that's kind of it.
And so if you can say that you basically are communicating, you know what time it is. Okay, you know what's up
alright.
Let's go on a little farther, and then we'll finish up kind of neural recommendation today. So on the heels of auto rec 2,015 kind of concurrently. The next year there's another auto encoder for recommender. Okay?
But this one says, wait a minute. We're gonna use a denoising auto encoder. Okay.
anyone ever heard of that?
Oh, so check this out. This is pretty neat and very simple.
you know. Autoencoder. Okay.
all they do is they say, number one.
get rid of the vanilla auto encoder that you know what is denoising. We'll figure that out and 2, they're gonna model the user for our case. You can ignore this
when I say, ignore, like, don't worry about it.
I want you to focus here
specifically on this denoising bit. Okay? So the denoising bit is this.
we're gonna insert noise into the input.
Aka, corrupt the input
and then we're gonna go ahead and do our same old thing, smash it down. recover it.
The hope is, if we have a little bit of noise, we can force the hidden layer that's like the like the embedding. That's like that compressed
middle part
to be like more robust
then just sort of mapping from the original. So let me go show you what I mean, so like this, this is our input our original input right?
And the idea is like this, you know, this is like user, a, you know, this is a user user, a.
And here's an item. They gave it a 4.
What if it I mean could have been a 5. It could have been a 3.
You gave this one a one. I mean, you could have given it a 2. It's like.
intuitively, you're like I wanted to see on the same. And think about that same thing with you. Like, if it's not really easy. you clicked on a video that would show up as like like an indication. I clicked on it.
Maybe you've done it by accident.
right? Maybe start watching the video. You're like, Oh, okay, I didn't want to watch that. Let me get out of there right?
So it's kind of like.
These are kind of imperfect measurements of like, what's really in your head. Right? Is the thing we can measure. And we can look at that. You get a 4 that we don't put you on the stand and ask you how much you like this movie, or you're like 5,
you're going to jail as a 4 4 in the back. No, it's like I like it even, for I asked you tomorrow. What would you give anybody?
You forgot what you need? Okay, so that's the kind of the idea here, and saying
the
data that we've got is not really reflective of what's in your in your heart.
Right?
So when we do all this auto encoder back here on this fig.
we're basically treating this as like, this is like the truth, you know. And so we're trying to say, we're gonna basically build our encoder to compress it down, to recover as closely as possible. We don't want to screw it up.
Okay? So the whole thing with denoising auto encoder, it says, No, actually, that's insane, like, really, we know that this
the input, like.
we don't want to be so rigid. It sort of doesn't really reflect who you really are. So let's put some noise in there. So, for example, Gaussian noise like messy Gaussian, you know, I'm talking about right like
something like this.
So just flip a coin, flip a little Gaussian coin, and if you gave it like a 4,
you know. So really, this is like 0. So it's like, you know.
do we? Do we add something, or do we take something away? We just flip a coin. So the idea here is like your original rating
is a 4.
We're gonna add add noise, and it becomes a 3.7,
and a different one is you gave it a 4. We're gonna add noise. It became a 1.2 and a different one was a 2, and we add noise, and it became a 2.1. And so the idea is, we're kind of saying is like
the actual rating doesn't really matter. We're going to move it around a little bit. We're going to add noise to it. Okay?
The other idea is, you can basically like it's called mask out or drop out.
You basically throw away some dimension you basically randomly override any dimension with 0 with some probability of queue. So that is, you can go through there with some probability you can swipe things out. Okay.
again, I wouldn't worry about this one so much. But the idea is, we come to this thing. This is our original.
and then, like, after we add noise.
it could be something like 4.2 4.7
1.3 4.8
5.1 da da, da, da, we're going to add noise to it.
Okay. does this idea make sense?
Ish, we're adding noise to it. The intuition is, we're going to be more robust, and we throw this thing into the into the crank
than otherwise. Okay. model the user. I told you, don't worry about that. Okay.
does it work? Okay, does it work? This is mean average precision
mean. you know, precision. It's something like that.
This is on some Netflix. This is increasing the corruption level.
Okay?
And they're basically saying for different kind of versions of the model, the more corrupted it gets. In this case quality goes up. it goes up, and then it comes down. If you do too much up a little bit. Maybe it comes down.
This is map at, let's say at 10 same deal.
It goes up. There's some peak here, maybe Peak here, you know. Maybe Peak here. But the idea is
maybe here the idea is some level of like corrupting. The input can squeeze out better performance. And the idea is you're better modeling the user. You're not as rigid. Okay.
yeah, please. Yeah. So there are cases where it doesn't. You know, it may be it's not a good idea.
right? Absolutely.
same deal. Here. This is on a movie lens data.
They're doing a denoising. Oh, yeah, yeah, don't worry about this one. This is, you can ignore this this last bit. They're doing a comparison to other methods. So they kind of pick their best version of CD. Ae, that's them.
That's them. They say this is just popularity based recommender.
You know, this one
that's itemizing filter. You know that one
that's matrix. We didn't talk about this one.
This is cool.
Let me talk about this one. These are all like they're kind of, you can imagine for variations of Ms, okay?
And what they're showing here is again, look.
it works really well. The simple idea
of doing an auto encoder. But then, doing this kind of noise you can actually beat out. For example, our voi ems.
Amazing. Okay.
so what does all this mean to you?
They had not a whole lot.
Let's say, that's a terrible answer. It should be a lot. Let me tell you what it means to you, number one.
It shows you the power of these new methods
in this case. Very simple method based on neural network architecture, which you know just to make your stuff simple. Okay? Number one, it seems to work really? Well, okay.
Number 2, you'll probably see something like this on homework for.
and number 3 on the final. I will 100% ask you a auto and order question
where I will give you a matrix.
It'll be very simplified. I will give you like. I'll probably do the traditional like auto encoder, and I will set it up for you, and you will have to show me you really understand?
Autorrect.
Okay, so all the stuff we talked about last time and into this time lock it down. Guaranteed
you want to graduate.
What do you mean? Maybe you want to graduate?
You don't want to graduate. Yeah, we were just talking about the job market. Yeah, you guys should take the Kinesiology classes and the wine tasting class. Is anyone taking wine tasting right now.
I know I know people who do it. And I told you this. They're like, Okay, I gotta leave the meeting early.
Had to go to wine tasting, and they have their little. They have like a little box. Have you guys seen the box that has like their little glass, or something? They have like a little like wine tasting kit.
You got to put in some effort. You guys had to drink so much wine. It was so terrible. It was horrible. Okay.
short and sweet. I just want you to show you this stuff was in the air to auto-encoder-based methods. Definitely understand auto. Rec. get the idea from the second one of denoising or of adding noise.
Okay. really, great performance. cool building block that you can use
for lots of your recommenders. So on your project. For example, if you wanted to add some special sauce, you're like, I need to do some user collaborative filtering, you could do a user base auto encoder to get a user embedding and use that as an example. Okay. Now.
second topic again, same time. Now, we're 2017.
Okay, stuff is cooking.
That's our void, Ben. He was faculty member here for several years, has has now gone to rice. Good friend of mine collaborator for many, many years. Okay. he's working on this thing called neural.
collaborative, filtery. Right? It's in the air. Right? Take collaborative filtering, add neural networks. Okay.
this thing has been super super influential. Now, I'm just showing user citations over year. This is unusual.
Okay, you're like, okay, what does 4,000 mean? It just means like, this thing came out. And it's doing gangbusters. Everyone is like, this is the thing.
Okay, this is like incredibly incredibly hot. So I'm going to give you just the flavor of what they're trying to do. which I think is quite neat.
Remember this picture. Yes, yeah, do you?
It's like one of those. You know those movies like changes, your whole perspective. What is that? The Hyperscore was
exactly.
it was all online. Okay, so we have
user, this is, remember, we call this user link back? Or what's another word for this thing right here.
What else we call it embedded.
People talk about embedding all the time, and I was out there in California this week. All everyone talks about embedding, embedding this. That is, they user embedding
item, embedding the way we found these, we did matrix factorization. big matrix chunk it in the 2 submatrices.
These fall out. And if you do the dial, we get out an estimated rating for the user. Okay, that's the dream. Right? This is traditional. MF, right?
The idea of this paper is what if we learned the similarity function? This is kind of like brain bending a little bit. Everything we've done so far is that?
But maybe they're thinking this,
so imagine there's P and Q,
that's P. And Q, that's like the user embedding.
This is the item
embedding. We found it by using matrix factorization. They basically say, what you're showing here is like some big fancy model. But don't worry. The whole point is this thing gives me user embedding.
This thing gives me item embedding. And at the end of the day. we just dot product. So
we know this, we know this.
this is cool.
This is a, you know, just a, vector. We have user ready.
That's what we do know how to do that right?
They say, what if instead
have user embedding item embedding? But now I don't dot them.
I run them through an Mlp a multilayer perceptron, a neural network
that somehow learns a function to combine the 2. And so what I mean is, this is still my user embedding.
This is still my item embedding.
But now this is basically fancy neural network to learn how to combine.
That's the idea.
Okay, so if you go back and look at the simple version here. Imagine, instead of just saying 1.5 times 1.8 plus 1.2 times 1.8.
It learns some crazy thing, which is, oh, take the first feature, multiply it by 80% of the first item, and then subtract out a number.
then do this thing. And then it's like this very crazy function that you and I wouldn't know what it is. It was somehow learn this crazy function
to combine these things to then give us a better rating than we get just doing dot product.
make sense.
We're gonna learn. We're gonna learn how to combine these things instead of just doing this times that plus this times that now it's some crazy function.
Okay? And the idea is we don't know how to design that crazy function. So we let the neural network learn the crazy function for us.
Okay, that's cool. Now.
Oh, yeah, yeah, yeah. Yeah. Awesome little thing just to kind of take you guys to the next level.
We know how to find the user embedding. I'm embedding. We do. MF,
you don't have to. You could have used autoreg to get user embedding and
autoreg use item embedding.
and then you combine them with dot product or anything. Vance.
this is the big big idea. Okay.
intuition. That product is fixed. It's not flexible.
There could be some nonlinear interactions. Yeah.
And if we learn the similarity function, it should be better. Okay, so pause it.
Now we're going to switch gears and say.
and I told you, let's see, like push back.
We saw that on the retrieval side. Right, hey? When Jimmy Lindsay wait a minute. I went back and tried all your methods, and they lose to like a simple method. What's up with that
same thing is happening here
where people say, wait a minute. Does this stuff really work? Okay?
So here we are, 2023 years later.
Okay, neural, collaborative filtering
up. Oh.
versus our boy matrix factorization
revisited. they have to go to.
They're gonna say, basically, what's up? Does it really work or not.
Okay. So this is what's interesting. They say, look, we went back and they said. learn similarity functions like, we just saw they're cited as the de facto standard. They're the best
strong baseline, they say in this paper we're going to revisit those arguments.
They show they are flawed.
I mean, just so, you know, like an academic, this is like,
flame. War! What is this thing like? A like a disk track like when Drake put out a disk track about
some girl. He lost a a granny, too, like 20 years ago. You saw this.
It's week. Bro, it's week.
Sorry is Greg gonna come to midnight yell, or what?
Right?
You don't know. Drake was supposed to be there. That was. That was the that was that was a couple of weeks ago. You know what I'm talking about.
Everyone was talking so Drake has bought a property near here, you know. This
Drake made his name in Houston on the Houston scene. So he's been to Houston. He knows it. He bought some fancy property between here and Houston, some ranch or something. Okay?
And so then Johnny Manzell put out a thing
I know what's up.
All these things were converging, and it was like, there's a chance Drake is gonna show up.
Did you go? Were you disappointed? Oh, no! Did you go? I mean, yeah. Drake did not show up, and it was very disappointing. So apologies
that was real. But now they're sort of holding out hope, maybe
because again, he's got property near here.
Okay, so imagine Drake puts out a dist track against who
against
someone else. Nick. No, I don't know these people. I don't know any Drake song. Sorry
I just I just never.
I only know he's with. I'm a swift tee.
Okay? So here's the setup.
They're gonna follow the Ncf paper. Use these data sets. They're gonna try and MF versus Ncf. And what do they see?
I'm gonna
okay. This is hit rate. This is like, kind of like a precision. Metric. Higher is better.
The embedding dimension. How big are the embeddings. Okay, more and more and more more capacity. They say this is dot product blue. Then they say. here are different versions of learning.
all weaker.
They say, here's in Dcg, you know, in Dcg, same story, dot products. Here are learned variations worse.
this that was on a movie lens. Here's a pinterest data set
again, dot products
all the learned functions. And on Ndcg dot product learn functions.
What's going on? Yeah. Could have could have
what's going on here.
What do you think? What? Why, why is this happening?
Do you understand? What's happened is like they're throwing like big punches right now they're saying this thing that you all think, is it? Is not it okay? And they give some more like theoretical analysis. For why? That is.
But basically they're saying, Yeah, it performs poorly.
What? Why is that happening? I'm gonna put that back to you guys. What do you think's happening there? What's your intuition?
The
it could be maybe the original paper was recording good results, and maybe they were recording over fitted results.
Maybe they Cherry picked. It doesn't mean they were bad people, right? But it could have been there could have been some train test leakage.
There could have been some some kind of experimental setup that kind of caused to look better than it really business, saying it is. Another thing could be that this paper you mentioned. Maybe they pick certain data sets where works well, they don't see the data doesn't work well, could be.
could be okay.
It could be that in the original paper the Ncf. Method was right. But maybe their implementation of MF. Was not right.
And so it showed it to be weaker than it was. Okay. Because again, we gave you guys MF. Code
for your homework 3. And it's a lot of code. and it's very easy there could be some kind of data leakage. There could very easily be some kind of error
that could be propagated. Okay.
the main thing I'm trying to show you is.
after all, the hype, deep learning hype, 1,51617, 1,819, you start to slow it. Wait a minute. What's up? So in the same vein
the year before was this paper? This is also, this is, sorry. This is the best paper
that, Rexis. Are we really making much progress? A worrying analysis. a recent neural recommendation approaches like what the heck, just like we saw on the retrieval side. Same thing happens. Okay?
So I'm gonna use their slides. What these guys do is they say.
okay, in other fields of machine learning, deep learning, state of the art algorithms are not as strong as expected. So what they want to do is basically do a reproducibility and evaluation study of all these deep learning methods in recommendation.
Okay, let's see if it works or not.
So what they do is they go try to find published algorithms. So they go find these conferences. This is where lots of us work is published over many, many years.
and they're all trying to do so. They have some sort of common setup. They're doing like a top in. They make a recommendation of like a top K list. Okay?
And they take all of these. Okay. they all. They also want to make sure that you can actually get the source code. This is another issue. People say, Hey, this cool algorithm? And you're like, Great, can can I get the code, you know, like
Number one, they don't respond to your email or number 2. They point you to their github and you go to Github. And there's nothing there. Okay, this happens a lot. So they did is they said, okay, read the source code.
We're gonna use public data sets of something secret. It's not available. They're gonna ask the authors and then wait a month. Okay.
reproducible papers.
Yes.
so this is saying, like, at 8 or 4 papers, 3 of them they could actually like, run the code
from Rexis. There were 7 papers that could only get one of them to run.
either. The authors didn't provide it. It didn't work.
etc., etc. So you're talking like 39 were actually, they could even begin to reproduce.
So this is a big problem. We, as a community are trying to fix this. They're like, if we're trying to advance science and advance like our understanding. You gotta share this stuff.
So now that typically like, whenever we do work, when we submit the work. We include a link and a link goes to code to funds it.
So no one has email you they can't just look at. Otherwise you have to trust.
Trust me to verify. I don't want to trust you. It would be like, for example, on your final project.
You're like, Look, Adam, it's really cool, and it's like this, totally light.
faked out Demo. And like you're hovering the mouse over, you're like, Click.
right? So you're showing me a video right? I would say, let me see the code in this case.
These are people who are doing work that's supposed to be in the public interest in like public science.
You need to get your coater. What is that about things? Okay? Then they say, Look.
here are the reproducible papers they can get up to. I showed you what autoreck, CDAE in Cf.
you can just see, there was lots of other activity. Cnn, Cbae, Cdl, all these different variations. Okay.
a bunch of them. What they see is also important is that these papers refer back to papers before them. So like
this one will refer back to these guys, I'm sorry these guys refer back to these guys. So it's kind of like, we're building science on top of science. And so if the foundations are weak. Then we have big big problems. Okay?
So they do the same experimental procedure. Okay. they use the original hyper parameters. So like all like the matrix factorization. All those parameters. They use the original ones
and their baselines. They do kind of like a sophisticated like, search for good parameters. Okay? But they're doing, not deep learning baselines. Okay? So here are the baselines again, what's cool?
We know that
we know that
that's just item cloud and filter using in years. Neighbors go find the 3 closest items, the 3 closest users.
Okay.
you don't know these 2, but these are kind of just variations.
kind of built on traditional concept. These are again not deep learning. These are not fancy.
They do some hybrid model, and then they consider a very like non deep machine learning
called slim. Okay.
let's go look at what happens.
So again. the bottom one always is going to be the fancy version. This is on some data set site you like. These are a bunch of quality metrics, you know. Ndcg. what they're saying is.
here's their number. Here's my item. Cnn, better better was better.
What?
Let's look at pinterest. Same thing.
Right? Right? Right? Right?
Let's go eat another data set.
You understand how popular is.
whatever's most popular, recommended Taylor Swift for everybody
out of this spot. That's my admiration.
How do you do like hair on fire? Something like this?
Okay.
here's a different one. CVAE. Don't worry about CVAE. But again, some variational auto encoder
again. These, oh, if there's something that's doing poorly. But some item handed variants and the records.
Okay.
this is at Ncf, we were just talking about
again. Same story. just an item based pan and beat it.
Different data set.
Here is a machine learning a simple, this is a
this is simple.
Again, this is fancy. And again they're saying, Oh, if you wanted to use machine learning. A simple method actually beats your fancy version.
Okay, so
and there's more and more. This goes on and on and on. Okay, so they're basically saying, What's up with that? Right like, why is that happening? This seems like insane.
I say, number one. the original work is comparing you against weak baseline
or 2
poor tuning of the parameters. So again.
you guys use item based, collaborative filtering. What hyper parameter do you have in item-based collaborative filtering?
But what the thing you had sort of Meta decide to set before you even run the algorithm
for item-based collaborative.
Let's think about that for a second. So consider. item, cf.
what do you think about like this kind of item k, and n. what's a parameter I need there. And
okay, like.
how many neighbors?
Also in the design of that, you have a similarity function. What similarity function do you guys use?
But you could use something else. So you can consider, K, could be. It could also be like what similarity function.
So these are all like hyper decisions. These are all decisions you're having to make they go into the design of the method.
So if you ran, if you took one of our off-the-shelf methods like, item cf. and you pick K. Is equal to one or K is equal to 3. You're gonna get different results.
They're saying is, you want to be very smart, because if you pick the right parameters you may get better results, as they're saying is against these comparison. These baselines they show kind of poor hyper, parameter tuning. That kind of gave bad results.
It would be like
if I gave you in the afternoon based on that.
So, for example, the hyper parameter there would be like, what time? What day of the week do I give you to quiz?
So imagine I give you a quiz Saturday night at 3 Am.
I'm gonna judge you for this semester. That's not part of their.
That's when you're like, you're like the peak performance. Right? I can do this. Let's go right. Okay. If you're like me, I'll be sleeping. Okay.
But I'm saying I'm still judging the same, you the same algorithm. But I'm tuning the parameter when I evaluate you, which affects the output right? That's the whole idea. And so the point is, they're kind of comparing to like your sleepy or drunk friends instead of feel like your peak performance. Okay.
They also say in the original papers, there's a number of methodological issues which are, you're basically doing some kind of train test leakage. Different kinds of like experimental goofs
would make sure your results look better than they should have been. It doesn't mean you're a bad person. It just can happen.
Okay. So
they say, Well, how do we move forward? You know, they're basically saying to the community, now, this is like a big wake up call, Hey, we're screwing up. We gotta do better. We need better baselines. We need better reproducibility. So, for example, you know, basically virtualize it. So I don't have to figure out exactly how you're running this thing. I can just take it out of the box and run it.
also be a little clearer about the the the design of these things. Okay, so
this gets us all to the big pushback. Now.
where are we?
What is all this? There's my thing. Yeah, here we go. So where I want to take this. Now, as we wrap up in our history of Rexis. Okay.
check. We know it
enough. We know it.
Okay. Ml. Starts to happen. We know a little bit about gradient descent. deep learning, revolution, recommendation. We know auto and rec.
we know a little bit of CDAE. We know a little bit of Ncf, okay.
I'm skipping over this topic for now. But we've now done the big pushback. Does it really work?
Is it a problem? Okay? And so in terms of again telling your parents or telling your friends where are in the world? Are you?
You are now basically, t020-19-2020,
like when you started college, this is what we're talking about. When you were finishing high school.
we're having these conversations. Okay. Now, the big move is Llms plus rexis. Right?
Can we do better? I'll tell you, like internally, like deep learning. Just so, you know, there are some methodological issues with the papers I showed you but 100 guaranteed. All these big companies are using this stuff like crazy. Because guess what? It really works. Okay, it works in production. It works at scale definitely.
The big inflection point now is, can we get Llms to help us do Rexis?
Okay.
that's the big deal
cool. Do we have a game this weekend? Some of your faces immediately went like, Yes. who do we play? Sounds like Caroline? Where do we play here?

What's up gang? Good morning.
Campbell II, Nicholus D
Morning.
Yeah. Zooming baby. At the old days.
What's up, Alex?
We had 2 ounces, and in the house today. Nice.
I'm real curious how many people will show up today, cause it's kinda weird, you know. Zooming.
we will see.
Or if anyone's in the classroom that would be great.
Yeah, Alex has got to stick together. It's very important.
Third, Alex in the house.
What's up, everybody? We'll start in just a minute.
People are confused. They don't know where to go. Gotta get on the zoom
stuff.
Campbell II, Nicholus D
How's the weather over there?
It's very nice. Yeah, yeah, it was really chilly, you know. Kinda cool this morning in the fifties?
yeah. But yesterday was like, you know, 70
crisp.
It's all good folks. It's all good. What's up? Gang
Will Miranda
cool.
alright good to see everybody nice to see you all.
Let me go ahead and share my screen. Here.
let's hope this thing will work.
I haven't done this in a long time, you know. Oops
nice.
I've lost my hang on folks.
I lost my chat window
chat.
but no one has their camera on. I was like, I've got the thing pulled up, and I can't see anybody. But I guess that's normal. we're not about
showing ourselves. It's all good. No sweat.
hey? What's up, everybody? I'm watching the chat right now. So if you want to throw anything into the chat. We can do this kind of like a twitch stream.
We can respond that way.
it's nice to see everybody. As you know. I am in a secret location.
Which I can tell you about when I'm back in person. Very exciting stuff all my secrets.
we're gonna kick it off here. Yeah, like, always. We start with our questions, concerns, and comments. Pro project feedback
from your peers was due last night. If your team hasn't done it yet.
It's fine. Go do it today, so get it in, get get it done!
Get it done. So if you haven't done it. Hop in there. Also the Ta. And I have been reading all of the proposals, and we will be getting you some some feedback. I'll probably finish it up on the flight back tomorrow. But I'll let you know. But I'll say overall, there's some really exciting projects, and I'm super excited, so I hope you are, too.
Homework 3 is out. I've seen a little bit of activity on the on slack looking at the homework
highly. Encourage you to take a look at that. It's a fun one, but there may be some tricky parts, and I'd rather you get it done
sooner rather than later. Any comments, questions, concerns. I'm looking in the chat. Also you can unmute and just yell out if you want to say anything.
What's up gang, or just questions about what's going on in the world. I'm happy to talk about anything right now. You got me.
I need something, guys. This is tough. I'm in. I'm in with a little box room here, and I'm talking to a screen.
I need some feedback. I need something. There it is. Thanks, Drake.
Yeah. How's life? It's pretty good yo axl. Thank you.
I need it. It gives me power. Gives me energy.
Is water wet, unclear, right? I mean, if you're in. If you're in water. Are you wet? I don't know.
Thoughts on the World Series don't really follow baseball. But
I wish I think Philly did not make it. I think they're. I think they're pretty good.
so it's not good. But in terms of the Texas teams not going to get there. How was my golf. It was all right. It was all right.
I'll show you some pictures when I get back. It's pretty cool.
thank you. That's what I'm talking about. That's what I'm talking about. Alright, let's get into it.
It's very exciting times. I'll just tell you this super energized
everyone I'm talking to right now, super excited about recommendation, all like the hot hot stuff it's happening. So you guys, we are
looking at the good stuff right now as a reminder where we were last week.
We were looking at some of these like Co quote unquote early ma days of neural methods.
So we talked about word to vet is like a foundational way to represent and learn about word embeddings. That's gonna come back probably next week. Okay.
we saw how we can incorporate those word embeddings into a retrieval scenario. We looked at Desm
That's pretty cool.
We quickly sketched over some other approaches just to kind of give you the flavor that there's a lot of activity. And then we focus a lot on how it turns out, a lot of those methods weren't really working well when compared to well tuned, like non neural baselines. Okay?
But then there was this like rise of these big language models that suddenly made it all worthwhile. So put a PIN in that we're gonna come back to that next week. Okay.
what I wanna do this week today and Friday. I'll be back in person on Friday
is to go back to recommendation right? So we're kind of wearing 2 hats. This semester kind of our search hat and our recommendation hat
to go back to recommendation and see some of the same themes in recommendation where there was, like all this excitement about neural methods. And so we're gonna talk about one. In particular, we're gonna talk about auto encoder and then we'll also do these these other methods that are out there sequential methods. We're not gonna probably have time to get into that. But then we will get into the hype cycle and push back on that which will all lay the foundation for our big, exciting move into
large language models and their impact on both search and recommendation. Okay, so I showed you this figure some time ago. talking about kind of a history of recommendation, I said, Look, you know, it's really not that old when you think about it.
you know. Item, item, cf, that we talked about I mean this 2,003 again. Not that long ago
Netflix Prize.
which again gave us MF, and very like, widely expanded. Mf. that's 2,006, 2,009. Okay.
and this is like an academic discipline. You didn't have your first kind of like academic Recommender System Conference until 2,007, which, again, is not that long ago that people were really kind of coalescing around this concept.
So since many matrix factorization kind of kicked off 2,006 to 2,009, that kind of sparked this idea of hey? You know what we can kind of optimize. Remember, we did all that stuff with.
and some of the sort of kind of baby steps into doing gradient descent. But this idea of doing some kind of like learning all those parameters.
So basically, that kind of kickstarted, this whole like rise of Ml.
And people started figuring out, there's all these Ml. Techniques we can use and really exploded in in excitement. At the same time, you also had lots and lots of content, and when I say content, I mean, you know, the rise of all this kind of like, you know, the web had already been there. But
kind of social media. huge amounts of like mobile. And you know, interaction data that suddenly, like there's all this new content that we could use. coupled with our machine learning to do all this exciting stuff. Okay.
deep learning hits 2,012. It takes a couple of years. And it starts to get into recommendation. So we're going to talk about one of those key methods there which is going to be this auto. Rec, okay?
So this is how we are today is kind of in the mid 2,000. What do you call it? The 20 tens 2014, 2015, 2020, 16, and then we'll get into a little bit about the pushback.
Hey? Wait a minute. Maybe this stuff doesn't work. And then now the kind of the new era where we live in right now, which is, hey? How can these large language models help us do recommendation? Okay? So stuff is super hot. You're right on
the bleaning edge of what's happening in the world.
okay, cool. So just to kind of give you a perspective again, you don't need to know this bit right here. I'm just sort of showing this to sort of set the foundation. This is a survey. So this is acm, which is like our big like professional society. This is like a computing. And it's computing surveys.
And basically, this is like, these are these surveys that kind of summarize a whole area.
I'm just showing you this because, this is about deep learning, plus recommendation and just trying to give you a sense of like, how much stuff was going on. Okay? And so what I'm showing here is just lots of different kinds of methods. And these are all. Just
these are all just like different papers using these methods. So this is like multi-layer perceptron autoencoded. We're going to talk about one of those methods. And there's like, I'm gonna count that there's like 20 different methods that do it.
These are convolutional neural nets, recurrent neural nets. Okay, restricted Boltzmann machine dot dot dot all these different ideas and methods and just boatloads and boatloads of people trying them.
So if you think about like traditional matrix factorization that would be like one method. And now, on this slide, I'm just showing you like another. I don't know if this is 80 new methods that are all trying to do some of the same kinds of ideas.
and just sort of real explosion and excitement around this
and not just applying it to like ratings like we did.
but also looking at like for text images, audio video graphs and networks. Considering sequence information, you purchase this, then you purchase that.
And so again, just to show you you don't need to know all this specifically, but it's trying to give you the flavor that there is all this excitement around all this. Okay, that's why I'm kind of motivated. Why, we're gonna talk about one of these methods and hopefully get you excited about it, too.
and don't really worry about this bit. Right now, okay, so let's get into one specific method. Okay, this is called auto. Rec. this is an auto encoder for collaborative filtering. Hey, we already know about collaborative filtering. Right? We know, item and user, user.
okay? And so the idea here is, let's bring this technology, this auto encoder.
which kind of been popularized out of this neural network, deep learning kind of explosion and then bring it to bear on our collaborative filtering scenario. Okay? So again, I'm watching the chat in case you have anything.
also feel free to unmute and and ask a question or just pipe in. If you want to.
I will tell you a secret. The building I'm in
it's one of its I'm not gonna tell you the full name of it. But part of its name is gradient.
Just to show you how important gradients are
out here. They name a whole building after gradient. Okay?
Alright.
Any other comments, questions in the chat.
are you guys all just like.
yeah. The other part of it is not dissent. Good idea. But no, no, but it does have gradient in the title,
cool.
Alright, let's get an auto. Rec, okay, so we're gonna learn a new method today. Okay, kinda like, you have MF, in your pocket.
We're gonna do gradient. Sorry we're gonna do autoencoders. Okay?
So some of you may have seen this. Maybe not. I don't know.
But imagine this is what this is, sort of the sketch of what an autoencoder does. So auto is like self right? And so the idea is, you're going to basically take in this case originally, think about like an image. So here's on the left hand side. There's like an image of the the number 2.
I think it looks like a 2. Okay, that's the original image.
What we would like to do is somehow encode it and smash it down.
So imagine like right here. If you imagine we counted all the pixels there, you know, there's like, I don't know how many pixels are. There's a lot. There's a lot of information. And so basically, we're going from like a lot of information. And we're trying to smash it down into this compressed representation that has just a little bit of information.
and then we want to unpack it back into the original
information we had. And so the whole idea is like is exactly. It's like compression.
But what you're doing here is you're trying to. Yeah, you're trying to kind of compress the original thing such that you kind of capture all of the kind of important bits about the original image, such that then you could unpack it and return the original image.
This is definitely not. This is definitely lossy compression. So we're throwing away a lot of information.
So notice like here, this reconstructed, input you want it whoops.
The dream is, you know, these should be very close to each other. And so if you're just looking at an image. you know, they should look alike. Right? Okay. But the point is is, we are definitely throwing away information. So it is gonna be Lossy. We're throwing away stuff.
So we're doing definitely, we're compressing it down to something, and then we're unpacking it.
And the dream is we want to compress it. Cause if you imagine like there, there may be lots of like redundant information. If we cannot throw away the redundant information we can get like a smaller representation. Okay?
And so then, yeah, this is exactly like you've watched Hbo's entourage. This is exactly like Pied Piper and so this is the idea, right? So we're gonna take the original input smash it down and then unpack it. Okay? So you notice why we call this auto encoder, because you only take the original thing and we just smash it down and we unpack it. So we're somehow trying to encode it kind of just using information about itself to get something else to get the reconstruction out of it. Okay? So if you look at this.
you're like, what does this have to do with collaborative filtering? Right? Well.
remember matrix factorization? Right? We had this gigantic sparse ratings matrix, this sucker was sparse.
And then we basically threw away a lot of information, and we somehow tried to encode it in much smaller. Remember.
this is like m by n, and this is m, by K, plus m, by K, which is way smaller than M. By N, right? So the idea is we, we had some kind of like
compression.
We took this big ratings matrix and we smashed it into these 2 smaller, dense matrices. Okay.
so
check that out. So. And then we could use that right to then go and recover. So this kind of like, imagine, this is kind of like
encoding. We encode it down. and it's kind of like we then
decode it back to the original matrix. Right?
So if you think about it like that.
We go from sparse to dense. So again, it's like you could imagine somewhere in between.
we're doing some kind of encode.
and then we're doing some kind of decode.
And it kind of has the flavor of an auto encoder. It's not an auto encoder. It's matrix factorization. But if you understood matrix factorization. We're kind of doing something quite similar. Okay.
so
autorec is basically a new way to densify that matrix to go from the sparse matrix to densify, to put numbers in there in our estimated ratings for everybody.
And so we're gonna define it like this. And I'm gonna walk through all this and don't worry about it. But it's very similar to our matrix factorization world. where basically you can think about this. This is like
that's like the original reading.
This is like our I don't call it sort of our our reconstructed rating.
and the hope is, you know, make the make the distance
small if we can hopefully get them as small as possible.
We're trying to minimize that right? So Theta gave it some model parameters. But the idea is just like a matrix factorization where we tried to find those P. And Q matrices such that the error was small.
Same exact idea. We have a bunch of ratings. We want to like, cook up new ratings. We want them to be as close to the original ratings as possible. Okay.
so if you go back to our picture of all this. It's just like, instead of having an image. Now, it's like, this is my ratings matrix.
This is my sparse ratings matrix. You know we already know about that.
We're going to somehow encode it into some compact representation.
We're then going to unpack it. and then the hope is, you know, the dream
is that these 2
are close to each other. Okay.
so that's it. That's autoencoder.
Okay.
high. 5.
I wish I we should do polls. Can we do polls? I think we'd have to set it up early earlier.
do you guys? I mean, I kinda like zooming instead of being in the classroom. Maybe we should only zoom from now on. All in favor.
No
immediate response. No, no, okay.
fair enough, jeez. I can try.
I can try. Yeah, the shirts. Oh, yeah, yeah, we're gonna see about the shirts. Okay? So that's the setup auto encoder
questions about it. I'm going to pause here
freak out's worries.
Samuel Smith
Quick question. You're probably gonna go into this later. But why?
Why do we care about encoding and making it getting spicy? We can do all this. But who cares?
So check this out. What do we get? This is why we care number one. we can densify the matrix
meaning. We can immediately use those estimated ratings just like we did in MF, okay, but 2, what's nice? This is really cool
in that compression. We can basically learn like compact representations of our users or our items.
So just like just like a matrix factorization when we have our dense representations for users or for items. and you can use those
like immediately, like in the homework. Right now we ask you, we say, Oh, go look at the dense representation for the movies and find other movies that are similar.
Right? Maybe you looked at the homework. Maybe you haven't. But the point there is like you're able to then look at movies in that encoded or dense space.
And you could find, for example, some non-obvious relationships. Okay, same deal. We can use this auto encoder structure, just like MF. Can also give us these dense representations for users and items which we could use
just like directly, or we could plug them into another recommendation model like, like an item item or a user user collaborative filtering.
It's a super super great question. And we're hopefully you'll you'll see how this stuff all falls out when we go back to densify.
This is some real data. I'm going to show you just a second how it works. Here's like a real. Here's a real sparse matrix. I have
sorry there's users. These are items. You know, this user rated that item of 4. If you use an autoencoder? I bolded the ones that are the the originals.
So the idea is like, I guessed, 4.3 6. The actual was a 4,
you know. Let me use a different color.
like
the actual was a one, and our guess was 1.2 3.
But again, what's cool about this is the user has not rated this item. But we can guess that item would be really low.
Or this user has not rated this item. We can guess that item would be really high.
So again, the whole cool thing about this is, it allows us to recover all those ratings which we can use directly, or you'll see we can use kind of the compressed representations directly.
so the thing I'm gonna ask this question is, wait a minute. So the dream is to make a more efficient recommender. So we don't have to use the whole fat.
A sparse matrix kind of yes, and kind of no the idea here is, you know. The point is, we want to do recommendation. So I don't know. Like for this.
for example, this user right here
like which one would, they prefer that item or that item?
You know, we don't know, based on the original ratings matrix. But the idea is by doing this encoding, we can come back and say, Oh, I think you'll give that a one. I think it'll give that a 4.
So I know which one you would prefer. So the point is by doing this encoding, decoding.
It allows me to understand
the relationships back to traditional collaborative filtering. Exactly. Exactly. But also you'll see it allows us to get these nice, dense representations which we can use to do other cool stuff. Okay.
okay, so now, we gotta figure out how auto rec really works. Okay?
all right. So you give me a ratings. Vector
are you give me a set of rating vectors. So you mentioned. Like every renewal, row by row.
or we can be column by column. Okay. but we have to have some kind of function. H,
right? So this is our reconstruction function, but somehow guesses
in our case ratings such that the error is quite small. Okay? And now here's where it's gonna get a little funky.
And you have to hang with me. Okay. so check this now, because this is like, wait, what is this function, this reconstruction? Okay.
okay.
so here's the deal we're doing neural networks. Now.
the good news about neural networks. All we need to really know about is what a matrix is
plus some other goodies.
So if you remember all our matrix stuff, I'm gonna walk you through. So you don't have to even know this is a neural network. Okay
for our world, for our world. It's a 2 layered neural network. Okay, the same size, input and output. So original ratings come in same size. Ratings comes out and in the middle we do this reconstruction is this big funky formula, this big H, this is that H of X,
is that H right there? Okay.
And so h of x is F of G of x times, v plus b times, w on what?
So the main thing is.
we're gonna figure this out. F and G, those are activation functions.
We'll figure that out in just a second
V and W, those are just that's a matrix
B+B, those are just bias vectors.
And X, that's just our original, like.
ratings. Vector okay. a lot of stuff going on here again. Don't freak out.
I've got you. Okay, I'm gonna walk you through this.
We're gonna make it super simple. Okay.
let me give you a visual perspective. And then I'm gonna do like a like a matrix calculation perspective. So if you're a neural networks person, this will look familiar. If you're not
no worries, you'll get it on the next slide.
Okay? So the input here. So imagine, like my matrix. So imagine we're doing it row by row by row.
Okay. so like, this would be like.
there's one of my rows. Call it this X, vector, okay?
And the input is x, one x 2 down to XN, so like, let's go back to our original ratings. Matrix this one.
be careful how I do this. Consider this ratings matrix right here. We're going to go row by row. So the first row is like 4 empty, 5 empty, one empty.
Everyone's cool with that. So 4 empty, 5 empty one empty.
So we're basically going to say, it's like, I have this X, which is like 4. I'll separate like that. Md.
do we say 5 empty one empty. So that's literally just that first row.
Okay, that's the first row
of my ratings.
Matrix.
Okay, that's X.
So if you think about it, if IX one, that's just 4
x 2 is empty, then it's 5, then something. then one, and then exit in or exit
6. That's like, X, sub. 6 is empty.
Is that cool?
If you're like, yeah, okay, cool. So that's the input, right? And so you can imagine what we're going to do is we have all of those guys
remember the function here. It says, Okay, take X,
that's
the XI just showed you multiply it by V. Well, you can think about that as like, I'll show you this in a second.
This is kind of like this V multiplication.
And this says, add
B, one. So we're gonna end up having it. Some kind of bias node down here, and we're gonna add it in
up here. and if we did all of that we'll end up getting some kind of like this will be our compressed representation.
So just like in the image, we have this big image, we smash it down to some smaller version. That's what we're doing here. We're taking this big ratings. We're smashing it down.
Okay.
there is this activation function. G that we apply here. I'll show you how that works in just a second. Okay? Then what we do. So imagine it's just like 2 things. Then we have.
we take this whole thing, and then we multiply it out.
started to draw this by W. There's your W. And then we add, in that B 2, there's your B 2, and if we do all of that and apply some activation function. The hope is we recover on the other side
the output this.
So you know what those ratings should be. Now, if we go back, I actually ended up getting like. what is that? 4.3 8? I'm gonna write that at 4.9 0.
So like 4.3 a 4.9 0 dot dot down to what was the last one
1.9 8.
And so the dream is.
we're going to take our input, we're going to multiply it by V, we're going to add in this bi, we're then going to do activation.
We're gonna take all of that, multiply it by this matrix. W add another bias, activate all that.
and at the end of the day, hopefully, it's going to give us
our original ratings. Matrix in this case, H of X, which is close to X.
Okay, so notice, this is H of X up here. which we hope is close to X.
That was the whole deal. Okay.
So again, this is sort of visual, look at it. If you've done neural network. So we say, it's a 2 layer neural network. That's the idea. So these 2 layers. Okay.
you guys are the best. By the way. you're like cab. If you say so.
I don't really know what's going on. But sure, sure. So let's do it. Matrixe, wise. Okay?
So I think that's a little easier. Oh, yeah, yeah, I forgot activation function again. You've never taken a a neural nets. Class AI class. Ml class activation function is just you give me some something in and you change it out. So like the identity activation
is, you give me an X, I just give it back to you. So in other words. you know f of one is one f of 2, you know. 2 etc. So that's the activation.
the sigmoid is, you know, this is the sigmoid.
It says you know, it basically transforms. You know you give me and you gotta go plug some numbers in. But you plug in, you know, if you plug in you know 0, it'll be one over one. Okay, so
you you basically just
yeah, sigma weight. Sigmoid kind of looks like I can't even draw it.
And let's do a step function. That's very easy. If you give me one.
2, 3, 0. So if you give me one.
it gives me one.
If if the 2 you give me one, if it's 3, you give me one.
if it's, you know, negative one, you give me 0. If it's negative 2, you give me 0. So it's just a way to transform an input to an output. Okay.
that's all. It is okay. So if you buy that.
it's just a way to do some kind of transformation. Okay.
thanks. Hold that thought. Now.
I've already learned. So I already did my optimization. So I'm hiding all that from you.
But remember, let me go find our formula again. There it is. Let me go.
Go. Grab this guy right here.
Do remember what our X was. Our X was something like. what do we say? It's like 4 empty, 5 empty one empty. Okay. So if you look at this
function here, the first thing we do is X, times. V, so we need to do x times. VV, as a matrix.
Okay, well, I already gave you the right here. It turns out I already secretly learned it for you. So if we did this X times that. V. So this is what is this thing? This is a one by 6.
This is a 6 by 2. So if I multiply all this out, it's going to give me a one by 2. And so if you do that, if you do, X times V,
and you go multiply all these numbers out, you're gonna get 5, 6, 3, minus 4.5 5 8 1. That's x times. V,
that's that.
Okay? And then I say, okay, now I gotta do what now I gotta add in my my bias.
Well, I've already learned a biased. I don't think I have it on here. Nope oops come back. But if I told you this, I said, if if that bias was just where do I have it written down somewhere, do I?
It's on a later slide. Let me go grab it. Hang on.
Yeah, there's my bias.
Yeah, there's my bias right there.
So if we added, you know. x times v plus b one.
So that would just be 5.5 6 3 plus whatever this point 1 one, it would become 5.6 7
8 minus 4.601.
So that's now x times, v plus b one is that thing. Okay.
now, we're going to apply our activation function, which is G, so we have G, times
x times v plus b one.
which is just G times 5.6 7 8 minus 4.6 0 1, all of that. And in this case we're gonna use sigmoid.
So we're going to use sigmoid. which is just. If you remember the last slide, one over one plus E to the negative X. So if you plug that in, if you do one over one plus E to the negative, 5.6 7 8 da, da. If you do that.
you end up getting 0 point 9 9 7 0 point 0 0 9 9 4.
Okay.
here's the deal.
This is what's cool about this.
This is my like compressed user representation.
It's basically saying, my original user representation was 4, 5, one, the sixth length
vector of their original ratings. I did all this magic.
It now is just a 2 dimensional. It's point 9 9.0 0 9.
That's my representation for this user. I've like, kind of compressed them down.
Okay, it's like a 2D user embedding.
Again, let's just follow our formula here. Okay. So now, all of that was just to get to
all of what we just did was we just did. G, all of that business is what we just did.
I just took X times. VI added my bias. And I then did my activation. And I got my 2D user representation.
Okay.
visually. What we just did was we did basically. this half.
I took my original thing. I multiplied by V, I added my B one. I don't really show I activate. And then what did I get out? I got out my compressed representation in the middle.
Okay. now, what I got to do
is take that compressed thing and then unpack it back out to the original ratings. So we have that compressed representation and notice on here. I have a W on the right hand side.
So that's exactly what we're going to do is now we take basically. let's copy this again.
paste it here.
So now we're gonna basically like UN, you know, unpack. Or you can can sort of decode. We're gonna do
Okay.
why are we using sigma weight instead of any other functions for? G, yeah, this is a great question. So I'll show you in a minute. In this paper they tried lots of different ones. These are. Yeah, pure design decision. It's like.
we'll try sigmoid. We'll try identity.
It kind of just depends on the data distribution you have. But there's not like a right answer. It's purely like you. Just you just pick one
and typically you pick one that's gonna hopefully give you the best, like the smallest error.
So so in our world, basically, what we have now is we now need to do our unpacking right? So we have this.
We have. We've done that bit right there. Right? Okay.
that's our all of that
is this 0 point 9 9 7 0 point 0 0 9 9 4. That's what that is.
So now we wanna do is take that thing
and multiply it into our W, because that's what our W. Is right there. So if you come up here. It would be like saying, 0 point 9 9 7
whoops, 0 point
0 0 9 9 4 all times W. So this is basically
G of XV plus b one
times. W, that's what we're doing here.
Okay?
And so if we do that, we're gonna get out of this thing. And if you if you work it all out you'll end up getting 3 point. I have it written down here 3.0 5 8
2.8 5 2.3 9 minus 1.1 6 minus 0 point 6 4 or minus 0 point 3
5. Okay.
So now we have this sucker. So that's times W.
Now we'd have to add in B 2, which is some bias. Well, I've already calculated the bias. So we'll add in this B 2,
and in our case it's going to be like 1.3 1 2 point O, 4 2 point O, 7 1.6 9 1.8 7 2.3 3
erase that.
and if we add that on to it, we get 4.4 4.9 4.5
0 point 5 1.2 1.9 8, and then we have to do F of all of that. But we'll say, F is just the identity. So it doesn't change it. Okay.
so
we did all this. So basically, this is finally, this is my H of X
is this thing here.
That's my recovered
representation of my original X, vector
so go back in time. What was our X vector it was
whoops
undo.
It was this thing.
So all we knew about the user is they rated the first thing 4. They hadn't rated the second thing. The third thing they rated 5 dot dot. So we went from 4 blank, 5 blank, one blank to
4.4 4.9 4.5 0 point 5 1.2 1.9 8. So that's the idea is, we've recovered. Now.
the ratings for the original user.
Okay.
so this we compress the user. And we and we decompressed.
Okay.
that's autoencoder. So I can take every row of that matrix. do the exact same thing multiplied by V, add the bias. activate.
multiply by this W add a bias activate
and I'll recover its ratings. Row
affected. Do I have it? Here.
So this is. We just did the one example. We did that, and we turned it into that
right, but I could then take in the second row. and it would give me out this row. and I could take the third row, and it would give me this. and the idea is I could go through. and I can then recover
all of the ratings just by doing this compression decompression or encoding
decoding. Okay.
so go back to our original motivation for this.
This is just kind of like MF, it's a different way to do it.
But in this world we do. We have the same exact thing. We get like recovered ratings. And so we can go use these directly.
Boom. Okay? And I see some questions in the chat. I'll get to those in just a second. We can use this directly. Okay.
alternatively.
Check this out. Where is it?
Remember when we multiplied our
we did our
ve x times z plus b one. And then we activated it. That was like our user encoding.
And we did it, and we got point 9 7.0 0 9 4.
I could do that for all of my users. And now I have, like a 2D user embedding or a 2D user representation.
And so if you just look at this, you're kind of like, well, golly, you know the first 3 users are all alike.
and the second 2 users are alike.
They kind of have the same interest.
Okay? So if you go back and look at the original ratings. It's like.
if you look at just these if you look at just these first 3
right, they tend to like the stuff on the left side, and they don't like the stuff on the right side.
whereas these guys. they don't really like the movies on the left side. But they do like the movies on the right side. even though they haven't all rated the exact same
movies. Okay? And so the idea here is we now have, like this, like dense
a representation of all of our users. Okay. so we can now do like user, user. Cf, right? We say, Oh, which users you, are you alike.
And we would say, Look, you know, this is something you're doing a little bit on your homework. It's like, you know how close is. You know, this user user one
to user 2 in the original ratings. It's like, well, they don't have anything in common, nothing in common.
They've only rated one thing in common, right? Whereas in the 2D latent representation, if we said like, how close is user one
to user. 2 would say, they're super close. It's like
they both love the first half, and they both have low values in the second thing. So they're super super close. So the hunch is, if one likes something, the other one will like it, and if one doesn't like it, the other will also not like it?
Okay? Now.
there were questions, what is the added benefit of having an activation function?
And also, why do we have a bias? Vector, right? And so mainly
so to the bias vectors, you're basically, it just allows you. It's like an an extra parameter that you apply to all of the entries which is just allows you to move them up or down, because sometimes just.
I don't have the sort of mathematical motivation.
But the idea is to give you some extra parameters that you can then apply across all of the users that are kind of beyond this matrix multiplication. So it gives you kind of like an added like tunable knob, which allows you to do a better job of est, or in this case of recovering the the ratings. Why do we have a activation functions as opposed to what normalizing outputs
as opposed to keeping the raw? It's not between. I don't not sure. I really follow the question.
maybe you can rephrase, or we can. We can come to it, maybe on Friday, too. so okay, so go back to the very start. And let's go back to the very beginning here.
Okay.
so.
okay, so sorry. Here we go visually. What we're trying to do is you give me ratings matrix.
I cook up a function. H,
that allows me to do the recovery. Okay.
it allows me to find the ratings as close as I can.
And we're trying to do then is minimize this theta.
These are some parameters.
Now, because this is not a machine learning class. Okay? And because you haven't taken a neural nets class. What I've done here is
I already did the optimization. Okay, I found the theta.
So what I'm saying here is like
these are the learned parameters. I gave these to you.
Okay, in practice you have to go. Use something like a gradient descent to go learn these parameters. Okay.
So I'm not making you do that. I'm just saying, imagine you have this structure. You have this input ratings. You can do some kind of fancy optimization to figure out what are the right learned parameters? Okay?
But for the purposes of this class, I'm just assuming we've got it. You've got it.
So now that you've got it, you can now hopefully understand and use it. Okay.
So again. here is the auto rec optimization.
And again, I'm saying.
Don't worry.
Also known as Acuna
Matada
chill out. Okay.
I'm just showing you this would be like the.
This would be the thing you would optimize if you were really like, gonna do this in the neural nets, or like advanced recommendation class for this class. Don't worry. But I want you to know it exists. Okay.
So at the end of the day
we have learned our parameters. We can go back and recover all of the ratings. We can also learn 2D user representations.
We can go back and learn item representations.
Okay.
and on and on and on. Okay, now.
oh, yeah, notice this also. we only have 2 layers. And so if you think about it as like a. it's like we have the original ratings.
And remember, we did this, and we did that. And then here are sort of our estimated ratings.
So you can see it's like one layer, 2 layers. Okay. you could generalize this if you wanted to.
To. Multi like you have, like a hierarchy. You could say I'm going to do this
to this thing and then down.
and then I'm going to recover to here and then out here, so you can make it like 1, 2, 3, 4 layers.
Okay, or you can make it even more layers. So
again, if you take a like a neural nets class, you can play with this like how many layers do we have? And then, as a result, you'll have these kind of like hierarchical representations.
Okay, this is one of the strengths and powers of all of these neural network models is you can build these like hierarchies of representations
back to our users. You could imagine you could imagine, like.
you have sort of the original ratings. May. Original ratings for a user. You have kind of like a course representation, and then even coarser.
and so forth. And the idea is each layer could be kind of have different levels of interpretation.
But again, don't worry. Now.
your question should always be, does this stuff work? Okay?
So what they did in this paper is, you said, Well, what is the right activation function? Is it identities a sigmoid? They tried a bunch of different ones. Remember, this is like an error. Lower is better.
And so they tried a bunch of stuff, and they said, Well. you know, if you use an identity for F and a sigmoid for G, you get a lower error.
Why, there's not a real like theoretical story. It's just like it worked. Okay.
this is one of their stories. Well, you know, this is, you should choose it this way, it seems to work the best. Okay.
they also say, well, does, is this thing better than just matrix factorization?
So here's like an MF. That you already know.
Okay, and this is their stuff.
And again, these are all errors. Lower is better. And so they're saying, if you use this auto encoder method.
Notice is smaller. They're
is smaller or Rmse is smaller. So this is really cool. There's saying, basically, like all that MF stuff we did in Netflix Prize
all that hard work. hey? If instead, you use this autoencoder structure. In fact, you can do even better. Okay. that was kind of like one of the big, exciting takeaways.
These are 3 different data sets. This is like data set one
data set 2
data set 3, they're saying across 3 different data sets.
It works. Okay.
what's next?
In fact, what do I want you to get out of this?
Okay, this may have been a challenge. We covered a lot of stuff today.
My main takeaway from all of this autoencoder
is like a replacement for MF, okay.
all you take is original ratings. You do some fancy matrix multiplication. Vector addition
activation function. That's the biggest new concept
at the end of the day. It allows us to recover the original ratings. Okay.
it also allows us to find our condensed or compressed representations for users and items. That's it.
Okay. if you've got that. I don't expect you to know how to learn those parameters. But I want you to understand.
How the process works. how to interpret the dense representations.
Stuff like that. Okay.
Questions. comments. free galaxy.
Also to the 31 of us. one of me, 30 of you
in our class of 91.
Sam says, How's Google? It's good man.
Had a good breakfast this morning.
It's good. It's very good. Yeah, a lot of smart people.
I've actually, I have a bunch of former students here. So I'm seeing a bunch of former students, which is very nice. They're all doing really great. So it's fun to see them. They're my babies, and they've all they've all grown up.
Same deal for you guys. When you guys get your jobs and grow up, you gotta stay in touch.
because I want to know what you're up to.
Any other questions concerns
life lessons?
No?
Well, folks let me just say I've enjoyed talking to you all this morning. I hope you have a great week.
I'll see you all on Friday.
Bye.
Samuel Smith
Thank you, you bet.
Later

It is. Thank you.
You have it.
and the advertisement of everything.
Oh, it's
aye alright!
Everybody is
like, really, who didn't care.
What. Where do you pay so for?
No, no, it's just like
that's good to see you big news.
Monday, Wednesday. I will not be here. We're gonna zoom in.
So that'll be like
specific. Right? So I'll be up early doing class, because.
big heart, okay? Yeah. So Monday, Wednesday, yeah. Zoom proposal. So we're having some issues with the with canvas. It's like, really like, not our best friend. So the plan currently is number one. Join the project channel on slack. Submit your proposal only to slack. Okay.
we can't figure out how to get canvas to let you guys do teams, or there's some issue. So at least our current plan is, if you submit on slack by the deadline, it's all good. We will then go in and make the teams for you after the fact on canvas. So it shows up. So don't freak out, if, like you, you're someone from your team, only one posted to slack, but nothing showed up in canvas. It's all good. Don't worry. You're not gonna get like being for that, even if it says you're overdue.
Okay. does that make sense?
But please, please, please post a slack. Okay, if for whatever reason.
you had a freak out, this is, when's this thing due? Sunday? Yes, Yup.
if something happened and slack, blew up. The main thing is email. Me, some make sure we get it. That's the main thing. Okay, I just wanna go back and mention about
the proposal. You have to do. Sunday post a canvas in slack. So we're saying, basically post it to slack.
I don't. You should be able to form teams. Maybe not.
I think we're having issues with this. But on slack project panel proposal, colon your project name, remember. Give your project team a good name helps me remember. And literally like
question, answer, question, answer, question, answer. That's what I want to see.
Team, name, and all of the participant names. Okay, project name, team member. Name's done it
cool. Okay.
today is a Friday, the twentieth, I hope. Yeah. Sunday's the 20 s
questions about the proposal.
Also, II told you guys like, please name me a little nervous
little nervous that you got some proposals that are like, why they don't exist yet
quite possible to the teams don't exist yet.
Make sure you get a team. I saw a little activity on slack with teams but 3
I can't guarantee. I'll look at it. But if you want to ping me the next 24 h.
But like I'm traveling all day Sunday. So I'm not gonna be.
Actually, I've you wanna know what I'm doing Sunday.
one year's way.
so I don't. I don't play golf.
But I'm getting up early to fly to California. My guy's gonna pick me up. We're gonna go to the Half Moon Bay Ocean, course.
And I'm gonna play around 18
question.
Good luck. I don't know how to play.
And these guys are real players like they look good. They have like the clubs, and they look at the right looking shirts and pants and shoes. Do they know that I know how to play? I know how to play. Come on, look at that.
okay. or
it's all good. I'll give you some pictures, though.
Yeah, okay, good. So we're cool. Project. We're cool.
Okay. Monday, Wednesday. Zoom proposal slack. Get it done
sick. They we're gonna basically wrap up kind of our advanced Ir. Next week we'll do some advanced recommendations. Okay?
And then my hope is, after that we'll do all the right community superfund.
So today, we may even finish beautiful weather. Yeah. Okay, sound. Using our word to back
the word embeddings. We know this.
You missed class on Wednesday. Go back and check it out. This is on your homework
again, centroids vectors, some like dot products, vector normalization. We know how to do all this stuff. It's very simple. I say simple. But maybe you look at that you're like, I don't think it's simple. If you don't get on slack, ask questions, I'll help you out. Main thing is, make sure you know how to do this on your homework now
where we left it last time we were talking about the in the out vectors remember, the end is like.
that's when the word is in a center word, and the out is when it's like the outer or context.
And so the idea we did that example like with Yale right like in the end. Wh. If you look at the in embedding the words most similar to Yale are words that kind of could replace Yale, Harvard, Cornell, Nyu, etc.
The most similar words, when you compare into out, though, are the words that are kind of about Yale, and it was like a faculty graduate, you know, dorms and whatever.
So there was an example I wanted to show you, just to give you the flavor of what's going on. Okay? So this is from the paper.
a little hard to read. They're showing you what is their Desm model score when they do in, out.
So they use the query as the in embeddings. The document is the out embeddings. And then what is an Ester, where they do in in. So the query is in the in embeddings, and the document terms are are based on the out, are also on the in embeddings. Notice, these are negative, but it gets higher, is more similar.
And so I just want you to see is that the first one is a passage about Cambridge
doing Softmax.
No, this is just in their Dsm world. So this is just there's no softmax here. This is purely just doing.
Okay. so check this out.
The query, is Cambridge.
Okay? So get the query Cambridge embedding, and then compare it to all of the words. So check it out. We have a passage about Cambridge. Cambridge is a university city. It's in England. It's near a river. Blah blah blah. They have a passage about Oxford.
Oxford.
Also a little town in England, also has a university. And so if you're just looking at like the words that are like
the ideas for Cambridge point 0 6 2 higher than point 070, that's good. But notice, these are quite similar. right? Because it's kind of like it could be the same thing.
If you look at the in out all of the aboutness scores. So it's 120, highest 107, s highest.
So what's going on. It's basically saying all these words, United Kingdom charters city status that tells you about Cambridge right
fastest growing manufacturing Science University. It's about that case, Oxford. But the point is, those are all words about kind of like a town that has like a university in it.
And so the point is, it's like, if you look at this. even though Cambridge shows that 5 here is 0 times here
they're in out. As saying, this passage is quite similar to Cambridge, even though it never says the word Cambridge.
Okay? Kind of cool.
So then they go down. They say, what if we just have a passage about giraffes? Okay.
So this passage is all about giraffes. So again, if you compare it like the query, is Cambridge still? So the passage about Cambridge is, first passage about offer in a second
draft is way lower.
That makes sense. Anything similar here. Negative, 162, even more negative. V to the N, okay.
So then they say, what if they took the passage about giraffes and replace giraffe with the word English?
So back in our vector, space world, if we query for Cambridge? Cambridge shows up a bunch here. We would match it
right because it has the word, but the point is, none of the other words are about Cambridge. They're they're about giraffes, right?
And so their point here is like, look
if you now, if you look at this in out. These had all words about like university towns, words about university towns. Nope, Nope.
even though the word shows up here, it's way lower. So that's cool.
The last bit where they just last one in the center word perspective. Yeah. So it's in all the document words in the end. So the last one is, they take a passage about Cambridge, but now they take Cambridge, and they replace it with
Giraffe.
And so it's cool about. This is the city of Draft University city and blah blah blah blah! All this kind of stuff. So the point is again. It never mentioned Cambridge, but all the words are about Cambridge. So again in the in out world it scores quite highly
right. This is like, not trick the fact that it doesn't ever, says Cambridge, but it's like all these words that are about Cambridge. So I believe it's about Cambridge, even though it says Giraffe.
Okay, that's just the main thing. Just to kind of show you what's happening here. If you actually dig into like the scoring function of why this stuff works. Okay.
cool. You can also see a little bit why it doesn't work so great. I think I don't have it on here. We talked about last time was in fact, let's go back and look at that for just a moment.
because that was so cool
is, they said, when you do ranking all the docs. The Esm. Was quite bad.
right. It was only good when you did, we said, When you do reranging, in other words, you go ahead and you run to get 1,000 docs, and then you just re-rank them according to the Es Zone. So that example I just showed you my, the theory is
like we saw like, if you query for Cambridge, Oxford shows up pretty high. right? Because it's not sort of optimized for Cambridge. It's optimized for the words that are like about Cambridge.
And so I think the argument is in just this pure ranking world. If you're looking for whatever it's finding all these documents that are not actually relevant. But they're kind of in the neighborhood because they talk about the things like the thing you care about.
whereas in the re-ranking world, bm, 25 definitely gives me documents about Cambridge. so I know that at least they have Cambridge. They're in that sort of space. So when I rerank them.
I'm kind of filtering out all the crap. And I'm just keeping the good stuff. I'm doing a better job at reranking, using the about this. Okay, that's where the intuition as to why this all works.
That was Dsm, you get to do it on the homework super cool. But you know what? We're not satisfied.
We're never satisfied. And it's like we're gonna go beyond want to go beyond. But
so Ds, and that was like 2016,
1520, 16.
So word event is like 2012, 2013.
Just think about how this stuff works. It's like
word of that hits everyone's like, Oh, what's this all about
people starting to use it a couple of years later. Microsoft's like, Hey, let's go. Use it to help us be ranking cool at the same time
you had the rise of all of this deep learning excitement. Okay?
So again, like, by 2,018, the deep learning guys have won the touring award.
Okay, so 2,015, 2,016, 2,01720 18. All this excitement, everyone knows stuff is working really well in like computer vision. Okay? And all these other domains. And it's like, can we incorporate those ideas and use them to help us do rink?
Okay, so again, this is not deep learning class, not even an Ml. Class.
So we're not going to go into the details of all that. I'm just trying to give you kind of a story right now, which is all this excitement, all this new ways? Right? It's not just ratio and simple stuff. It's like these very complex models. We can go do ranking and really kick butt.
Okay? So again.
this little bit don't really worry. I just wanna show you like examples of what people are doing. So we know, this is like our traditional setup. Right?
We get a query. We get a document.
We generate a query representation, oh, how do we do that? That could be a vector
tf-idf, vector in a vector space.
we generate a document representation that would be like a Tf-idf vector representation of the document. So now we have our 2 vectors, and then we do cosine.
Right? So we know how to do this. This is for traditional stuff. So this could be, you know. For example, Cosine. these guys could be tf-idf.
vectors, we're done. Okay.
this is old school. People started to say, What if we try to do other tricks. So one would be like, I've got a query and a document. We generate a bunch of features.
So this is like our learning to rank style.
You know where we said, oh, what are the features? It's like the number of words
the page rank, the number of clicks.
all these features. And then we we talked about. Well, you know, you could use some simple. We use some simple Ml. or you could do fancy deep models
on top of those features. So this is kind of what people are thinking about learning to rank. We have a bunch of features, but now, instead of using simple models, we use some fancy deep model.
Okay? But the the conceptual idea totally the same. Okay.
they need people do other stuff. And so I'm just showing you here, just like, there's all these papers. This is 2016, 2017. And don't worry about the details here, but it's the same idea. It's like
we generate some vectors and generate some vectors and generate some patterns. Then we do some deep network on top. Okay?
Or or maybe we use
we have a document text, we get a document embedding right? So this is kind of like de sm.
right?
We generate some query embeddings and document embedding, maybe using word to Vec, and then we do cosign on top of that. Okay, so kind of
here is where, like the this is like the fancy.
the fancy, I think right, what is that about
this is the fancy
deep models to generate the representations. And then just do go cosine. Okay.
so what's happening is
all this stuff is happening. Okay, 2,016, twenty-seventh and 28. Everyone's little crazy trying to figure out how to use these new technologies using deep networks
do the same old problems with all these face.
Okay? Again. Version banks. So this guy giving them comes along. he said, wait a minute.
Wait a minute. Let's go see what's up
really works. So I alluded to this last time. So in 2,018, he says, this is all fake. This is fake news. It's all fake. It's all misinformation of it.
So what he does is this. so I do want you to understand what's going on here.
All these new methods have been introduced. Okay, he says, do they really work?
So what he does is he goes and finds 2 recent neural ir papers, new deep learning ir papers.
He doesn't call out the authors because he doesn't want to embarrass them. It wasn't me, by the way.
okay. And then he takes a a bunch of traditional non big, deep learning models. Okay? And you don't need to worry about the details here. But like the you know about. And you know, it's just counting right.
If you remember the formula for being 25. There's some ts, some ids and some other parameters. But you're just counting words.
So this is classic.
super simple.
And this is basically, you know, sort of who word counting. Okay?
Then he has a couple of other versions. Again, don't. I can introduce them. But the idea is with some query, likelihood model, and some he calls it RM. 3. These are other like sort of I call them quote unquote, simple.
like, these are not big Ml, models. They're just simple. There's different ways of counting. Okay, that's all they are.
he says. Let's go see how these things stack up against these new fancy methods. Okay, let's see what happens.
Okay, so let's go straight to the results. Okay, so this is.
This is an average precision.
and this is precision
at 20 precision. That key. You already know that it higher, is better.
So. These are the. I believe these are the the results in the papers.
Okay?
So they said, look, we have some simple model. So again, these are quote unquote, simple.
and these are fancy.
and he says, Look. my fancy model
bonus awesome.
Betsy model dominates
the best.
Jimmy says, wait a minute. Wait a minute. I'm gonna go reimplement. All this stuff. and I'm gonna do it myself. That's what you see down here on the bottom.
Right? He went and implemented himself, and he said, okay, let me go find.
So basically you can. This is his version of this query, like in the model, the numbers almost say. that makes them happy. Okay.
here's his version of a little bit better still, losing. Okay. But then he says, Wait a minute. I can go. Take the add it to another simple model. And it turns out there's some like, there's like one or 2 parameters to combine them. And I'll go really tune those parameters just right.
Okay, not like matrix factorization when you have millions of parameters. Now, these deep models, where again, you may have thousands or millions of parameters, but like 2,
okay? And so he goes and tries it again. He does a couple of what?
What?
What? The heck he says. Look
what the Vic. he says. Wait a minute.
That's even better.
He says, my simple model. These are simple, but these are simple.
These are simple. my simple models.
This would be like doing some cosines is being in your fancy neural network.
What are we doing? What? What the heck?
This is madness. Okay, this was just one paper. It happens
he does it again. average precision precision. At 20, our boy being 25, invented in like 19 published in 1990, something counting based on the vector space model and the probabilistic retrieval model from the 70 s. Old stuff.
he says, well, here's the M. 25,
and then they have all these versions, so don't worry about details. Again. It's like, well, we had no X or a Y. We added these other squeeze out some version
that's like a 2 7 2. Okay? So he says, look. we can do this is like, basically the fancy model. Right?
This is simple.
that's fancy. And the best
same deal, he said, I'm gonna go do my own version. Okay? So first of all, here's the 25.
I think it's better than there be a 25 for some reason.
so maybe, that they were like, you know, I don't know what was going on.
But then, again, he takes his simple models. These are all simple.
And he's like, Wait a minute.
yeah, I'm actually blowing you out of the water
way. Better.
So twice, he basically says, my simple model dominates your neural model.
So now let me turn it back to you. What's that about
what's going on?
Maybe their their neural network wasn't big enough. It wasn't fancy enough. That's a good argument. Wasn't fancy enough.
What else is going on here?
You don't have to know the details here. You're just like, think about any kind of you know. Think about?
Hmm.
He took chemistry class or biology class. He runs an experiment.
What happens?
Errors screw up
that. Yeah, that's good times
right? It's like something that has like that low precision.
So I feel like this. So like one of the kids like in high school. it was like
89
8. Well, that rounds 8, 9, 5, which rounds doesn't work like that.
Okay, So what's one under? So one, you could say, like these sort of fancy neural models are not fancy enough. So why would these? Why would their baselines be so weak compared to this
one? They intentionally chose weak baseline. So 2 they didn't intentionally choose. We baseline better.
Okay, 3. They chose baselines, but they didn't try to kind of optimize them as best as they could. So that's his argument here. Like, if you do, plus is RM. 3, you do have to optimize like a parameter.
But is it you can imagine. Is it 40% of that? And 60% of that is my score, or whatever. And the idea they didn't do that.
Wait a minute. We've been going crazy trying all these methods. They actually don't work very well. And in fact, this is kind of like setting off a bomb in the community, saying, everything that you're all working on is trash
is a joke, is garbage?
Oh, no, this is a big deal.
Okay.
so that he has some caveats. To this he says.
well, you know his method. They're not taking advantage of any behavioral data. Click stuff. He doesn't have that. This is purely just text.
So you could say, Well, if you work at Google or being, they have that. So maybe these methods do work. And then the other sort of objection to his claims like, well, maybe the neural Mo models. These deep models aren't working so well, but they're really cool, and we should keep exploring.
and we'll figure it out
because well, we can always build those models on top of the strong baselines, right?
Or like your point was, maybe, is it significant? Is it enough? Queries, you know. Maybe this is just over a single. a collection of data. What if you considered others? Maybe it would work there.
But the end of the day. It's like it doesn't matter. The stuff didn't work. It's a joke.
So this is his party, boss. He says it is unclear to me
if you're just using text. He says, if you don't have behavior laws
like clicks. And all these kinds of stuff is is unclear. If these neural techniques are actually
more effective. Do they work?
This is like.
you know, Mike. this is like a mic drop.
It's like, this is a joke.
Okay? Does not work. Okay, it's 2018.
And a year later he takes it all back.
And so what happened in the intervening year? Okay, this. This was the main thing. What happened in the intervening year was this, he says, look also, it takes something to to write both of these right? So the first is like, you know, he's taking on like his community saying, you guys are doing all wrong.
And then, second point is, well, actually, it's like they were doing it wrong. But something else has happened. So now it may turn out that you're right.
Okay? And so let me see, there's a lot of text here. But I want you to read this.
He says, okay, I believe there's now clarity, deep, transformer models, heavily pre-trained. That's an important word
via language. Modeling tasks have significantly and substantially improved the effectiveness of retrieval, even in the absence of training data.
Okay? So we're gonna figure out what that means.
What did I get wrong. It was the assumption, thinking back in perfectly reasonable one. that an effective document ranking model needed to embed vast amounts of relevance judgments that we know about
either derive from behavioral long data or human editorial judgments. That's like relevant or not.
he says, beyond the scale of what any academic institution can afford. Okay.
we thought that we needed that. But he's saying, in this new world, we don't need that.
Okay, he says. Instead, the advent of models that are self-supervised.
They gave me the very premise of my arguments. Okay, the brilliance of the language modeling task.
If the text itself serves as the prediction, target, and thus the only necessary ingredients were large, unannotated test collections and lots of compute.
Okay, this is a lot of words. You may not know exactly what's going on.
So basically, what's happening here.
add Page.
is basically what he was saying is kind of like this. If you think about it like current models. You have a bunch of docs.
You have a bunch of queries. Q. One, Q. 2, q. 3. Zoom in. You put those together, and you have maybe a collection of query, Doc. relevance judgments
and based on that. we try to learn a model. you know, learn a rancor.
And so the idea is, basically, we were kind of constraining because we know it's expensive to get those document collections and to label them and do all this kind of stuff.
And once you learn a model like you guys did on Roshio, or whatever
it's kind of like, you can only squeeze so much because the issue is the model that we built here. This could be, you know. this could be Rosio. It could be some deep learning model dot dot dot.
The only knowledge it has. Is this
so what he didn't foresee.
and which you guys should be all familiar with, which is, now there's all of these
big models where you take all of Wikipedia. You know all of the web. All of of Med. you know all of you know your emails, whatever
all of this stuff gets shoved into. Now you what we call you pre train.
You pretrain a model.
So let's call that, Bert. We're kind of like word to Vec.
then we apply to our ranking task.
And so the idea is basically, in the olden days we had to have all of this like labeled data. So this is labeled
whether the labels, the labels, are relevance or not.
Now, this is all. There's no labels.
It's just a bunch of data. But somehow.
in the olden days we learn a model only from what we got when we go. Now, instead, we free create a model over all the data in the world. We don't need labels. We'll talk about that in a minute.
and then given that you've already encoded all of this like language knowledge. Now we can apply it to our ranking task. It does really? Well.
okay, it really rocks. That's the big change.
Okay? So when we talk about this like pre-trained models. What we're talking about is basically after word to effect the rise of these large language models.
Okay? So everyone in here knows about like some Gpt right
llama very recent, but, like Bert, is kind of the Og big language model that got everyone excited. This is 2,018. This comes out of Google.
famously. Like all of the authors of the Burt paper, all been left Google to go to startups and stuff like that.
because, like (201) 819-2020. And even to now there's all this thick, crazy investment in these big models. So the point here is like
the training of Bert. We're gonna get into that
billions of words.
Talk about parameters. Remember, we talk about matrix factorization.
right? We have number of users times, the latent factors. We have number of items, times, latent factors.
those are parameters. And so they're saying, like, the vert model has like 300 million parameters. That's a lot.
Okay? And so then, after this. like Gp, T, 2 had 1.5 billion parameters 175 billion parameters. I mean, this is dubious. The claims of the latest. Gpt. 4 has a trillion
parameters. Okay. Paul was like.
if you want to go down that path, I don't want to, but you could think about it kind of like the capacity of the model to like
build associations and whatnot, and so naively, just having more parameters, I mean doesn't necessarily mean it's gonna be a better model. But it's like a proxy. For like has more capacity. Okay?
And the point is, we talk about large language models. It's like these, things are now ginormous. Again, they're trained over tons and tons of data.
and they're extremely expensive to build and train.
Okay.
so like, if you wanted to go build your startup and take down. Gpt. 4,
you need boo-koo bucks.
Okay? Because these models take like
monthsed training.
it's been millions and millions of dollars to train these things.
It's insane, and the models are only getting bigger and bigger and bigger. So this is what's happening right now. So every company in the world is right now trying to do these Llms.
Bell Lms.
Every company in the world is trying to do this. Okay.
And if they can't do it, then they're trying to buy companies that are doing it.
Okay? And there's a gazillion startups all trying to do this right now. And the dream is, I mean, this is again speculation.
But the idea is.
if you can basically crack and get like the best one
you're going to be. You're just going to. You're going to eat up the rest of the industry. This is sort of this idea. So it has sort of like a monopoly effect.
So think about like, when Google search started out, everyone started using it. All of a sudden it was 90% of the search market boom.
Think about windows as like operating system. Right? It ate up the whole operating system market. There used to be other operating systems. They all died.
and I bought Linux. Okay, they're all dead phones, right? Android or Ios. Everything else is going on.
So there's this argument that, like, you can build the biggest invest Lom. All of these, you know.
downstream applications that use it
will all then be derived from your one model. And you'll basically own everything.
Okay?
Right. You thought you got in trouble for submitting homework. Be a lawyer. You basically can get disbarred right? Because there's this argument that, like these models are going to be so awesome. It's not just on some ranking tasks, but they're gonna enable all of these downstream tasks, like all of your legal argumentation.
all the sort of medical like a doctor reasoning all of this like classroom instruction learning. It's all gonna be like subsid taken over by these big big models. Okay. So that's why I'm so excited. That's why everyone in the world is so excited. This is where all the the heat is. Right. Now people are gone. Forget it.
Okay?
So if you go back in time again, you can kind of say we're to vac like 2012, 13.
It's kind of like a progenitor of these models. Right? So that's the reason we talk so much about word Tovec, okay?
And if you think about word to Vec.
remember, what were we trying? We were learning those 2 matrices? Right?
It turns out, if you go back and you take your AI class that's like
it's kind of like the connections. So we're doing some.
You guys, remember, you guys took a like a neural network class. So basically, remember, we had like, Wn, it was like this, and we had w out
like this.
What was this? N, by V. This was
v by n, or or vice versa. But the idea was, if you think about it. it's like, what if you had the inputs.
you projected them down to N, and then we expanded them back out to V.
This is kind of what we were doing in word to Vec. Remember, we took tax. We had the cabin bedding. We didn't multiply it, and got the scores for all the other terms.
So it's kind of like this looks familiar to the spiderman.
It's kind of like a little single layer neural network.
Okay? So it turns out, I'm just sort of not getting into the details of all that. But the point is like word. Event is like a very simple neural network in essence, even though we never talked about that.
we just it's a matrix stuff.
It's all the same. Okay. So the point is, we're to vex very simple network.
And now, instead of just having a simple network. you know, under the hood, you can do all this crazy architectures that have, like.
you know, many, many, many, many layers, many to many different transformations, etc., etc. It's all the same stuff, though.
Okay, so
I want to tell you about Bert for just a minute, and then we'll get out of here. Okay, so Bert is a big language model comes out in 2,018, and it blows everyone's minds. Okay?
So why? So remember, this is a language model pre-trained over lots of data.
Okay? And then they just go and apply it, some downstream tasks. So what I'm showing you here is these are a bunch of like natural language processing tasks. Okay.
this is like some semantic similarity. This is some natural language in friends. These are not ranking tasks, but there's a bunch of different tasks that historically, people tried to customize like you build your model to attack each one.
and all is crazy is birth comes out.
And again, if you walk across it, it was getting like the best results across, like every task.
There's one model.
and it's like and there's lots of other things like this.
So again, historically, like you're doing document ranking, build a model. You're doing some kind of like a in this case, you know some kind of semantic similarity of sentences. You do another model.
These bird guys come along. They have some big fancy models. And somehow it like automatically just destroys everything.
So yeah, wait a minute. What's up with that. Okay?
The way Bert worked is you train it on unlabeled data by Wikipedia.
This is called pre-train.
You feed in a ton of data.
You train some model.
and then the second phase, you can fine tune it on some downstream tasks
at these days. We don't even fine tune anymore. We prompt.
So we say, Chat Gp, they train a big model. Now I just I just ask Ted, Gp. To do something for me, and it does it
right, give it a new task. It's never seen and somehow can generalize and do. It
doesn't have reinforcement learning to do that.
So there are some phase that I'm not showing here when you get into sort of the more modern like Gpt variance. But yeah, there are other ways to like, get some labels into it. Rl, hf, we may talk about that in a few weeks. Yeah, yeah.
Okay.
so let me just show you the whole intuition here is to how this is different from word to Veck. Okay?
So the man went to the blank to buy a blank of milk.
Okay. So the way they train these models. This is why you don't need labels. So people talk about pre-trained language models. like all we need is the data itself. So they have what they call this like a masked language modeling task.
So you basically go in there and you erase
some words. Okay? And then you basically ask the model to guess what word goes in there. Okay. so you notice this, you don't need labeled data. It learns on itself.
So the idea says, the man went to the blank to buy a blank of milk. What kind of milk do you buy a blank of milk, a cup of milk. a gallon of milk, a pint of milk.
Yeah, the actual gallon. But like
it's just like we did kind of word to vet. It's kind of like, well, would you buy a cat of milk?
No. Would you buy a dog of milk? No. Would you buy a
a bag of milk?
Canadian? If you're Canadian, they buy bags of milk.
which is totally insane.
Would you buy sneakers?
And so we're going to go into more detail and model later on? But it's trying to give you the flavor of like what's going on here. It's like, we don't need label data. All we need is the text.
okay, so we hide some stuff, and the model tries to guess the stuff.
Okay.
and has another task which is called next sentence prediction, which is given a sentence he tries to guess if the next sentence follows it or not. So, for example, the man went to the store. He bought a gallon
screw.
That's the actual next sentence that's derived from your data.
You also have the mainment to the store. Penguins are flightless.
It was not a next sentence.
So again, just there's no labels here. Just the data you have itself. You then secondarily learn, does a sentence follow that sentence?
Okay, so that basically, what's happening is your model parameters are kind of learning, like the relationship between these sentences, what kind of logically flows? Okay? And all falls out of the text. I have itself
okay.
And so just to show you like example. So Bert trained on, say, book Corpus and English media.
they can take days and days.
Yeah. And this is a couple of years ago, you know, it could take you a couple of months, let's say, with some gpus to train this thing. and it's expensive. but just like word to fact like it gets doing the homework.
You can go just grab the pre-trained bird embeddings and just use them
bye. it's already done for you. Okay.
yeah.
So this is, Bert, we're gonna do more details about all this. Do you guys know the big insight underlying Bert? Why, it works.
There is no labels. But what is the technical insight
transformers
on the ball?
Optimus prime the decepticons, you know. Megatron
star screen.
You bumbled
the no, I was thinking, one of the the dyno bots. Yeah. They made a movie with the dyno bots, I think. Didn't they do? People still watch these movies.
not me. I saw that book.
I saw the second one in the theater.
and when I walked out I felt like I had seen the end of cinema. It's like, after this movie, they don't even make movies anymore. It's like as an art form, it's totally dead. It was awful. Okay.
So main thing from today. And I go. This is sort of like a nontechnical day, just trying to give you the vibes. That's why you think think about this, which is
our history, which is, we did word we did. Sorry we did. Tfos sign. Maybe begin 25 right? Just counting words.
We tried some learning to rank stuff, try different features and put them together.
Okay, then we get to the world where we have word embeddings from word to beck.
and we can kind of carefully put them together. Okay, then, for those
2,01314, 1516, 1718, people are trying all these deep methods, trying to fancy and fancy your Ml. Models
kind of not really working.
And now, in the post, 2,018 era.
we rely on large language models to do all this for us.
In fact, we may talk about a little bit about this next week on how you can really use one of these for ranking. Okay, but the point is, these things show up. And all of a sudden we do see big dramatic increases in the quality of like our search and ranking. Okay, big, big change.
and underlying it are some sort of key technologies which we'll be talking about in the next few weeks.
Okay, so that's the main takeaway you're living in this golden Age right now.
You don't even realize it. That's happening right now. If I were you, I would drop out of. You're about to graduate. Just quit, drop out
investment.
No, you don't. Wanna don't worry about investing your money, invest your personhood to be working on these problems
because you're gonna you're gonna change the world. And you will enrich yourself.
So I just okay, we're gonna go off. Just
people who did this right. And there's always people peeing afterwards.
Do you know much? Look at? Paid
at least $30 at least $30. What do you think the total compensation packages for someone who's an expert in one of these things?
Total. Comp, I'm talking stock bonus salary whole deal. What do you think? Get out of my face with that. That's a joke, yes.
but yes.
Well, so think about it from this perspective, like GDT. 4
was the fastest is the fastest growing app. All time. Got like hundreds of millions of users within a couple of months.

Local stuff you like that
and casual mention pretty much.
It's feels what it feels great.
Oh, yeah, yeah.
it's good. Good. Enjoy the weather. We'll be back up to 90 on Friday, I'm afraid. Yeah.
Friday and Saturday were the nineties again.
Homework very much, mainly
like a final exam preparation.
Okay, so yeah, practice problems, things like that. So my mind should not be super tax.
Okay, it's gonna be designed to be practice. This one. However, has more stuff to it. I give you a lot of time. So like multiple weekends, I think you have 3 weekends to work on this.
There's 3 problems. I don't think you should take that one, even though it looks like a lot, but it's the kind of thing I would take a look at. Now, for example.
the first problem is matrix factorization. We give you all the code to do the matrix factorization
you have writing. That's great. It's more analysis. We say, look.
read through it all, and you can do that on your project. By the way, you want to do that and give you code you add in that. And then we basically say, now go back in like, let's investigate the latent factors. So we give you some movies and say, what are the movies most like this movie in the latent factors space?
And then we say, what are the movies most like this movie in the original rating space.
And so the idea is, we want you to kind of get your hands dirty with actually looking at the latent factors, doing some cosines and comparisons and figuring out what's going on should be fun. The second problem is word to vac plus. We're going to talk about today. So it's very timely the key on the word to that business is you're not actually going to train the word to Vacc.
You're just gonna get them. Someone is already free train. We're gonna we actually give you the the call to get the free training right now. Once you have them, how can we use it to help us do better search and ranking as we talk about today.
The third one is, wait, web, wriggle function. So this is basically you're gonna use duck, duck, go.
Yeah, we got the duck. Go, Api. We even wrote the little snippet just to get back like, send a query and get back some snippets from duck go.
and it's akin to that paper we read where we said, oh, I have a word like Svm. Or Pdf, I'm going to go Google, it get back results. Tf, idf it
and then use that to represent it. We're gonna do something like that to go back and look at our lyrics data set. But now you're using duck. Go to kind of give you richer representations. Okay? So to my mind.
there's a lot in the homework. But to me it's more of like figuring out what's up
versus it being like a mind breaking like you have to write everything from scratch. We give you a lot of starter stuff.
In any event. get after it
like, I mean, what are you gonna do on the weekends? Watch our football team?
Who's so great
but to buy weeks.
It's actually it's a good weekend. No, football is good. We're not gonna lose. Okay, you know. And I say it from in a, from a place of like like love and compassion. Again. This is like my seventh or eighth football coach. I've suffered just like you suffer for real. I've been here with the excitement.
I was here when Johnny Mansell came, and Kevin someone was the coach, and we went 10 and 2, and everyone was going crazy. And then it kinda petered out, fire that guy get a new guy and then it fire like I get a new guy.
So I've suffered.
I'm just saying like, I've only had to suffer for so many years. You have to suffer enough for the rest of the org. Sorry project proposal. Sunday.
I've only had a couple of people ping me like, Hey, Kyle, what's up? Please do that.
Okay, I'm not gonna give you a project. But
you got an idea you worried about something. Ping me, I'll respond.
okay, get on that. Anything else we're worried about?
How about your other classes?
Are you worried about your other classes. Yes, which class English, you guys don't do English in here. Okay, I have some thoughts on the project that I want to share.
Okay, as you're thinking about the project. Almost everyone's gonna do a demo prototype.
If I get that's fine. That's great focus on simple newspaces.
Add your own twist. What I mean is this.
if you wanted to do something like you know what? You know what I would do. Some kind of restaurant recommender.
We'll do the restaurant recommender to satisfy everybody on.
That's what happens. People do it. And you're gonna have 15 features. Think about like I wanna do restaurant recommendation. But I wanna do it for broke college kids.
Okay, where I want to do.
I told you asking video gain recommendation.
we go. I wanna do video game recommendation. But I'm gonna do it for, like non English speakers who just come to the Us. And are trying to learn American culture.
I don't know what that project is, but it's a very like
focused right.
Add your own twist, which is again, you don't have to do everything. assume away stuff, simplify.
and say, like, I had a project once where they said they wanted to start doing recommend restaurant stuff. And so they've got this yelp data set. So Google around, you found this good yelp data. It's too big. Instead, they realized some of the reviews talked about
the bathroom.
So then they said, You know what we're gonna make like. you know, their their their app was like a poop emoji. It was like. where are the good bathrooms?
And so all they did was. They looked through all the reviews, and it was like a text mining project where they look through reviews. Anyone talks about the bathroom. Is it clean. Is it nice? etc., etc. That's what they used. So then they had a map.
and it was like, wherever you were you could find the closest, cleanest, nicest bathrooms. How do they know it? Based on the reviews? Okay, so again, that's a project very focused, very narrow, but really cool.
So that's one. Get all we need to think about that. Whatever your problem is, keep narrowing it down until it's like super super
on point.
Now, the second issue is, I only mentioned here, I guess, from the next slide. But like data.
so one of the big issues is data and thinking about, like, basically, are you doing like an offline versus kind of like an online demo? And what I mean about this is
a lot of us have aspirations, for I'm going to have this cool demo. It's going to be live. It's going to run versus.
I have no data. I have nothing to run it over, or I'm doing. Api calls that are super slow.
So you have to think about you might do. And it's totally fine, which is, I have offline data. I build all my offline models. And so what you, Demo, is kind of like a synthetic. Hey, Kev, imagine you were doing this? We've already pre computed all this stuff. We've already done all this stuff. And you're kind of showing me like this offline version. Okay.
versus online. What I mean is, Hey, you come in here. We're hitting the Google Maps. Api. We're hitting the yelp. Api. We're hitting all these Apis.
and then we're doing stuff which can be very slow.
they can be very challenging. Just keep in mind either one is fine, but you may want to think about what's your best bet
separately related to that is data. Like, if you're going to do restaurant reviews or restaurant recommendation, you got to have
reviews or data about the restaurant. Where does that come from you got to find it.
Secondly, is, you may not have user interaction data.
right? So a lot of the things we talked about is like, Oh, this person likes this thing. I can use that information and you don't have it.
You can ask your friends or next slide. you can use check Gpt.
as your data augmenter. This is super smooth trick. Okay, what you do is you go to tech. Gpt, this is an example I found from a recent paper. They said, they're trying to do movie recommendation.
They said, I've watched the movies in the past, the Matrix Terminator Judgment Day. Now, there are 20 candidate movies that I could watch next. Please rank these movies. Blah blah blah, and it rings it for you.
Okay, let me give you another example. I'm in the mood for songs that feel like skateboarding at dusk along the waterfront. What do you recommend
speedboarding? It has a coolant relax.
You want songs that capture that feeling. Here's a list of cool and relaxed songs.
This is so easy way I can generate data, for in this case it's like a vives based music recommender.
I don't know how to do that, I can ask, and it kind of gets me started, passes. It passes the vibe check.
I like horror movies. My favorites are the shiny and the Texas teams all massacre. What songs could you recommend that has similar vibes?
And it says, Oh, I like some that are eerie, dark or suspenseful.
And then it gives me, you know, some songs.
Okay, you could even do stuff like, I like horror movies shining and Texas sheets on massacre, which song would, I prefer? Wild streams like yellow, swift, or yellow submarine by the Beatles. Explain your reasoning.
And it said, Well, we think you're gonna like deal with some reason, it gives me a bunch of reasons why.
Okay, I'm not saying this is the answer to everything. But historically.
like we didn't have this tool. So again, what I'm saying is like, this is a nice way to bridge
and get nice sort of synthetic data. I'm not claiming this is right. I'm not claiming this is right.
But it's a nice way to kind of Bootstrap a project, especially if you're lacking data or you're lacking like, how do I get? You know movies to songs? II don't know how to connect those 2. I can use Chat Gbt as kind of like a bridge. Okay? Or something like this. Yeah.
it's awesome.
So do I hear about the value of the product or right to me. I always say, like, I want you to do a project that you care about. And so, even if it's just from like this is not a business school thing. So I'm not gonna like, Am I gonna write you chat or something?
And so like the you're trying to just show something using this cool, even if it's more like us.
intellectually honored. Cool. you guys have no real value. If you can convince me that you're excited by it, I'm excited. Okay.
But if you're not excited, I'm not excited. That's the best thing. So technically, it's gonna be interesting. But a lot of these projects are like, it's like topics I've never been heard about, you know. But you're excited. You convince me you're excited and really excited on there with me.
Cool
any other questions about check. Gbt.
so what I would suggest is this like.
hmm.
do we have to do like enough, right
or so in my mind?
boom!
Now again. I told you before, like, if your projects is just for backwards around Jetg, I'm not that excited.
But
you know again, we haven't really gotten into like prompt design this like prompt engineering business. But if you had some like clever ways to do this.
sure.
But if your whole project is basically yeah, a wrap where you'd like people say, like, What do you? You know? What movies do you like? And like this is, recommend something. And you just call in, and just sort of passing that to me. That's not very exciting. So you're welcome to do like a very much like project. Totally fine, totally fine.
But I want to see like I'm systematically do it, or some sort of rigor to it. Does that make sense a little fuzzy? But
yeah. okay. the other thing is like you could use this as like an input to your
other model. You know, we talk about user. See? Phones kind of thing you can use as input as well.
Okay.
any other recounts
good vibes. Okay.
cool anyways. Very powerful, very cool, cool.
and and to kind of to your point, which is like.
remember the nondisclosure agreements I signed.
but I would say, is like people are very much exploring the capability of of these large language models to do, personalization to do recommendation. What I'll say. Like as a big picture the whole community understands, which is
these Llms like Chet Gpt, they know a lot about movies and books that are like widely known.
but like they don't know a lot about fresh content. They don't know a lot about like the current Tiktok Video, I'm watching. right? But they know all about like they know the shining. It was a movie from 40 years ago. Books have been written. They know all about that.
so kind of like the long tail of items they don't know a lot about. They don't know a lot about fresh stuff. So there's ways you could think about. I mean, I'm not trying to push this too much, but like personalization or long tail plus Llm is a big challenge.
When I say Long Tail, I mean, like all the stuff it's like, not super popular, right? So it knows about all of the like famous popular stuff. But what about the movie no one ever saw, or the new Youtube video that no one's watched or the new. Whatever the new indie album from your friends in the other dorm.
Okay. cool, alright.
Well, in fact, this is actually a paper you can go find like this is a paper from this year. It's on archive. And the whole thing is pure like recommendation via prompting.
It's what they actually do. It's kind of neat. They basically
they do like a 3 step prompting
where they basically say, like.
if you felt like, you guys have played these language models like goofy stuff happens. If you just use, ask it to recommend movies. It doesn't do a great job. But if you first say, like, Hey, I watched these movies, hey, Lars language model
what featured the most important to me. And it says, based on what you want. I think you like this stuff. Then you say, Okay, great. Now that you know my user profile. which of these movies would you recommend?
BA, BA, BA, BA, okay.
because people really try to do this.
it looks fancy, but it's just like it's just prompting over and over and over again in a particular way.
So cool stuff. Mmm.
hmm, interesting. Okay.
enough of that back to the good stuff. Regular topic. Last time
we were doing this whole word to vet business. Okay?
So we have window. And I kind of kind of glided over this bit, but like the idea was kind of trying to figure out the probably these words given the center word.
So what I'm gonna try to do is just in a few minutes complete what we were doing last time, and then try to link it back to this probability business. So hopefully, it all like kind of comes together. So where I left it last time was, we have all this training data.
And so, for example, cap scratches that's like our center.
And that's like our outer word. So in other words, it was like, you know, the cat
scratches.
So in the context of like this previous slide where it's like into is the center word we're saying right now. Cat is the center word, and we're looking at the words around it. One of them is scratches. So in our training data, we have this.
We had this double of cap and scratches.
Until then we walked through this whole thing which we basically said
we have, like our in embeddings. Remember, this is just a big matrix that has
like a 50 dimensional representation for every word. so N is 50
the size of the vocabulary are 100,000 words. So somewhere in there is the chat embed. When I say the can. Embedding, I just mean is like some numbers like we looked at for King 0 point 3 minus 1.2. All these numbers. Okay?
And so we said, basically, all we're gonna do is take the center word. find cat is, look it up and give me the representation for the cat. the embedding. So it's just a look up. It just says, Go find that like a green blob. That's the green blob.
Okay. So now we have the cat embedding. Then we said, take the cat embedding.
multiply it into the out.
that out matrix, which has, like the out representation, the outer representations for every word.
and the idea is, it gives me a score between cat and every other word that I know about.
We end up with this one by the size of my vocabulary 100,000 long and like a score. Okay.
so
then, we softmax that puppy. Okay. So now we have a score between cat and every other word.
We did that magics. Where's mine? Yeah?
Then we did our softmax, which turns a vector of reals into a probability distribution.
When you see probability distribution. You start thinking about
probabilities.
And we're gonna connect in all just a second. So I got my score vector, here.
this is a score. Yeah, hang on. This is a score vector between
at and every other word.
Then we
turn it into a probability distribution.
So those scores. Now you turn into probability distribution. Okay.
Softmax.
And this is where kind of like. I then encourage you to go take a machine, learning course, or to go do it, you know, off on your own independent research.
where I kind of we leave it, which is basically at the end of the day.
For this training example, I would say, this is the actual ground truth.
This is my ground truth.
Now write that because that word right there, that is, scratches
scrap. You can't even. What am I even doing here?
The point is
for this example. It was cat scratches. When we did all of this, we kind of said, Well, scratch has only got point 5.
It's really far from one.
All these numbers are not 0. We wish they were 0
right, because ideally, we would like exactly recover the one.
But we didn't.
Okay. And the reason we did it, because, remember where these numbers come from, what decide these numbers?
Well, those numbers, and where do those numbers come from?
Well, by golly, they come from W in w out. that's it.
So the whole idea! When we get to the end of this whole thing is like.
you know. you know, our dream.
you know, is that like our probability, distribution
is close to the ground truth.
But it's not.
but it is not
so. What can we change?
But we can only change W. In and w out. What are those? Those are my embeddings.
or all my words.
So just like when we did matrix factorization. And we were adjusting P. And Q,
remember the MF world, we had P's and Q's the user latent matrix, the item latent matrix. And we could change those. And in the homework you have the code, you get to see it happen.
It's sick. You're gonna love it.
We did gradient descent
in the homework. We did a little different way, but the ideas in class would be gradient descent to update P's and Q's
to estimate our ratings better.
Same deal. Here, let's go back to w and wound. Figure a way to change those embeddings
so that our point. 5 is closer to one. And are all of these other non-zeros or closer to 0 boom. We updated it. So there's some kind of update rule. We got to do some kind of
gradient descent. There's some kind of update rules.
Go take Ml. Or see me later.
But for this class we're just gonna end it there because you're not. Ml, it's not Ml class. You haven't seen that before. Okay, my whole take away is if you buy this whole like notion of, I got embeddings. I have cats, scratches.
I find some kind of difference. I have some kind of update rule that changes all of those embeddings like literally changes. These numbers
change those numbers somehow. It is it closer to this?
Okay.
that's just for one example, cat scratches. We also have cat, the
and cat.
dog and cat run and cat, poop and cat, my
and cat bed in all these words, okay?
And so imagine all those
examples come in, and we're constantly mucking with those embeddings, because we'll never exactly make it be one, because that would mean if we see cat, we always see scratches. We don't
right, but sometimes we do.
So. The hope is we're going to see this enough, enough, enough enough so that words like cat and scratches will have kind of a higher probability than words like cat. And
what's a word totally unrelated to cat?
No, I think each is quite related to Chad. I need to, Michael Rabbit.
I'm not gonna say that word. I feel like you guys are playing jokes. I feel like Pat and he might be near each other in a sentence.
Oh, yeah.
I think she likes cats. I bet she's a cat lady. There's a video she's dressed hypersphere and caps beautiful.
You can imagine a world where cat and hypersphere should be way less probable, right, whereas cat, especially much higher.
so generalize that to all the words related to cat, you would want to have higher probabilities, and all the words that are not should be lowered. Okay, now, if all of this works and we solve it again. Figure out how to solve it in another class
to go back. I'm just gonna try to like hopefully tie this bow so we can talk about the other stuff.
which is sort of rewind. Okay.
we have a likelihood function. So the idea is we're trying to figure out the best choice of all these probabilities
that maximize the likelihood of the actual word pairs I've ever seen.
What does this mean? It means go from one to T. That means loop over every word in my vocabulary.
Look at the window around those words of size, M, so like 2 window.
just the window.
So it basically just says, Look at every possible I showed you, Tad, and scratches this says, look at every possible word in every window around those words.
okay.
and figure out the probability of those words in the window. Give us a pincher word, such that it Max up like we were to multiply all that out, it would give us the biggest number.
Okay, in practice, you do this negative log likelihood. Don't worry about it. But the idea is. you already understand this intuition. Okay, so check this out
the question, how do we calculate this thing? What is the probability of a word given the center word?
Well, we just did it. We just say.
when cat is the center word with scratches is the context word. So look them up in those embeddings.
We basically, this is the softmax
of the embedding of cat times, the embedding of scratches. We just did that
like. So, for example. if that is cat. and that is scratches.
All this says is softmax.
That's like cat embedding.
that is, scratches
embedding. So how do we figure out what is the probability of a word given the other word. We just do the softmax of the embeddings go back. What do we do here?
We took tax embedding. We multiply it by the scratches embedding.
because that number we softmax it.
We just did it. So we can interpret this number as the probability.
This is the cool thing that's like the probability of scratches
given. Cap.
Okay, I just want to show you all this. It all is connected.
We're all connected. Oh.
we're like in a hypersphere, hey? By the way, on Friday, don't forget the quick quiz. You gotta do it. I have 4 designs to vote for for our swag. Okay, they're not the when you vote. They're not the final versions. You're kind of saying like, I like this style, but the color may change. The font may change. Okay.
but I realize I've just talked myself into like shelling out like $1,000 for T-shirts you'll probably never wear.
and
I want all of you guys in slack, just putting that emoji. Okay.
you guys don't remember this when I was a younger person
a long time ago. You guys know, Eminem is like 50 now.
So when imminent was just blowing up, I don't know which album it was at some point you went to the Mtv. Video wards or music awards. They don't even exist anymore, I think.
is one of the videos, the Grammys. It's something where he's wearing like his white shirt. And then, like I, either. They walked in with him. But like all these
other guys like blond hair and like the white shirt, it was like his army.
They were all everywhere. Okay? So that that's kind of what I'm envisioning right here is we're all wearing the shirt. We're all gonna dye our hair blonde.
Someone can find that on Youtube. Okay.
yep, so.
but whatever you're talking about.
we can change the value in the zone you have, yeah.
to update the cost function. So it does be seems like from the branders of the model.
Yeah, these are the these are the kind of like model parameters.
All the Ww. And then you have some kind of update rule which again, I'm not showing you. But the idea is just like when we did matrix factorization. And we said, like, oh, this is updated. Blah, blah, there's something similar. Exactly.
Okay. Now, where are we?
We did that. We understand. We're just back whole family of this thing. And you saw it last time. No, today
that I thought, that is bad.
I'm not sure we're gonna do all this. But now we're gonna talk about given. We've done all this. Who freaking cares? We care. I'll show you how
this is. 2016
again to you. You're like I was a baby, but it's not that long ago. Okay.
Microsoft.
Microsoft
says we're going to do a dual embedding space model for document ranking.
We know how to do document ranking. We got tf, ivf and cosines right?
They say we're gonna do dual embedded space model. That sounds fancy
it sounds too fancy I need to leave right now. why am I in this class?
Why am I here? This is too complicated? No, it's not. It's very simple. I'll show you how simple it is. All they use is what to think. And you have the dual. You know, the dual embedding spaces.
Okay.
in and out. in and out. That's it. So when they say
dual embedding space, they just mean WN.
The Wx.
That's it. We're devac embeddings. We already know how to do that. They just use them.
Let me give you their motivation. They say this, they say, look okay. Well, first of all, it's illegal. We're gonna use word embeddings. We're gonna use word to Vec
to do smarter ranking. Okay.
their motivation is being 25. We didn't talk about B in 25, but being 25, very simple. And the idea is it's measuring some notion of aboutness. Okay, how words are related. So not just counting repetition of terms, but the aboutness of terms. Let me show you example.
These are 2 documents about Albuquerque. Let's read the first document.
Allen suggested. They could program a basic interpreter for the device. After a call from the gates claiming to have it working. Does anyone know what we're we're talking about here?
That's Microsoft. That's Bill Gates and Paul Allen.
Are we old 100 people
Americans like.
The one time. Sorry.
Okay, good.
So the question is, this is all about Bill Gates
and Paul Allen making the sort of early versions, of whatever they stole, some code or whatever. Okay
is this about Albuquerque?
No, which words are about Albuquerque. Albuquerque is the word New Mexico.
but the rest of it is not really about Albuquerque.
Now consider this document.
Albuquerque, most populous city, Rio Grande, city population. There are lots of words in this document that are kind of about Albuquerque.
Right? It's central. It has. Altitude, is New Mexico. It's an accounting. It has a populations near a river. It has all of these kinds of characteristics. All these words are kind of about.
So then this is the big intuition. This came up last time. There was a question like, Wait, wait! We have in and out. Those are 2 embeddings. They kind of do different things right? Like if you're a central word versus, you're an outer word.
They exactly exploit that. Let me show you what they do.
They say, well, you already know this WN.
W. Almon.
This is the fact. We already we just did this right. This is basically
find pat finds embedding make pass embedding, multiplied by everything else. Get a distribution, Softmax, and get the probability of all those words with respect to the initial word.
So their idea is that these embeddings are kind of like for the central word, and these are embeddings about the context words.
and they kind of capture different. Kind of semantics. Okay.
Example, check this out.
If you consider Yale.
If you take Yale's in embedding. So if you go to go to word to Vec.
Get WN. And WN. And compare Yale from the in embedding
to every other word in the. In embedding. What are the most similar words to Yale?
However, in my view, move. That's that
it's saying like in the center context. You see what this means. It means like sentences that say.
Yale is a college Harvard as a college, Nyu as a college.
Yale is a fancy place. However, it is a fancy place. Yale mit is a fancy place. So those words are like, what could you replace Yale with?
That's the center word.
Now consider using Yale.
and you you compare to the out embeddings of all the words. So you see what is Yale most like? What words are most occurring with Yale.
while Yale faculty alumni orientations in New Haven graduate.
Do you see what's going on here?
It's like Yale is like, if you're looking at Yale. This is a central perspective versus as an outer perspective, they do. Another one seahawks.
You don't like seahawks. And
okay, fair enough. Fair enough. Any cowboys fans in the house.
Cowboys, man like it's tough. It's tough. Sorry we're talking football again, but
I don't. Wanna I don't want to make it too much like A. And M. Football, but it's like
cause. Even when I was younger person, cowboys won a bunch of super bowls, and now it's like it's like 30 years enough.
My dad my dad was born in Chicago.
and so he grew up as a he did. He only withers as a kid. He moved to Louisiana, and he was a cubs fan his whole life, but they did win it in his lifetime. 2,016.
He saw them win. And you talk about sweetness is probably one of the happiest nights of his life. I just say that to those of you. If you have anything in your life? Is it an inn? Football, is it cowboys? Is it seahawks? Is it forty-niners? Is it? Whatever
you know? Maybe in 50 years you'll have that okay
to pick it up. So the point is, we interpreted, these are words that are used kind of interchangeably
seahawks, beater. Other team. You can, you can replace it, and they all make sense.
whereas if you do, in out these are words that are kind of about the seahawks, they're words that happen near Seahawks.
Now, I want to leave this up here. Does everyone understand what the comparison is here? What's happening? This makes sense because this is this, is it?
This is their analysis, they said. We have word to Vec. Looks like at the end embeddings and the out embeddings. They kind of capture different characteristics of our words. And so when we do this word comparison, it's like in in healthy kind of replacement in out. Tells me kind of words about it.
Okay.
it's cool. So now
they say, let's go build our dual embedding space model. Are you ready for your mind to explode? You say no cats easy, because we've already seen all this stuff.
So check it out.
We're searching over documents. We have a million documents
right now. All we know is for each document we have word embeddings. So like, imagine, here's a document.
word, word that we're we're
go get the embedding. go get the embedding.
go get the embedding. Okay, we have another document. It has a bunch of words. baseball.
So here's document one. Here's document 2. I go get all of the word embeddings for all the words in there. So that would be like
a bunch of numbers.
a bunch of numbers, a bunch of numbers.
So that's like the embeddings.
For all the words
in d. One.
and we do the same thing for d 2.
Those are all the embeddings
for all words in D 2, and then we just take the centroid.
and then we just take the centroid. I say, real shield.
So adopting it is just a centroid of its word. Vectors
skip the vectors. You can maybe normalize it. You just take the average. You know how to do this. This is not good, right? It's the average word embedding at the document.
Okay? So
we now have a representation for every document based on the word to vet word embeddings. Get all the embeddings. Take the centroid boom!
Boom! Now let's do cosine. basically. So we'll say, Well.
that's that.
So? Then we have wait. Oh, well, now we have a query. So they say.
take the query words.
so here comes our query. So if the query
is, cat scratches.
Okay?
So this says, well, I have 2 words.
So go do the cat embedding times. Document one.
So basically, I would do like, I'm just gonna write like.
let's undo that
we get the CAD embedding.
I'll put a little vector on that Times document one
just.it. Then we take the magnitude of the CAD embedding
and the magnitude of duck one.
Okay? Oh, and then we just then we say, then do add
scratches embedding times. Document one over the
scratches. Magnitude dot one magnitude.
and then it says, the size of the queries, then do half of that.
You could do it other ways.
But what I want you get out of all of this is
because you're gonna do this on homework. By the way. okay.
he just says, find the centroid.
then know how to read this map because each query word embedding times the document centroid, normalize it, and then just take the average over all the query terms.
and that gives you some score, which is like how similar this document is to the queries.
So you've already seen all this stuff. So also, when I talked about like vector space model and cosines, and then, like in 60 s. And 70 s. And you guys are all like snoozing. You're telling your parents, Lol, this guy's teaching us about like old boring stuff.
It's still doing it.
The inputs are a little different. It's not like a sparse, superficial vector like we did T of ids, it's now a word of Betty. Same idea.
Questions about this. This is key.
So it's pretty much just dotting the query, vector
where the queer embedding. Yeah, for the distributed representation of each work like an embedding dotted with.
and that all the embeddings on each document
well, for each document is just the average embedding of all the words.
But
so we had, like King Queen, all those things. You take the average or take the centroid. Okay.
why did you do that. So this is a simple way. Somehow, you in theory, we would like to. you know, because you can imagine you talked about this before when we did like ratio, we said like you could have like. If the word embeddings were here, here, here and here
you would find the the average would be sort of in the middle, where nothing exists so it could be muddled.
So it may not be perfect, but it's sort of like this idea of it sort of. It's a simple way you have the point. It's simple
with me for that.
You could try to be a little smarter about how you do this, but the challenges. How do you do that? Because you don't know which words
to compare to?
Okay. Because right now, we just have embedding. We just have a 50 dimensional vectors.
And so we're saying, we don't know how this document is related. You're saying like, could you take the document not just average all the word embeddings, but maybe, like chunk it into different parts, and then compare to those parts something like this. What I would suggest is, you can try lots of tricks. This is very simple.
But here's the question I ask you.
which we have 2 for the query words, we have 2 embeddings we can choose from the in and the out.
and for the document we could choose the in and the out. So go back to this picture here? If the query was Albuquerque right? The argument is that this document has lots of words that are about Albuquerque.
so which representation of Albuquerque on the query side, would you want from the end of the hour?
What's the end? If this is the end?
Which representation would you want for the words in the document
be out
because you're saying like, that's what we did here. in out. We want to do like if the query is Gale.
we don't necessarily. Maybe you do. But like the idea is, we don't want to find documents at Harvard. What doctors are about faculty and alumni orientation, and things like that are about Yale
cause. The idea is, those are words near Yale. They can help us find documents about Yale.
So kind of the argument from this is like in out, seems like really smart.
So
that's what they do. They say
same deal, but the query representation is the in embedding the document comes from the out embedding. But it's still the same exact formula we just did.
So this is the dream that it's like about this. Okay.
so this is why I'm I'm gonna get weepy now. That's why I'm so proud of you.
You've grown up so fast. And
Hi, everyone who came in here.
you know you hadn't even shaved yet.
And now you got the full beard, you know. It's like you've grown up.
Read this tall, maybe this tall. Okay.
You know how to do this
like to me. It's like, I think if I showed you this in the first week of class, some of you would have been like I'm out of here. I don't know what's going on. but it's like.
you know what word embedding is, you know, about centroids.
the multiplication. Some dot products. No big deal. Okay? And this is like cutting edge research just a few years ago.
Okay, so you guys are doing the cool stuff now.
does it work?
Do you think it works?
Yes, I know. So let's just do some experiments.
Great
lots of Bing queries. lots of documents. They have
some results. And they do 2 approaches. They say one.
we're going to run B in 25 kind of like vector space run B in 25. Get the top results, then rerank them according to this thing.
or we rate all of the documents using this thing. you see the difference.
So like being 25 is kind of like, gives you some good candidates, then we just re rank them
and so check this out. But does it work?
This is ranking all the documents.
The so ndcg, we we know. Ndcg, let's look at this, this, this little, just this number here.
Say, what if we have 21? What if we use the Esm
sucks.
Super socks Mega socks. It's like 0.
It's terrible.
So Whoa! Whoa! It's like
awful
crash!
Awful trash!
Get it out of here!
I get it out of here. But we we owe we owe Dsm. So much money guaranteed we can't.
You can't get rid of it.
We're stuck with it now.
Okay.
so I'll talk about. I'll talk about this bit later. But the main thing I want to see trash now. Oh, we just look at the remaking. Now wait a minute, now wait a minute.
So there's your BM. 25.
Now what happens?
Oh, in, out.
in, out. Now we can beat it.
2 things I want you to take away from this right now. Number one
BM. 25 has nothing. Fancy going on.
You don't really remember the formula, but it was basically like you just count some words and stuff. It's simple.
So to me, the takeaway is like Mega super simple. It works pretty well.
or we can run word defect. We can get all this stuff and do all this. But and we can maybe squeeze out a little like per second.
Okay.
moral of the story
works. It's kind of cool.
I'll talk about this example next time.
Dsm. on your homework.
Hmm.
One approach to use word embeddings not great at ranking. Good at re ranking.
Okay? The idea is, you have to give it some good documents, and then it's pretty good taking a a better consideration of about this.
Okay.
another takeaway.
even though here you say, well, you know, it kind of squeezes a little bit. But somehow. if we learn better representations of our words. And we do a very simple way of just averaging them for our documents, we can get something.
Okay. And this. Basically again, like inspired a lot of people to say, Wait a minute.
There may be more sophisticated ways
to model text using machine learning. And maybe that will help us do even better gear.
Okay, so come back on Friday. We're gonna see what happens next. How many of those efforts died
terrible horrible like they were terrible.
But then it launched something even more exciting.
which is what we use today. Okay, do great folks? We'll see you all soon.

Yeah.
let's cancel this already done, the meeting
for a single public
that seems an important
from the Us.
What you?
So
they just a little bit
essential.
Okay.
okay, howdy?
Enter some. Someone put it on Youtube, I'm sure. If you want to have a class dance
fascinating.
you know. Go with our swag. Speaking of swag this week. Quick, quiz, I'm gonna show you for shoes from okay. And the way it works in the quiz is like there has to be a correct answer. So I leave them all correct answers, so don't they? Don't put. Don't select all of them just like one of them. So it looks like you only got like a quarter. Don't worry about it. The main thing is just let us know your thoughts on that.
Okay, what's going on? Few things homework. 3. Gonna go out soon, like, hopefully today, tomorrow at the latest.
What? What's the head shaking? It's fun. This is, gonna be a good one. You're gonna like this one fun fun
project proposal due Sunday. If you don't have a team, get a team if you don't have an idea.
ask Chetgt for some ideas.
Yeah.
Then, if you want to ping me with your ideas, feel free to
hey, Kev, we got 2 ideas. What do you think? Hey, cab! I have 4 ideas. What do you think? Hey? This is my one idea. Is it just right? Is it good. Let me know
also whoops
next week. Can I write
next week? Monday, Wednesday. I'm not going to be here.
Oh.
but we're still gonna have class. We're gonna zoom it.
I think I'm not supposed to do that. But it's gonna be on zoom.
Okay.
I'll be in California Monday and Wednesday. But we will have class. I'll just zoom it.
It's all good doing it.
What am I doing in California? I'm going to go out and visit and do my other job at Google for a week.
Yes, virtually.
I work at Google as a visiting researcher.
That's it, please.
And I'll be hanging out in California. I'll be a Google Hq. You guys want to give me your resumes and I'll pass them around. Okay.
okay, I'm sorry I have 45 resumes, here is that cool
But anyway, take a look we'll be on zoom next week. Any other thoughts concerns.
It will worries.
Yes, we're gonna get more working with my favorite part of
that's good. I mean, there's really gonna be. Yeah, I'm learning a lot.
But
you know, great. But people really, really great, great people, very good product problems. And obviously, I think, compute right big resources. So very cool. Yeah, lot of secrets. I cannot tell you. Okay, good. Any other questions concerns.
Sos like, help. Help me. No, it's all good. It's all good. Okay. cool. Alright. Today's fun day.
aren't they all fun? The weather is great. Some of you are complaining that it's too cold.
A month ago it was too hot. Would you rather be 50 or 100
50 50, you say a hundred. Get out of here.
are you serious?
Come on, man. Layers layers.
Last time we were talking about this whole deal. Everything we've done up until now superficial, like su surface representation. Pat in our Vectorspace model is just a cat dimension. And so back in the olden days. Right? It would be like if that was cat.
and that was kitten.
Never! The twain shall meet right. They're totally orthogonal. They had nothing to do with each other.
That's what you see here like. If this is the cat dimension, that's the kitten dimension.
You do any kind of cosine any dot product, anything zeros right, and nothing in common.
So we said, Let's move away from this to representing words not just with like a single dimension. But instead, let's consider words to live in some rich, dense representation space, our embedding space.
and so we called it a distributed representation. And so the idea is like these are dimensions.
And so now words have, like some
prevalence in different dimensions. So it's not just a single dimension. Now, you're sort of over many. So you're distributed. And so this is where we left it. Last time, we said, let's do these low dimensional vectors. Our word embeddings
very similar to what we did in matrix factorization.
And the idea now is we're going to go down, maybe 25 to 1,000 dimensions to represent our words. Okay.
that's it. And so I think we have this picture. Yeah. So the idea that cat is some 8 dimensional vector, this is a distributed representation.
That's a distributed representation that is in embedding
that is a dense. Vector we kind of use all of these kind of interchangeably.
But the point is this, is what we're trying to get to. Okay. So I want to do today is motivate this a little bit more.
and then we'll talk about word to veg and how we actually find them. So I grabbed. This is a great resource. This guy does a lot of these illustrated.
You don't know about him. He has, like an illustrated transformer. All this kind of stuff. Very nice. I took these examples from him.
What we're looking at is, this is a word embedding for key.
Okay, he's actually using what's called glove, which is very similar to word defect. Same idea. This is the actual like glove, or, in our word, word, defect word embedding. There's 50 dimensions. If you look at, it's like 5.6 8 negative, 1, 2, 2. All these numbers.
This is the representation of king. And you're like great. There's 50 dimensions. That's king.
Okay? Okay? But the idea is, we have these for all of our works.
Okay, is it? Well, look, you can take the 50 dimensional
vector and then just line it up.
So now fit me, it's like this, that's our vector right? So this is just our. This is our embedding.
this is our
distributed representation. These are all of our dimensions.
you know. Boom, boom, boom, boom! And we have 50 of them. He just lines them up, and he color codes them by like the values.
And so he says, Look, if it's very positive he makes it kind of more reddish, and if it's very negative, he makes it bluish
so that you can very quickly, visually look at the numbers of the 50 dimensions. Okay.
So just to show you what he's doing here is now, here are the vector representations for king, man and woman. Okay. So now I turn it back to you. And I ask you, as you look at these.
wh. What do you see? What do you notice? Things that you observe from this eyeball on these things.
They all are affiliated with person. So there may be some kind of person representation in there somewhere. Ma may. Maybe it's this dimension. It's hard to tell. But I would say across the base, I see something in common a little bit, some personage. What else do you guys see
how about this which pairs are most likely
he and man or man and woman, man, woman.
And so again, if you look at, demand involvement, it's like.
you know, these are the same. These are close, close, these pretty little different. It's pretty close. So why would that be?
Are man and woman so close?
And think about all of the ways you'd see them as words quite interchangeably right?
Right? Men do something. Women do something right. Have a lot of commonalities. King is a little more specialized. Teams don't do everything that men do. Teams don't do everything that women do a little weird. You see things more around other kinds of content. So it's a little different. So I would say, man and woman definitely more similar than our man and king interested.
Now I ask you this. What do those dimensions be? We did a distributed representation for, like banana.
or like, you know, it's a fruit, or it barks or doesn't bark. What do these mean?
We don't know. This is the trick we don't know.
This is the hard part about this stuff is like. typically we don't know all we can do now is after the fact. Go look at lots of words and then try to go. Okay.
I think this maybe means, you know, somewhere.
But typically we don't know. We just know, somehow encoding the notion of madness, womanness, keenness. Okay, so here's a bunch more
queen, woman, girl, boy, man, king, Queen Walter.
So let's start with water. Water is the only thing that's like, not a person or personage
does water look different from the others.
Yeah.
I mean just sort of eyeballing. In like definitely like this dimension.
I see some differences. I feel like water is somehow different, makes sense. Water is in different contexts than queen, woman, girl, boy, man, king.
How about girl, boy
versus girl, woman
grow boy, both like young people.
Girl Boyd.
These are pretty close, I mean again, a few differences. little different.
So
pretty similar makes sense.
because you'd expect to see girl and boy many of the same context. Okay.
how about king and Queen.
he he looked pretty similar to me.
Yeah.
now, what's cool about this? This is just a few examples. Right?
So one of the neat things about this. So so what does the takeaway from this? The hope is somehow these word embeddings are somehow capturing some semantics, some meaning
right. And if you compare things. It kind of makes sense now. So there's some semantics that's the hope, right that somehow capturing what these words mean.
So you can start to do some kind of like funky like arithmetic.
And so you do this stuff, you say, what if you took the vector for King? The embedding for king subtracted out, man.
added, woman.
so here's King. Subtract out, man, add, woman. So you end up here
and you look at it. It's kind of close to clean what what?
And when I say P, minus man, I literally mean like, whatever this number was, it was like 1.1 minus whatever that number was, whatever that number was. Give me this number. Okay? So you're just doing like dimension by dimension addition and subtraction.
And at the end of the day. It's kind of like it makes sense. If you were thinking about, like all of these words
in some space, the 50 dimensional space, it's hard to for us to visualize right? But you can do lots of this stuff you can say like, I mean, you could do Queen minus
a woman plus man, and you will cover something close to key.
Okay.
this kind of like embedding arithmetic. When this happened, this is all like 2012, 2013, 2014 stuff is happening. Everyone is losing their minds. Now, are you serious?
Very easy. This is totally crazy.
So check this out. So then people say, well, it's hard to visualize this. So let's project this down to just 2 dimensions.
Okay, cause we can visualize 2D. So if you know how to do that. There's like principle component analysis. There is the there's lots of these other tricks. It's still dimensionality reduction. But we take King and we try to figure out like, if we'd only had 2 dimensions on 50, what would it look like? Okay? And so they did that for a bunch of words. And these are again. These are glove embeddings.
But I want to see this isn't a 2 dimensional space
these relationships, which are, notice. brother, sister, nephew, niece, uncle, and they're all kind of separated by this obedient brother, and he went up. We find sisters.
If I click go, and I only do that direction. I can find ants.
Okay, Sir Countess. So, in other words.
if I just knew the direction
from like Duke to Duchess. and I just applied that to earl, I would almost pull out count tests.
So the idea is somehow like where these words live in these embedding spaces is somehow capturing these semantics. Check this out. These are companies and their Ceos
same deal. It's kind of like, oh, you go a little bit right, and you find the CEO.
This is superlatives check this out
slow, slower, slowest. It's like all the same kind of geometry.
Okay.
dark, darker darkness, loud, louder, loud, strong thunder strongness.
So again.
if I just know the one word I could recover the others, and like, regardless of which word you have, they all have the same sort of like superlatives like the Ur is up, and then the S is farther over.
over and over and over and over again. Okay. so again, we look at this stuff, people again lose their minds. They say, this is amazing.
These word embeddings somehow are deeply capturing structures of language. Okay.
we're gonna use this stuff next time.
Oh, and on your homework
to do like ranking and retrieval to do search.
So I don't know about you. When you look at this you probably go. I can't have some words. Bro, like, I know, like
I'm not stupid, I know, slow, slow, or slowest.
but but no one told it. This. We just learned it.
I just gave it a bunch of Wikipedia documents or a bunch of web documents. Magic happened. We learned all of these dense embeddings.
and the stuff is there?
What
do you see? Why people are freaking? We're freaking out.
And you're like, Yeah, we got chat gpt. This is baby stuff
to run. We gotta walk to walk, we gotta crawl. This is us crawling. Okay, this is us figuring this stuff out. This is amazing.
Okay. maybe you believe me, maybe you don't. you should. This is cool. bye.
seem pretty good. How do we find them? That's the big question. That's where we left it last time those are glove embeddings. So there's this whole like glove protocol.
We're going to focus on one called word to vet makes sense. You take a word to vet. It turns out. In fact, this is the paper I showed you last time. There's our boy Jeff.
etc., etc.
This is word to Vec.
People went nuts with this after word Tovec came out. and so
everything was in to that. Everyone's way of making a word to that I could also do. I don't know. What do you wanna do
you do? Nodes to that. But in a graph we can do image to back. We can do sentence to back Doc to that Tweet to back.
Dare I type in
to that
curated list of 2? Vex?
This is just awesome to Vick, or is this the list.
Yes. Curated list.
Oh, yeah, okay, so sorry. There it is. There's word to vec. I don't want the. but there's GL. 2 vec.
But roll to vec. I think that's diff to Vec. Dr. Vevec. graph to Vec Tweet to Vec
batter pitcher baseball people to Vec.
are we watching the baseball playoffs. It's a big Texas thing happening. And we tend to. I think people tend to like, hear the Astros, I think.
Did you like the Rangers? Yeah, yeah.
you're a Ranger guy. Here we go. Oh, yeah. Who? Who here likes the Astros?
They cheat right? They're cheaters.
I don't know. I don't follow baseball. I don't follow baseball, I know is they're cheaters, right? Whereas the Rangers, or what like paragons or virtue, get out of here.
Okay, okay. I don't know. Illustrate. Sorry I got a illustration to that
Lda tovec sentence to vec wiki tovec.
I don't know why it's topic. There's got to be a 2 in there. Entity, defect.
string to vac. I guess. Note to veg, item, to back. Author to that playlist. Yeah, right? Do better do better. Since the vet met to back game to vet
emoji, to vex
image phrase, context.
Ingram, app to the back. Are you with me?
Move you to back, PIN to back Wordvag. But but but and
Wikipedia to veg, these are all capitals. Patient
pack it.
Features. Okay? So
action to that.
Okay, you got it. You keep. Okay. That's the end of it. Phrase to back gene to that. Okay, so phrase to that phrase to vex. So I show you that praise to that. Amen.
okay. So this came out and you can see it like, launched 1,000 votes. Right? People are like, this is amazing. We can make everything and then make a dense representation. Okay? And the whole idea is, if words look like this.
Well, what if I embedded, you know, like the baseball players, somehow baseball, you find relationships. Oh, the shortstops and the pictures are somewhere different in the space.
I don't know how they did that, but that would be the idea. Okay, same deal.
Put everything into a dense representation. We can go crazy with it. That's what people are doing. This word to. Vec, okay.
big idea. We're gonna learn those word vectors. Okay? And I alluded to this last time. But today we're gonna really bring it home. So you're all gonna understand how word Tovec works.
But you'll be able to basically almost do it. Okay.
But the big idea is you just give me a boatload of text that would be like Wikipedia
using all of Wikipedia, is like one long sentence. Okay, you read it left to right.
We have a vocabulary, our dictionary. We know how to do that. From the very first day of class we have a dictionary
we go through, we walk through.
and we consider a center word, and then we consider all the words around it up to some window at some parameter, we can decide 2, 3, 4.
Okay, so we have a center word, and then we have an outside words. Okay? And the main idea is we're sort of measuring some similarity of the center words and the outside words, okay?
And then we're just going to keep adjusting the word vectors to basically figure out like to maximize or have some objective function that we're trying to maximize. Okay.
so that's the whole idea. We're gonna do it in just a moment. Okay, so just visually, what we're doing is
problems turning into banking, pricing as into in my center word, I define some window, maybe a window of size. 2.
We decide. And I look at the words on the right side, the words on the left side. So I'm gonna have the center word. Then I have the outer or context words.
okay? And
I'm not gonna do the full derivation of this bit. But you can imagine thinking like this.
given, that I'm looking at the word into the beginning. What is the probability of banking is right here
given that the middle word is into what is the probability of crises
given that this is where it's entered? What is the probability that this is coming?
Okay?
And so we're basically just trying to find those probabilities.
Okay? And if we do a really good job.
we find the best probabilities that explain the actual data and words we've seen.
If we can do that, we then will find our word embeddings. Okay.
don't worry. If you don't didn't understand. We're gonna walk through it right now.
So the idea, then, is like the center where we move over. Now we're banking is center. So I have a different set of outside words. Window 2.
Okay.
make sense. Ish, okay? So now we do.
okay, yeah. So visually, we're gonna end up getting 2 sets of embeddings. Okay. those are embeddings. And I'll show you in just a second. Okay. but the idea is we're gonna take like a center word.
We're gonna go find this embedding here.
We're going to multiply that embedding by the embedding of all the outwards. And the hope is we recover
the word on the outside.
I'm gonna show you how this works in just a second. Yeah, so this would be like the sort of in and out. So the idea, this is the center.
This is the center word, and this is the the outer or context word. So let's do an example. If we need a real sentence, I don't like this sentence here.
We'll do like
the cat
scratches
my knee.
and so, if we said like, let's say, cat was the center. So that's the center.
Then these guys are our outer ones. And maybe there's something back here. Okay? So the idea would be
if that word was cat. Maybe that word is scratches.
Okay? So the idea would be like, it would be really cool if we took that. Don't worry. These are the embeddings. I'll show you just a second.
We find that cabinet.
We then multiply that into the other embeddings, the different set of embeddings for all the words and the dream would be, you would find scratches
that would be really cool.
That'd be really cool.
I'll show you how this works. Okay, the setup.
Okay. I have a boatload of documents. Consider, let's say, a billion docs.
Yeah, this could be Wikipedia.
some web pages whatever.
Okay, I got a bunch of documents
and we could read them.
read them word by word.
So we just walk down the sentences. Okay. so out of that, we're going to get a dictionary.
a dictionary
of our unique words. We know how to do that already, right?
And so that might be.
you know, there's cat. There's Kitten.
There's scratches all these words.
We're gonna call the dictionary V, and the num oops
and the number of words.
This is the size of the feet. That's our vocabulary size. It's cool.
What is our goal? Right?
We want to find
embeddings.
or some dense specter.
for every
word
in our vocabulary. And so let's say, N is equal to 50. Then
we want, like, 50
dimensional vector
or each of our
words
that makes sense. So we have vocabulary. Yeah, kitten scratches of all that kind of stuff.
It's really long, you know. Maybe maybe the number of maybe this is like a hundred 1,000.
And then we say, Okay, we want 50 dimensional vectors for each one of those.
And if we have that we have our embeddings. Okay? Like, if we go back to King, if you did that at some point you would say, well, by golly.
there's a 50 dimensional vector, for King. We're trying to find this.
but not just for King. We want it, for
we want it for king, queen, woman, girl, boy, bubba cat catches, scratch, etc., etc. all and on. No, no, no, no, no, no, okay.
that's the dream. That's what we're trying to do.
It seems impossible. If you were to ask me.
Okay, it turns out people had already done this before word today.
What do you say? Heresy? Impossible
that people weren't using machine learning? They were just counting stuff.
But basically, like we said already, so like, here's some words. Here's some words nearby, and there's different ways to do it. The basic kind of count words near each other and then do some normalization and tricks like that. Okay, it's kind of like some rules of thumb. But you didn't actually learn anything. So the big insight here is, we're gonna use Ml.
to do that. So if we're gonna do, ml, we need training data.
Oh, no. You say. what is our training data? Again?
Right? And so we have the sentences like the cat
scratches
my knee.
So we're going to take those sentences. And we're basically going to transform them into pairs. So before I was showing you like window of size 2.
To be honest, we don't really care which position you're in. We just say you're in the window, or you're not
so in this case, you would say like, if the window was size, let's say
window size equals one. Then we would say.
we need.
Then we need the center word
followed by the outer word, and we have a bunch of those.
So, for example. what are the what is this? Give me a central word. Here, let's start with cat.
So what is an outer word for for cats?
If the window is size, one cat
scratches cat
the
okay, what's now we we then we walk over. So now my center word is scratches.
What's my center, or what's my outer word for scratches. Pat
and Mai? And then we step over, and now our central word is my
my scratches.
my knee.
and then we walk over, and it's me. and we go on and on and on. So
that's my training data. That's what it looks like. Okay.
when I give you a billion documents, all you end up doing is
forming these pairs.
And so this is now you're trying to think about with this. So over a billion documents.
right?
Have.
it's gonna be near like, what?
But words, what words come to your cast? My. the wine. fine tapes.
So you can imagine, like overall. We're gonna have lots of words near cat in the window that tell you a lot about cats
to go back and think about man and woman. So if man is the center word. think about all the things that man would do.
Man sleeps, man walks
and toss
and talks about football
and boring. Woman walks. Woman talks
so all of these contexts where the center word. If you swapped in something else. Like all those context, words are quite the same.
It kind of gives you the sense of like, oh, yeah, yeah, it kind of makes sense that like.
if you look at all of the kind of write-in words
they have something common left side. I think those words are like, there's something going on here. We haven't learned anything. That's kind of the idea
that we're doing. So everyone buys. This is our training data.
Okay, so trying to get the big amount of documents or just the pairs.
it's I mean the the documents we then transform into these pairs.
I'm just my question is, when we're training the model just putting in the pairs.
Yeah.
let me copy.
Add page by. Why is it not let me paste
based.
Give me 10. Okay?
Because everyone buys the training data. Okay, so what do we want to get out of this? We need our embeddings. Right? So we're trying to find find our embeddings.
It turns out word defect has 2 kinds of embeddings based on. If you're the center word, or if you are the context word.
okay? So I'm gonna draw them for you. Typically, we we call them Wn and W out.
okay
in
fee.
I'm I'm writing this way, just because it makes my life a little easier.
What is in
in is the what
N is no in is that our dimensions? This is like our our number
call it like the the number of dimensions
in our embedding.
like 50. What is the size of V,
that's our vocab size.
which is like a hundred 1,000. So just keep this in mind like
this is 5,000. 500,000.
This is 100,000 by 50.
It's not scale of scale. Now.
what I'm trying to argue here is that
that is the embedding.
let's say for cat.
So what I'm saying is, we want to find the embeddings. This is just a bunch of embeddings. When I say embedding, I mean, it's just a bunch of numbers, right? Like we did matrix factorization. It's just a bunch of numbers. But the idea is like this is, this is the size of my vocabulary. So these are. Each one of these columns is an embedding for a different word. This particular column is the embedding for cats.
That's just the distributed representation. Exactly. That might be the embedding for for for boat.
or whatever. Right
then, this is also a set of embeddings.
And so you could imagine this is like the embedding.
or, let's say, scratches.
And so my whole point here is like the in and the out are kind of like. This is when
this is kind of like. The words are in the center.
And this is for when the word is in the outer or the context.
so if we go back here and we said, Remember, we had like cat scratches. So I'm saying, it's for cat scratches. We can find the embedding for cat. It was. It was the central word, and we can find the embedding for scratches. It was the outer word.
Now, when I go to this other sentence scratches my.
I would go find the embedding for scratches and the embedding for mine.
And this is kind of weird that we have 2 sets of embeddings depending on. If you're the center word or you're the outer word.
But it's all the same stuff. It's just a vector a bunch of numbers representing the thing
is this cool
questions about N or V, or what the heck I'm talking about. or anything. Yup.
Great question. Cats embedding numbers here, and let's say here would be the exact same. No.
very good question. No, there are 2 sets of embeddings. They may be different, and in practice
dependence. Sometimes people will take them and average them. Sometimes they'll just use one or the other, but they are different.
You can think they kind of mean different things. One is like the center, one is the outer. They're not always the same question. So that's like a direction. And then, whenever you're going from out.
you can kind of think about it like that. That helps you think about it like the the kind of the point of them is different.
We have 2 sets, not the same.
The hope is, I mean, you would probably think that like
relationships here would hopefully be similar to relationships. Here
2 sets good question, other questions.
5,000 100,050. Right?
So remember, when we did, MF,
our goal was to find those latent factor matrices, those numbers.
Our goal now is to find these embeddings find these numbers. These are so. They're like latent factor agents expecting things. A bunch of numbers.
Okay.
so what we could do is we could just randomly fill them up. Put a bunch of random numbers in there. Okay. but we don't have any like objective. We don't have any goal right now. So I'm gonna try to walk you through
like, really, what the goal is. Okay, this is this is super straightforward, I hope.
And I say that
everyone's cool these matrices.
Okay.
okay.
I'm trying not to make a joke
about athletics or anything like this. I know we talk too much about athletics. You talk about other things that are inspirational art
movies. We love movies.
We'll talk about movies. We'll move you to that. Watch this weekend.
I wanted to go see Taylor Swift eras right, but I did not make it. We didn't have it. We're gonna go next week. 100 million dollars, big hitter.
We love it. Okay.
So check this out. Now, I'm going to blow your mind. Okay, when I say, blow your mind, I mean not really.
Remember, this is my W in. I'm screw writing the W in there. In fact, I'll erase that
this is Wm, okay?
And so imagine that's my cat embedding. So this sucker is WN,
okay, let me ask you this.
if that is a one by V,
and it's all zeros.
except for that is cat.
If I multiply this matrix, my WN, matrix times, this really long vector. where the only one is passed. Where do I get out of this?
Do you guys remember, be the distributed representation? Remember this. Yeah, you guys did that. So what falls out of here is just a vector
it's a vector for the center word. Okay? And what is what is the size of it
in by one.
So
whoops, whoops.
And so what is it you said? That's that's just the cat embedding.
Okay, so this is just a lookup like. let me let me color like that is that
there's just a lookup. Right?
So sometimes when we start doing more of this Ml, deep stuff, we'll call it like a one hot. Vector, the only only thing, only one thing is one.
But all we do is we just go 0 0 0 0 0 one grab, that element. one grab that element. So we're just pulling out that vector, it's just a lookup. But that's the cat embedding as a center word. Okay.
I might need a lot of space. Now, now, 2.
Now, that's going to take that same. Now we're going to use the W out.
which was V by in. So this is W out.
okay? And let's say, that's that is scratches embedding. Okay.
what if I took that and multiplied it? Buy my one by in this is a.
this is the center word. So this is cat.
Okay. So
if I took my out. which is just my another set of embeddings for the words in there the outer word. and I multiplied it by
this in by one cat embedding. So there's a number times a number plus a number times the number plus a number to move. I get something.
Okay? And so what is the size of the thing that's going to fall out of this?
It's a V by one.
and each word is like
and
at some point in here there's scratches. The point is, it's my entire vocabulary, and we're gonna get some numbers in here. And I'm just gonna brighten some fake numbers.
And you can think about this is like, this is like,
basically, our second step is like, we're basically like generating
a score vector.
for the center word
and all
other
outside for our
on text words.
So let me walk you back through that
we have some in embeddings. I don't know what they are, but right now they're just random numbers. Who knows?
I just say for my training. It is cat scratches, so I say, go find the cat embedding. Whammo! Take the cat embedding, multiply it by my out embeddings. So now I have a score for cat and a cat and and cat scratches. I have a floor for every other word in my vocabulary. With respect to cat.
Okay.
it's kind of like
you're taking the cap and embedding vector
starting point. And you're multiplying by all the other vectors. Yes, in the vocabulary to see where all of the other ones will end up in relation to that. So we're kind of measuring. Now, we're kind of like a similarity.
Some kind of score knows their negatives in there. Right? So this is the cool thing. Now, I'm gonna pull your minds.
Okay.
You guys know, Softmax.
Anyone in in words define what the softmax is?
Is it easier than the hard Max Softmax some maximum?
What does Softmax do, you guys?
Some of you are like, Kev. I can't answer, because I don't know it. And I respect that.
Yep, just to make the data. And there.
yeah, it's like a transformation to make the data into a probability distribution. It makes it all positive, and it makes it sum to one. So check this out the softmax.
We use this a lot.
Okay, I'll go over for just a second. Here all it does is takes a vector of real numbers, real numbers, meaning they could be negatives and positives
and turns it into a probability distribution, a probability, this probability distribution just has to all be positive and add up to one.
So, for example. if my venture real looked like this. maybe 1 1 5 0 point 4 0 point 2 1.2.
We do the softmax. What is the softmax? We exponentiate each thing, and then we divide by the summation of the exponentiation exponentiation to the power of
so check it out.
and everything comes positive. But notice. The negative is the smallest number.
Right? That's good. It's maintaining the order. But negatives are still smaller than positives. Bigger positives are still bigger than the other positives.
Okay?
Then, given this sucker.
Now we just normalize it by the summation of that and turn it into a problem distribution.
What's cool about this is, oh, okay, like, turn that into a probability distribution. Yeah, the big numbers output 5, 3. The small numbers point 0 4. Give the numbers or whatever it adds up to one
what?
This is. Soft magnet. This is cool. This is a trick. It has a terrible name. I think
it's confusing. You're like, what's the maximum? It's not really maximum. It's like a transformation.
But it's kind of like a soft. So it's like a more like probabilistic man. I mean, maybe you can convince yourself question.
why?
Why do we do this? That's we're getting there. This is a softmax. It turns up. It turns out you use this all the time. And this world of like deep learning the machine learning stuff.
I'm gonna show you why we need it now, does everyone believe this? You can go do this on your own. Go try it. It's fun
turns vectors into probability distributions. probability distributions. It turns vectors into probability distributions said that 10 times probability distributions.
It turns vectors into the product.
Wonderful.
I'll get this out. Now, I'm gonna we're gonna land this plane right now. Okay.
so go back. What are we doing right. Now. we say, look.
I have cat scratches.
I go find the embedding for cat, wamo.
I multiply cat by all of my other out embeddings for every other word. I get a bunch of scores, a bunch of numbers.
Now
let's see if I remember how to do this.
Check this out. Now
I'm gonna softmax that
it's still one by V,
but I don't know what the numbers look like. But let's say it's 0 2 0 0 1 0 point 0 5, a bunch of numbers.
Okay.
you guys believe that I can softmax it. Cause we just did an example.
We did an example with 4 numbers.
Okay.
I paste it there for some reason.
I softmaxit. Okay. Now, here's the kicker.
What was the what was in our training set? What was the ground truth here.
yeah. Like, what was what was the what did I? Input what was I looking at? Remember, I had a bunch of pairs. What was my pair?
We started this whole thing out. We said. well, I didn't. I guess I didn't say it. I said it in words, but I didn't write it.
We said, we cat scratches. Is that probability? That? So here's cat scratches.
That's the cat embedding.
So now
in this 1, one by V world. What does the ground truth look like? What was the actual word? I observed.
Tad was the input scratch scratches. So what does this vector look like to represent scratches.
They're all zeros except for scratches. Okay? So here's the thing. This is what's cool.
is it cool?
We just took 2 pairs.
a cat and scratches one pair
tunes. Okay, cat scratches. We bump up the bomb. We got a number. Oh, it should have been this. We made a mistake. We made an error. What if we went back and changed all the parameters, all the embeddings, to make this error smaller?
Let's do that. How do we do that
gradient descent
both time. Yep.
See, you guys, on Wednesday
where we're gonna finish this little story, and then we're gonna apply it to retrieval. It'll be great. See you, then.
bye-bye.

No, I am starch
start no audio.
Everything. Linkedin.
Hey? Howdy?
I'm not wearing my normal room shirt on Friday always where my daughter dance teams.
but tonight it was called the Bob and Zoom. That's where all the girls dance with their dad, although you already have any interest. Wanna go to the high School
football game at halftime tonight I will be on the floor dancing to my girl.
It includes snapping. twisting, turning.
I will, I will sashay, I will kick. It's all gonna happen so. But if you can't make it. it will be documented 100% guaranteed.
And because she's the senior historically. When I've done. I've done this before.
I'm not on the front line, so you can't hide.
but because she's a senior, I'm on the front line, so I have no one else to look at to see how the movement going. So there! There are kicks and stuff, but it will be filmed. I can't guarantee you'll get to see the film, maybe. Still, a still image will make it some. You'll find it. It's a it'll be out there. It'll be out there anyway. That's my life.
like always folks.
So the midterm, I returned. If you don't have your midterm
you can pick it up from the Ta
during his normal office hours, or you can meet him, or you can just randomly find him in his office. You can get it from that
homework 3 is gonna be out soon, ish?
So we'll figure out what that means. It'll be fun. The project proposal.
So anyone here have a team already like you think you're locked in. Very good.
Have an idea what you're doing
as you cook up ideas, though. Please feel free. Feel free to ping me, and I'm happy to give you feedback any other issues. We've got
numbers.
you know. Watch the football game last night.
Is anyone going to go see the Taylor's with movie.
The movie is going to be a huge hit.
If they had to add, they've had to add extra showings. I'll be there, so I'll see you there. I haven't decided which era I'm gonna wear.
That's right. You dress up right. I'll be dressed up.
I'm gonna dress up as one of the boyfriends that she broke up with and wrote a song about, okay, anyway.
So where are we folks just to kind of reset you? I showed you this last time. Remember, we spent 7 weeks kind of doing all the foundations. And now we're here. Okay. And what's exciting is we're now moving into advanced stuff. We're heading towards Llms. All the hotness. It's gonna be great.
So we already know some basics of machine learning. We're gonna do some more machine learning. But again, I'm gonna present it in such a way that even if you've never taken an Ml. Course, you're gonna know what's going on. And if you take an Ml. Course, my hope is you're gonna see it, maybe in a different perspective from how you've seen it before. Let's first start with the headlines. It's all in the news these days. Google, Turning Web search over to the AI machines. This is happening right now. And oh, this article was printed, this article came out what year?
2015. So we've been talking about this forever. Okay.
the same headline is basically used today. Oh, okay, being is moving, using chattypt Google, same deal turning web search into using some notion of AI, okay, so this is 2,015. We're gonna talk about like what was happening around 15. We're also gonna move forward in time. So
here's a quote from Chris Manning, professor at Stanford, co-author of your textbook. You have a textbook. Yeah, we've had readings from this thing.
He's one of them. Okay?
And cigar is the big like, information retrieval community. So this is a 2,016. Okay, this is around the time deep learning had kind of started to hit.
Okay? And so the idea is the community us, we're building search and recommendation. We're saying, Wait a minute. All this deep learning big Ml models is happening. What do we do?
And basically say it's powerful. It's amazing. However, there's big, big pipe. Okay. And so basically, we need to be careful as a community that we don't get like blown away and have some irrational exuberance.
Having said that, I just want to show you. So again, Cir, this community of Ir, researchers of practitioners, of Googlers and Microsoft people coming there and talking about what they're working on.
These are the papers over time that are working on some aspect of neural ir, some like a deep plus ir models. Okay. that's 42%. So he said, this is 2060.
And so the idea is it just totally shot up? Okay, so these methods were like kind of taking over. We're all super super excited. I don't have it past this, but it's even higher now.
And I would say, like these days, probably like.
kind of research, using some kind of machine learning some kind of deep model. I would bet it's over 90%. Okay.
So even if you're not doing research on the specific technique, it sort of assumed you're going to use it. It's like Tf-idf and vector space and all that stuff that used to be. Of course, that's a part of it.
Same deal. These methods are now just a part of it. Okay. 2018.
These dudes win the touring award. You know these guys.
Ngo.
Jeff Hinton, young Makoon. Okay? So they win the touring award for their contributions to deep learning. Okay.
so again, the deep learning revolution kind of hit only a few years earlier, 2,012, 2,013 by 2,018. They're getting the highest. This is like the Nobel of computer science, right? These are the big dogs. If you've been following, like, I think this guy was just on watch 60 min. But
talking about the concerns that, like AI is going to be sentient and like overrun us. And we should be scared. So basically, he was trying to sort of say, he's kind of like Oppenheimer, right Oppenheimer versus the bomb, and then spent the rest of his life saying.
that was a big mistake. I shouldn't have done that. And so he's kind of becoming one of these people where he's like, I help birth and create this new thing.
But I'm really worried about how it's going to devastate the future. Okay.
I'm not sure what his answers on all this.
Okay. but anyway, so if you don't know their faces. This is a big deal. They kind of helped launch this whole deep learning revolution
which has then impacted us in. In a big Lee way, now
2018. So we can kind of give you some history here. So I was 2018. These guys win the touring award 38. This guy, Jimmy Lynn.
he basically says, Hey, hang on everybody. All this deep learning neural network. Big Ml, models
is hype.
It's hype. And we'll talk about this in a few weeks, maybe maybe next week, where he basically said, I can actually beat all of these neural methods
using just old school like, bm, 25 stuff.
Okay, so this is, wait a minute. These guys won the touring award. But these dummies don't know how to really use it. And in fact, it's a joke. Okay, this is 2,018.
One year later Jimmy Lynn, same guy.
He did a repentation. He took it back. He said, basically, I was wrong
that the stuff really works. And so in one year something happened.
No one guess what happened between 18 and 19 wire drinks group.
Hey? Here's money 28 and 29. This big change.
It's like, it's not Jack Gp, I know the whole layers. Okay, but it was kind of these precursors. So if you guys heard first. we can prefer like not burden, Ernie.
but like the ERT. Bird, there's like a big language model. So the idea was, wait a minute. The big change was now. There were these big models that encoded lots of information that suddenly made these neural methods really powerful.
whereas kind of pre this, we didn't have it. So we'll talk about
Bert. And then eventually, you know, we have Gpt and all these other. You know
all these other models. But that was the big transformation. And so we're gonna get there. I'm just trying to give you kind of a history and so now we kind of live the aftermath of this, where everyone is using all these methods these days. Okay.
so what we're going to do.
I told you, this is the advanced stuff. We're undergrad class. Okay? So we just learn this, the basics. Now, we're doing advanced. Yeah. Yeah. So we're gonna talk about
early methods early days of neural methods. And the thing is, I'm gonna present it in such a way that you don't need to have ever taken. AI. So you don't need to understand neural networks. Okay.
is that cool?
You still need to understand? Oh, oh, no, don't say it. You still need to remember the matrix
and some matrix manipulation. But we're not giving any neural network. Any. AI AI AI stuff. Okay. we're gonna talk about word to vec. Anyone heard this word to Beck.
This is hot. This is kind of precursor. This is like 20 year on. This is like 2012, 2013. It's like precursor to Burt precursor to chat. Gp, okay.
we're gonna learn about these methods, how we can apply them to Ir. We're gonna talk about the height cycle and the pushback.
and maybe we'll talk about Colbert or Colbert.
Maybe we'll talk about it. Okay.
this is where we're headed for next 2 weeks. So the point of what I'm trying to do here is say, from a ranking search perspective, right? You guys know how to tokenize. You guys are how to build your index. You know how to do free queries, additional indexes. You had to do wild card queries with your permit. All that kind of stuff
that gets you basically to the bound. And you do some basics of learning to rank right
now, we're gonna get super charge. We're gonna learn like, how do we take all that stuff and move it to? Basically like you were ready to go walk in the door. And you know, like, how these systems are really built today. Okay.
yeah.
that's amazing. Right?
It's amazing.
It is, yeah, it really is. It is I'm not. Gonna I'm not gonna jump around like I've done before. Let's get started. Okay?
And this focus is a big deal.
You don't get this everywhere. Getting here.
Let's get started. Remember this picture.
Random user says, Meet me at midnight. That's my query.
We build a ranked list of albums. The cab's algorithms.
We can mail them out to you. Okay.
this is basic ranking. Right? Query comes in. We ranked it. We're done. Okay.
I want you to think about how we're doing that matching between, you have a query. And you know, we say, this is like a document, right? Somehow, we're measuring some similarity between the query
and the document. Right? That's kind of the thing that powers how we do this.
And we said that a similarity could be like a cosine. Maybe we learn some way to do the fun. That function becomes a little more complex. And we learn some rancor. But the end of the day we're doing some kind of matching. Okay, so think about vector space for people
when we match a query to a document. what are we really comparing?
So here's my query.
meet needs at midnight.
And then let's say, let's say my document. It's like, Hey, yeah,
you know it's ice cold, or whatever dot dot dot dot dot.
How do we compare this document to this query in the vector, space world.
Rosine.
sometimes scores for the different axes and the axes are terms.
Okay? So there's not any deep understandings just like.
Now, how many times words appear here, kind of word appear magic.
And so we mentioned before. So like if we say midnight
anyway, it's in the document. It says, you know, 12 PM.
There's no match.
because it's it's very surface level. It's just trying to say midnight and 12, is it? Whatever this is like we. That's why, when you guys submit your homework, it says, 1159,
no, okay.
So for meet, imagine doesn't say meet. It says, what's another word for meet
convene, join. Whatever the point is, we're only superficially connecting, like, do the words match.
Okay? So the point is.
we'd like to be more robust. We'd like to really understand the queries. So when they say, Meet me at midnight.
what do they really mean? Not just the surface words, but a deeper meaning, and if we understood that we could do better than that, so the idea is, our method. So far, I would say, are very superficial and very kind of brittle
make sense.
make sense. If it doesn't
question time.
we cool. Okay.
so
we want to make it more robust. So one idea is called query expansion.
So basically, we have a query. and then we just expand it by adding extra words.
Okay? So if they said midnight, we say 12 am as well.
Okay? And typically, you know, we could do a manual for source.
Okay? Or we could try to learn word similarity. Let me show you a real example. This is pubmed. This is some like scientific resource.
If you go search for a cancer.
you type in the word cancer. And you're searching this like database of documents about medical literature and whatnot.
It translates the query cancer. And it says. Really, you mean neoplasms or neoplasm and other fields or cancer.
I did not write neoplasm, but there is a manual expansion from cancer to neoplasm.
And so now it's doing a match over the document space for cancer or the word neoplasm.
Okay? So the idea is more robust.
You said cancer. But you also could have meant neoplasm, because maybe that's the more technical term for it. Okay.
does this make sense?
So query comes in. We read, we rewrite it.
Do that 10 times fast. We rewrite it.
Okay.
this is a manual to source. Some humans did this. Why would humans do this good things and bad things?
Why do we do this?
Get more relevant documents, great, great, great! Great!
Is it? Cheap and easy? No, we gotta pay people to do it. People who understand this.
It's expensive. Why would we do this?
Gotcha
don't have chat. Gp, yeah, free chat. Gpt.
and this is, yeah, go ahead. Yeah. And what kind of user are we talking about here at pubmed searching for cancer. Doctor, medical researcher. This is super high value, human life, etc., etc. We'll put some big dollars in here to really understand the queries people do, and to help them rewrite them by defining some manual for source. Okay.
now, contrast that with your on
Tiktok, or whatever
you know, you're on Google search, does Google have manual thesaurus?
But presumably not
right? Because why? Because why do they want to, you know, pay people to this crazy? Okay? However.
you could try to learn some kind of relationship.
Okay? So let's do some kind of query extension. Let's do an example. Okay, so come back here. So here comes, give me a you guys give me a query.
a regular query for CAD's algorithm. No. the query is, meet me at midnight.
We're now gonna rewrite that we're gonna add in synonyms. So again, we're gonna turn this into
it could be meat convene. Join
me myself.
I at
and midnight it could be night. It could be late, it could be 12 million
am. It could be whatever.
And so then we take all of this, and we send it to our still using our cosign.
So here's my question, what happens to our recall? Hmm.
compared to the exist? Yeah. Recall. Go recall, go up. Number go up stock, go up level
number, go up. What happens to our precision goes way down.
and I'm going to give you a bunch of crap. That doesn't what you want it, most likely. Right? So recall. Go up precision. Go down, boom
cool.
Let me give you another example.
So sometimes let's say the query is interest
interest rate an interest rate. Right now, interest rates are going up
right? So if you were doing like 19 synonyms, what are some synonyms of interest
in his life.
I like references.
I don't wanna do interest.
I'm into. I have a proclivity for
okay. How about rates? What were some of those rates for the speed. the rate of travel
score, right range. So imagine this is an example where, like just naively expanding the single words gives you kind of total nonsense, right? And so the recall may go up to way down because you kind of lost the semantics of interest rate, because we're sort of being naive and done okay.
And so
in practice, what you can do is try to be smarter about this, and I'm not gonna tell you more. But the idea is you can try to kind of be more intelligent about learning good query expansions. Okay? So maybe interest rate, you would learn. Oh, I don't expand it like word by word. I take the whole phrase and do it. Okay, always want to be careful. Now
let me give you a very clever idea on how we might do this. And this is, we can use the search engine to help us figure out word similarities. Okay?
And so this is like it has a very fancy title of the web based kernel function.
Yeah, it sounds to me. Sounds fancy. The idea is very simple. So it works like this.
So suppose I have a query.
They do some examples. I'll do it this way. Yeah, they do example in a minute. They say, like they have. Pdf. and you have a portable document format.
And so what they do is they basically say they want to understand, like.
how similar is Pdf, the portable document format. Okay? And so at the superficial word level, are they similar?
No, no, not at all. Because Pdf doesn't show up here. There's no matching.
But we know that. Pdf, maybe you didn't know this. It's portable document format. Yeah, it is.
They're acting. It's the same thing. Okay? And so the ideas are going to say, take Pdf.
turn it into a query to let's say, Google.
okay. you get back a bunch of results. Okay. take those results
and count the words.
And then you take portable document format, turn it into a query. Send it to Google.
You get back a bunch of results count the words.
And then we say, okay, so what is the similarity
of Pdf and
portable document format? It's 0 here, right?
But then you say, well, what is the similarity of like call this x one, and call this x 2 instead? What if you compared the word that came out of the search engine.
So the idea is, I go to Google, I type in Pdf, I get a bunch of Wikipedia articles about what Pdf means and what it is. I go type in portable document format. Google tells me it's all about like.
you know, the portable Pdfs and nonsense always words describing Pdfs. And so if I have the words that come out of Google here, the words that come out of Google here, if I compare, and x 2, presumably it's going to be, not 0.
It's gonna be something right?
And so it's the same idea, you say, well, how similar are like, you know.
that's the thing. And then the taboo
something right? How similar are Texas? A. And M. And
where I always forget. Hello! Good to you guys, to you.
or, to be honest football.
Yes, yeah.
we're dirty, and what's not here doesn't get here. So just like, calculate that as, like, how many students tuition did you have to lay off which it's like.
it's totally not the same. You can't compare it like that.
Okay.
you guys have the idea.
We're basically moving from a world. Where, like, how do we compare 2 things where we, there's like 0 in common
to a way we can compare them where there is something in common. And so this is a paper.
and you can go read like this is their formal algorithm. And the reason I want to show this to you is because this is not that old. And all of the techniques they use is stuff, you already know. So they say, look, issue access, query to search engine.
Get the end documents back. Okay, compute the turn back here. Only keep the top turns.
Find the
fun centroid.
normalize. Okay, it's almost like, who is exactly plan
all this stuff.
It's almost like you learn it for a reason.
Do you have? But sorry
you have glasses where it's just like every week is just some random stuff.
It doesn't make sense. Yeah, yeah, you should.
Hmm.
hmm. Maybe there's a lesson there.
you know. Sometimes
I'll tell you why we're here, I'm just showing you this. Okay.
let me show you what they did. They just some experiments. Okay, just to kind of demonstrate in this case, using an external resource, a search engine. They basically say again, what is the cosign support? That machine? Svm, the cosine is 0. Why is it 0? They have no words in common.
But they say, what if we use their political kernel function? It's like point 8. So it's like 80% similar with a user search engine. Say, well, who is the Microsoft? CEO? Steve Walmer was higher than II don't know the time he was the CEO who is the founder? Bill Gates was higher for founder than he was for CEO. I guess Walmer was CEO back then. Okay.
but you can see it's not like always super perfect. You can say, like
Microsoft, founder, okay, founder of Google 1, 8, 9.
Bill Gates are 6, 7, 7. It's a way higher here.
it's very like it's appropriate that Larry Page founded like a web search engine over web pages. And so we don't talk about this anymore. But there's a thing called page rank.
So pager was like the original Google, how do you rank web pages? But it's named after the guy, not the web page itself.
So you guys get what we're doing here. It's like we're saying, if we just compare the surface level of the words, it doesn't make sense. If we could somehow like enrich our representation of words, we can do a better job.
I know this seems very simple, but this is like foundational to kind of everything we're gonna do from now on. Okay? And so, oh, yeah, there's more of these space exploration, NASA. 5.
Whereas vacation travel and space travel, even though they have travel and comments. They're not very similar.
Okay.
the classical Java Java is an island. It's also a language. So Java Island, Indonesia is high, whereas Java program is low. Right? So anyway, a bunch of cool examples.
Now. all of this is heading us towards what we call distributed representations.
That's true.
And so again, this is a big big idea. This is foundation of a very modern, like Nlp. Natural language processing very foundational for all of our ir moving forward. So we're trying to move away from very superficial representations to these richer representations. And so a distributed representation. The idea is something like, well.
you know this, we're moving away from like saying cat and kitten. And this is the cat access that you can access.
If we take the you know the the dot product, it's 0. They have nothing in common. So we're moving away from this
to trying to represent the thing. So a distributed representation would be something like this, you would say like, Oh, I have these characteristics like
it barks. It's fruits. It's in Long Gate, it has a tail. It's ovate these characteristics, these descriptors.
And so if I was representing banana.
I would say, well, banana is a fruit and then elongate. Okay, a mango is a fruit and then ovate
a dog barks and has a tail.
So if you could define all of these attributes.
we move away. And now we can say like, Oh, our bananas and mangoes alike. Well, yeah, they're both fruits. They both imagine this representative space.
whereas is a dog in a banana, a life.
Well, according to this, not really right.
This is a fruit. This is an animal. This one barks, this one doesn't bark.
etc., etc.
So the idea is currently we just represent banana by like banana, is like the axis, like the tenth axis in our vector. Space
or dog is like the 50 fifth axis.
But now, when we move to a world where all of our words now sort of encoded in some distributed manner. This makes sense. We don't know where these come from right?
And so the point we're not going to name these. We're gonna learn this stuff, I know, right? I know. But the idea is we're now moving words from just like sort of symbolic to the sort of distributed perspective. Okay.
Ms gold
cool. It's just like, kind of like we just did on this.
the example here. this Pdf portable document format, this kernel function. So we basically turn it into kind of a new representation where this was like
word, one word 2, word 10,
and so may. Maybe the first word was like adobe. And this this word was like adobe. And then what's another word that would show up with Pdf like a file
and like an attachment.
Change, Catherine.
And so the idea is you could kind of determine. This is like a distributed representation. There's still this, we're using words to represent a word
we're moving from. Just like Pdf, is Pdf, to Pdf is actually sector of like adobe file attachment dot dot dot, etc., etc. Okay. there's kind of like a precursor to this notion of it is in essence a distributed representation. Okay?
And so the idea is basically we can now measure like the similarity of these words.
if we represent words like by their neighbors, it'll tell us a lot about. If the words are related or not, you shall know a word by the company it keeps. So, for example, here's the sentences. You have the word banking.
So the idea is like is banking.
But how would we represent? Thank you. Well, look, we're around the government debt problems, crises 2,009, and regulation
Europe, India system, etc., etc. It's all these words
around banking tell you a lot about the word banking.
Does that make sense? And so imagine a different word.
Your favorite
they do
in they were, but they do live with.
you know they're useful.
That's good.
So what are the words around Taylor, swift
scale seating
Travis Kelsey's the football players dating what other words around Taylor songs, albums, touring eras Jess background. You've been you've been living. So she had a bunch of like, and she went to these like
the games industry chief. But all the press articles said she went to this jets game
so running fast.
If you can imagine other contexts where switch is going to mean things like pass runner programming. Sorry you also have the programming language swift. So you might see other things about like a Java C functional versus whatever.
So
the issue here, this is actually a very kind of good precursor. So we know, words have different meanings. Okay, so we're gonna learn basically how to use this eventually to get to this thing called word to Vec.
but word to vet learns basically a single representation for every word. So Taylor swift.
swift, running, swift programming language are all gonna get collapsed into one thing
problematic.
So later on we'll get to something called Bert.
And Bird is going to learn a contextual embedding. So it's going to learn the difference between Taylor Swift.
swift programming and running swift.
Okay.
But the common idea is basically look at all the words around the word, and use that as kind to help you build this distributed representation.
Okay, so
what we're going to do is we're going to. We're going to try to find these low dimensional vectors. Okay.
question before you go and read all this. Have we seen low dimensional vectors in this class already? Yes, those weren't words. Those were users and items and ratings.
Okay? And then we decompose it into those 2 latent major latent factors that were super dense.
Okay.
but that's the exact same idea. We now do it back to the word world. Okay? So the idea is
just like we talked about like movies, only have so many genres. So we have like.
who needs to be elderly
18,000 movies. And we predicted down to like 10 dimensional space or 20 dimensional space.
Same thing here is we're going to consider that, like the words really project down to a low dimensional space, is it maybe 25 to 1,000 dimensions? Okay.
this is where we're gonna have all of the important information
to be pushed down to low dimensions. Okay? And so the idea is, we want to go from like
just counts of words down to this word embedding. Okay? And so the dream is, we're going to end up with something like this.
Remember, like. And again, matrix factorization, we said, like this movie has these latent factors or this user has these blatant factors. Now, we're gonna say, this word cat has these latent factors, these embeddings. Okay, so this is like 1, 2, 3, 4, 5, 6, 7. This is some 8 dimensional
word embedding.
Okay?
Recall matrix
factorization.
So the methods we're going to use are a little bit different, but the end result is very similar.
which is before we had like users and items had since embeddings.
Now we have words have dense embeddings. You don't know how to get here. but that's the dream. Okay? So the idea is.
you know. this is a
distributed representation.
because now we have like these 8 dimensions. it's not like barks.
fruit.
elongate, ovate. it's whatever these dimensions are. But now cat is no longer just. Cat CAD, is this 8 dimensional sort of combination of these 8 different dimensions?
So this is what we call a distributed representation. And so the dream. the dream, I'm gonna erase all this. Sorry
the dream would be. We would have
if we had the word kitten.
The hope is, it would be something like point 2 point a negative pointone negative point 2 0 point 1
negative 0 point 4 0 point 4 0 point 3 something close by.
So that would be the dream right? Because cats and kittens are very alive.
And so is this
low dimensional embedding space. If I compared them using cosine, using whatever
the hope is, it would be very similar.
Same deal in our matrix factorization world. If I look at the embedding of the movie
as you guys are watching these days. Well
and part 4.
They're probably very similar.
If I compare the embedding for avengers or 7 and compare it to whiplash. small Indian movie about a drummer. Presumably they're quite different
things. But
I just got Trump's. Vicky's in a simple checkmate, sir.
very good point. Very good point. Same thing on the on the user space from Ms, right? Like
is that it'll also have limited.
Okay.
10.
Does this make sense? We want to get here? You don't know how to get there, but we want to get there. Okay.
And so what I want to understand is.
if we have all of this.
go back to our original ranking. Meet me at midnight.
Okay?
What I could do is I could take every word, meet me at midst.
I could find its embedding representation. I could average them do something, and I can compare that to all my documents.
where I translated all the documents from the words into their embedding representation. And so if in the document they talk about like having a party after 11, you know
that would be close to midnight. That's the idea. They would match. You know they have no words in common
that we need to read together.
But how do you phrase this Internet? Oh, yeah. Yeah. So this is also something funny.
The question was like, what do we do about phrases or exact matches? So yeah, I would say in the last many years mainly critical of the big search engines like Google. They say I'm searching for the thing. I can't find it.
because the old days they had more like exact match like I know exactly what I'm looking for.
like I'm looking for carefully. Homework, free homework solution. Pdf, or python. That's that's the query I do, and instead.
somehow approximate match.
and you lose the exact action.
Okay, so I would say, is in practice this, with this we lose the exact match.
So in practice we talked before about
when you have a query, come in, don't just you don't just fire off the query right? You can rewrite the query. So we said before, like the query comes in, you do the phrase version.
you do the this version same thing. If meet me at midnight, comes in. We fire off the meet me at midnight phrase, and we have still, like a phrase based ranker boom.
We see what results we get. We don't get good results. Then we fire off this one. Then we fire off this embeddings based one, and then we aggregate my question, I think, how do you represent Brady with these embeddings
at this point? We don't. So at this point you could say like, if if the phrase was, meet me at midnight. You have a meet vector, a me, vector, admit vector, midnight vector, and you had to combine them.
You could average them. You could Max them. You can. Then something like this at stable. You guys. Now.
how are we gonna do this
bizarre?
Okay? Many ideas. Jesus.
we're gonna learn about word tovec. We're gonna we're gonna use our Ml, to learn the right vectors
to learn this. Okay. okay? And again, you have all that you need to know, because you already know how to do
know how to do that
gradient descent.
But you know the item, the book, you know the concept.
We're not gonna be great at this practice.
But you know the idea. Okay, that's and so let me show you the setup here.
This has worked to Vec, this came out of Google in 20, I guess, is 2013.
this is word Tobeck.
this guy. He was at Google for a while, I think. And then he went to. I think he went to Facebook.
And now he's just a rich guy in in Europe somewhere. But he's a European rich guy. Do you guys know this guy.
Jeffrey Dean.
have we talked about Jeff Dean in this class?
Yeah, his email is Jeff at Googlecom.
Yeah, he's employee like he's in the first like 20 or 30,
and I'm gonna
sorry. Just hang on everybody.
We'll fix it.
Yes.
Oh, I guess I'll just put it back up here. It's fine.
Can you guys see this? I hope I haven't lost it yet.
So here's just to quickly look. He joined Google in 1,999.
And then he says, these are some of the areas. These are some of the things he's done at at the hang on.
This is very funny.
he built their code search. He built Google, translate some of early Google news. He did the initial development of Adsense.
He was one of the first 3 people who did Google ads. He did big table. He did mapreduce.
He designed, implemented 5 generations of their crawling, indexing query, serving just like he built Google.
thank you guys.
if you care about big table. The reason I show this, this is a very foundational
work on like big storage. And so there's Jeff d but I also wanna show you Andrew fights is A and M class of 99 and still Google employee.
He was like employee, like Number 100 or something. He's on our advisory board, and she fights. So he comes back to campus every now and then.
thanks so, anyway. So sorry, Jeff Dean, and sorry.
Jeff Dean.
let's do Jeffine fax real quick. Hang on. you guys know you guys know. Chuck Norris. Fax.
Yeah. Okay. Well, you'll get the idea. Yes, compilers don't warrant Jeff Dean. Jeff Dean warns compilers.
Source documentation
that these keyboard has 2 keys, one and 0.
okay.
you know, if these are kind of 3 answers.
yeah, this is, yeah. He builds his code for committing. Is it only type of compiler and linker bugs
we created the for world's first one over. N,
is also related.
So yeah, so anyway, this guy's very, very productive.
So if you know, he also did. Tensorflow.
you know about tensorflow?
yeah. And there's a he partnered with this guy, Sanjay Gimma Watt. And so there's a great article about them
that you can read. I'll I'll post it on the slack.
isn't isn't the New Yorker if you guys want to check that out.
the friendship that made Google huge and talks about how these guys work that they would code together.
so check that out and I don't want whatever's happening there. Okay? So
anyway. So word with that super super influential. Okay.
I'm going to tell you the big idea. And then we're going to talk about it next week. Okay? And this is going to be very fun, very cool.
The the whole idea of words is that is makes sense. We have words. We're going to turn them into vectors.
those distributed representations. So so the idea is, we have a lot of text.
Okay, just basically, you take all of Wikipedia and you just flatten it out.
Okay. every word in the vocabulary has a vector.
we start out is all just kind of random. Okay, just like in matrix factorization. we're gonna go through the we're gonna walk through the text.
And we're gonna try to figure out like, we're gonna try to force it. That words that are nearby basically are similar to each other. Okay.
and we're gonna basically keep up updating these probabilities. So updating these vectors to maximize the probability until we're done.
Okay. visually. we're gonna do something like this.
We have this word into, we're gonna try to figure out what is the probability that the next word is banking given? This word is in, choose. What is the probability that the second word is crisis or crisis given, this word is inch.
What's probably the previous word is turning given. This word is into.
And so we're gonna end up adjusting all of these probabilities by adjusting these vectors at the end of the day we're gonna converge these beautiful word to vex embeddings. Okay? And then you can define your contacts window. It can be 5, 10 to whatever. Okay, so have a good weekend come back on Monday. We're gonna learn word to vec.
See you, then
theory and.

Welcome back.
It's all like, you know. Talk about downhill running rest of this semester. No legacy of Thanksgiving. Right?
Yeah. It's gonna go downhill. It's boom! Boom, boom! Next thing you know it's Christmas and you're gone. Do you want graduating this semester?
Some of you are gone. You're never here again
going fine fine.
It's gonna happen before you know it. Cherish these moments. Cherish good to see you all.
it's not showing I hit the thing.
Now you can see I do have the midterm. I'll give it back for those of you who came today.
For your friends who aren't here. I'm not gonna bring him back ever again. They can pick it up from the Ta or
something.
Yeah, I don't know. We're gonna talk about the project today we're gonna wrap up Netflix prize. And like this is kind of like the second half of the semester. It's not the fourth quarter. If it was the fourth quarter, we would do this.
Say, we own the fourth quarter, we own it. Yeah. So it's the second half. We don't even know the second half. So
okay. we're not gonna do questions. Midterm.
You took it. It's graded it's here. What I say is this.
if you're 85 or above.
don't worry. You're fine if you're below. I'm not saying you're in trouble. You have plenty of opportunity to make things right.
but I want you to be clear. I. So if you're like that at 70, something it doesn't mean you're like, gonna fail the class. It just means you gotta write the ship particularly for the final. Okay? And what I will do is, I know the way the the rubric and sort of the class grading is, you're just like, will you just calculate all the numbers, and you add them up, and that's your final grade.
But in there there's a little wiggle room in terms of like. if you like. On the midterm, you kind of come in below average. But on the final, you really pick. But
like that, that matters. Okay? Shows me. Okay, yeah, yeah. You know.
midterm kind of caught you off guard. But you turned it around. So I'm just saying that because I just want people to feel like if you feel like, Oh, man, I feel bad.
Be clear out about it. You can fix it.
Come to office hours. I will help you. I want you to do. Well, okay, that's the main thing.
There is a rubric. Check it out.
Explains how the like. How we got the
scores. It explains what the what the answers are, and all that kind of stuff. Now, I do want to mention this. We have a regrade policy. It works like this. Once I release the graded assignment today.
you do have 7 days to request a regrade.
Okay? The way that works is basically more like, you know we screwed up, we added wrong, or you know we clearly made a mistake.
Ask for a regrade.
I say, I want it in writing. What you're going to do is you're going to give me the midterm and then literally a piece of paper where you're like, yeah. Question one.
I got it. But you didn't have the points. And you give me that piece of paper right?
And I say this because what happens is so oftentimes students want to litigate
have on. Here. You take a look. I don't wanna look. I don't wanna look.
I just want a stack of paper, and then at a later date I'll go and look and read. Okay.
we have 7 days, and I do have to. 7 days. No regrades. It's not like your final grade comes in, and you go try to resuscitate your your midterm to find some points. Okay, again, must be written, and I also say that we will regrade the entire assignment. So if it, you know, we gave you points here. But we took off points there.
You know. we're not just gonna give you the points back we may discover we accidentally gave you points. Does this make sense?
Typically for undergrad classes? I get very few regrade assignment regrade requests. For whatever reason my grad students, half the class wants to regrade.
So don't be like that. Don't be like that. But the point is, you have a policy. You're welcome to do it.
Okay, any questions about midterm, would we? Great policy of this kind of stuff?
We're good. Okay, cool.
Now reminder where we are in this course. Remember, if you go to other university.
you spend
14 weeks here. Search engine.
We did the basics in 4. We then covered basics of recommenders. We didn't get to all of this. We didn't actually get to implicit.
But we did a lot of this stuff in 7 weeks. Now, this is week 8. Okay? So we're kind of right here.
kind of halfway point.
So we've got a lot of exciting things we're going to be doing for the rest of the semester. I'm very excited
now. One of the big things is we have a big project, and I want to spend a few minutes talking about the project. Okay, this is like a big deal.
This is to me the most fun part of the class I tip. I don't talk about the project until after this, because I want you to see basically the basics of everything. You now know all the bases.
Now we're gonna make it fancier and do more stuff. But you've seen everything.
Now you can go think about a project.
Okay? So let's talk about the project.
The project is super fun. Why is it super fun?
Have you ever asked yourself, have you ever been in the class where a project was fun? No, it's impossible, right? It's impossible.
Let me explain why this is fine.
I'm going to give you a grade, in course, credit
for doing something that you think is cool and fun, that he wants to be.
If you can't find a project somehow related to this course.
you can use Jaggptt and plug it into something, project project project
something you want to do, anyway. I give you course credit. It's pretty good. Now.
group project 20% of your grades, big deal teams of 3 or 4,
not 2, not one, not 5, 3, 4,
either prototype, slash demo or research project.
I would say historically, it's like 90% Demos.
maybe less than 10 research. But you're welcome. I list it because if you want to do research, you're welcome. Okay? Topic-wise, your choice.
We have a showcase in the 3 classes after Thanksgiving super fun it is. I said it was fun. It must be fun.
Okay, yeah. Yeah. So okay, Tommy, your choice
broadly related to the themes of this class.
This class talks about just about everything. So how can you not make your project connected? Okay.
proposal. You've already seen all the basics?
I want you to think about. I want good projects. And I don't want people saying, Okay, I'm going to do homework one over again. That's my class project.
Okay? Oh, yeah, we did see if I did Cosign, I'm gonna boring.
That's the foundations launching pad. Okay? Now.
here are the requirements codes who live on Github.
It can be private. You share it with the tamb. You have to use some non-trivial data. or I put this a little, you know, if you're building something where you're interfacing with people.
you can maybe get around that. But the point is, it can't just be like a vaporware project like you need real nontrivial data or some kind of you know. let's say, user feedback
something. Okay.
code libraries. You can use anything you want.
We encourage you to do so anything.
Okay?
So you don't have to write everything from scratch.
You found a cool library that gets you like the third base. Use it
now do something cool on top of it. Okay.
someone else wrote a cool Api for whatever use it.
I say, we will consider the technical depth because you can't just do a hey? We wrote a wrapper around someone else's Api, and that's a pretty good project. Okay? No insufficient.
Okay? And we'll talk about with the specific projects. What meet makes it meet the kind of technical depth?
Level teams must be 3 or 4
Your team will probably be composed of your friends.
I always say this. Consider that. Do you want them to be your friend after the class
you're just.
I'm serious.
So I've had many times. It's like, sometimes it's like a crew.
And I can tell they're like balancing across multiple classes. And so different people are handling different projects. They. I've seen this right? Very smart, very like high trust, not even hypersphere. I mean, like, super trust. Okay? More practically, it's like, these are my Bros. We have fun we hang out. We'll do a project.
Will they really be there for you when you need them? And if they're not
okay into friendship. keep this in mind. Now. it's 20% of your grade. So I want to keep this in mind. You're gonna get an overall grade based on the overall project.
There will be peer feedback. I will look at Github. I will do in my own assessment of your contribution. That is very minor. That's like a little Delta. Okay.
in other words, you'd rather be the worst performer on a great team than the best performer on a crap team.
Does that make sense? So the project. The project is a 95 project. If you're the worst performer, you're gonna get like a 94, 93, if it's a 75 project. And you're the ace.
You're gonna get a 76, 77.
Okay, so just keep that in mind. If the project has to be good, this comes back to
who's on your team. We got to land a good project. We gotta land a good project. and so we'll talk about this a little bit later in terms of strategy.
But the the death now of the near project is, there's 4 people
you'll take this component. You'll take a component. You'll take a component, and then we'll mix the other 3 components together at the end.
Guess what? It's never worked ever.
Okay? Why? Because someone's linked.
It didn't work. Well, you thought it would.
Don't do that. Okay. Now.
here's how it goes. There's a proposal. I changed the dates a little bit in canvas. Give you a little more time. So the proposal's due on the 20 s. Now, that's in 11 days. Okay.
you're gonna have feedback. 2 days later, and then we're hands off. In the olden days I had intermediate project checkpoints where I held your hand. I said, Here, let's be friends. I'm gonna hold your hand and and drag you over the finish line.
But I've decided some of you are graduating. Many of you are seniors. I should not have to hold your hand. I took the training wheels off the bike.
You can ride the bike on your own. So basically, after this, October 20 fourth.
I don't really see you until over a month later. Okay, this month, right here. This is the month of
like.
Don't want to be too negative here.
but you gotta use this month working in the project. Okay, this is right up on Thanksgiving.
Okay? And so
I've seen, maybe a project team is like we have no project going into Thanksgiving.
Don't be that team.
Okay, don't be that team.
And we'll talk about this final final bit here. Okay, proposal feedback. The proposal
again.
the idea is you should have a pretty good proposal, so don't consider this like, I'm just going to goof up a proposal just to get the credit and move on.
because the idea is you want to give me a proposal to pretty good shape and do some course correction. So you're cooking.
because otherwise it happens, your bad project proposal. I have to go back to you. Then we start bleeding into November before you actually figure out what you're doing. Time is gone. Okay, so spend your time. Now, proposal.
October 20, seventh, 20. Second, you're going to post it to canvas and slack
canvas is for us. We'll set it up so you guys can make teams.
So that way it's officially submitted. But then you'll post it to slack as well. It's a Pdf.
okay.
we'll have. I think I've already set a project channel. So you have proposal, Colon, whatever your project name is, post it
the proposal
you want to have a name of memorable, concise.
So bad names for projects.
CSCE. 4, 7, d. Project proposal, cabin
back. Okay.
and edit. You're gonna give me your project name, who's on the team? And then I give you a set of questions to answer. So if you're a prototype demo team.
you're literally going to say, project, team, name, all of us. And then you're going to go in bold.
What, exactly, is the function of your tool? That is what it, what will it do? And then you
have a sentence or 2? Answer. and then, in bold, you'll say, why would we need a tool?
You'll answer it? If you literally just bold these questions.
does this tool already exist?
Why would you one care? What's the difference at the moment? How do you plan to build it. We're going to use these libraries. I'm going to use this stuff.
What existing rural resources?
How are we demonstrating usefulness? And then a rough timeline of when we try to get stuff done? These are the questions you're gonna do. So that's why I say, it's like a one in 2 to 2 page. Pdf. so I'm not into like the fonts and whatever, and like the margins, I don't care. But just you have questions, you answer them.
and so it may take you one page. It may take you 2 pages, but it's not like I care.
Just communicate what you're trying to do
does make sense. Yeah. but boom! But boom! But boom! But
Now for the nobody in here doing a research project, I have a different set of questions.
What is your research question, why is this same deal? Bold? Answer, bold answer, bold answer.
Okay.
how do we go grade the proposal it's worth? I said, 10%.
It's 10%. Yeah, basically, it's it's true, false. It's it's 1 0. You submit it. You're going to get credit.
You submit it. You're gonna get credit.
In other words, if you submit a proposal that meets those requirements, you'll get full credit. So I'm not judging you in terms of
like
the clarity of your pros or whatnot. Okay? So if you do the proposal, you'll get credit. But you do a disservice to yourself if you do a bad proposal that you haven't thought about, because it just slows your team down to actually getting started.
So the point is. the Ta and I are going to read every proposal, and we're going to give you feedback.
We're gonna say you're at. You're trying to do too much narrow in this part.
You try to too little, try to expand it. You underestimate the challenge of this part. You should focus on that part. So we're going to give you feedback to kind, of course. Correct.
You're also going to get feedback from your classmates. Okay, so that's due on Sunday by Tuesday.
Yeah. So this is from us, will he be? Course correcting? Oh, yeah, my advice is always this, simplify.
simplify it. You want your project to be super simple, super sharp focus.
So don't do 10,000 things. Do like one thing.
That's it.
Okay, if you're gonna do some recommendation project, don't say we're gonna do build a recommender that supports
XYZ. And no, just do the one thing. Okay, do it really? Well.
then, you're gonna do for peers. So this is team based feedback. So basically, every team should provide feedback at least 3 other teams. So your team of 4
has to write feedback for 3 other teams. meaning that's easy. right.
And so you're going to go into slack, and you're going to post your constructive comments. So things that are not helpful is stuff like this is awesome. I love it.
You're going to say I like this a lot. But or I noticed here you said this, have you looked at that?
You want to give feedback that you yourself would want to receive. Hey? You know about a good data set cool, you know, about a cool Api that can help us cool. Give me that feedback. I don't need big paragraphs.
I just need quality feedback.
Okay? So if someone is trying to say we're gonna do some
project on some spotify music. Now, having data, if you just drop and a link to the spot up there to spotify data set, if that was your your content was just the link that's perfect.
the super high quality.
Okay, I prefer that over. It appears in your project that you're proposing to, you know, bubble. But you just Bs, I don't want that good constructive feedback. It could literally be one sentence done.
High quality
questions, concerns, proposal or feedback
pardon.
Yeah. 15%
grading, basically just do it. You'll get the credit if it's reasonable, if it's thumbs up and emojis. No. okay, no.
Okay. Then we have our showcase. This is after Thanksgiving. It'll be Monday, Wednesday, Friday. I still want to see exactly how many teams we have, and figure out how we're going to do this? So right now, we have like 90 something students.
If we broke it down, if we had exactly 4 people per so
team, how many does that get me?
That's the great deal.
Thanks. 23. Project groups.
map Road, mass. No. let's say 23. But if it's all threes, it's like 30.
And so that'll depend on how many teams we have, how we do it. Okay?
But basically.
And again, I had to figure out the space if we're doing it here, or if I can try to corral some other space, that'll all dictate what it is. But the point is, this will be your chance to show off your project. Basically, if you're building a
if you're doing a prototype, you're basically gonna do a demo. Okay?
And so we'll figure out exactly how many minutes you have again. Sometimes it works out. We do it like all in parallel. So we have a, we have a different space. We're all set up in different like pods.
Other times it'll be more like serial like each team gets up, they show show off their demo. And then the next team goes. Okay, we'll figure out those details later on.
But the point is, this is your chance to show off your project. It's got to work. It's got to be real. This is like the big deliverable. Okay.
you can't get up here and say, our projects are really cool and we're gonna finish it next week. This is the time. Okay?
questions.
It's super fun. Did you feel it's fun? I keep saying.
Then
I'm I'm kind of changing this, because I realize, after the fact, this is not the way I want to do it. There's gonna be a final deliverable due that December the fourth. This is like.
think it's not actually a Friday. It's a Monday. Yeah.
So basically, we're gonna have a final deliverable. And I'm debating this. Originally, I wanted to have like a project website.
I feel like, I don't want to do that.
Maybe I don't know how you feel. I just wanna get your Github repo link
any data you use? You have a link to the demo that you built.
And I was also wondering, would you guys want to do a project. Video.
This is the whole thing. I'm trying to figure out how to do this, because basically different versions in the past, where, like sometimes for the showcase. I show the videos here, and then we go into like a parallel like a demo session. But the point every team knows what's going on.
Here's the project 1 min. This is the thing. Here's the demo. So that's just one thing. So that may be what happens.
So this made the video begin pushed up. We'll see
the point here. This is like your final like
the final blessing of the code before you give it to us. Okay, so the idea is one of them.
What's gonna happen is like in class. We're gonna be sitting here judging and listening and learning. And then it's over so like these like 50 min. They're gone.
Now you give me a link to your code, your data, your demo. Now I have time offline to go back and look at it and so forth. Okay.
and typically, what I'll also do is
if you feel like I didn't need to really appreciate your project, I'll let you sign up for like a one on one like a demo session. So not before the class, just to me, where you can show off anything you want. But I'll clarify all this a little bit later. Now.
the tension of rubrics to kind of give you a sense of things that we're looking for again motivation. Why, why is this interesting? How cool is your solution? How'd you analyze what you did, what you you know? Conclusions in terms of sort of like this is just sort of the structural stuff, like a merit
is like, is this a hard problem? Is your approach interesting? Is there are results significant, did you use algorithms and methods relevant to this course?
Dot dot dot dot, this kind of stuff? Okay.
what I would suggest is this.
but yeah, and I have a peer feedback form, too.
So you guys will get to assess your peers. Typically, this is structured less as like a tearing down and more is building up. So I have a little spot on the bottom where I'm like, basically praise, I call it praises or something
where you can say, Hey, you know, Bob was really awesome. Let me tell you why Bob was so awesome, so typically peer feedback is less about tearing down and more about again finding people who did a really great job.
This will come down to like at the final analysis. Like, if I'm looking at someone you're on some AV borderline or BC borderline. It's like you were really a star on a team that can help.
Okay.
technical contribution of the project. Okay? So again. You've already seen the basics. You do not have to do a project which just does the basics. Okay?
Launching pad. You are not required to use anything you've already seen. You do not have to use Tfidf.
You do not have to use the baseline estimate. You do not
if you can. You can use Chatguvt. If you know what a graphenroll you can use anything you want. you can just somehow connect it back to this course
and make me think it's real. Okay.
okay.
I'll give you an example of a project that we had one time it was a group recommender. And the idea was this, actually, it worked is like 3 people show up their phones.
You say we want to watch a movie, but it's hard to figure out what we all plug them in to watch.
So they did is each of the 3 people.
They have a little bit of user history about 3 people, and they post to them series of movies that you might want to watch.
They collected that feedback from the 3 people they did, aggregated it and tried to figure out across the 3 people. What sort of like across their preferences? What should we recommend?
Okay, it's a very practical. It's like, Hey, wanna watch the movie. It's Friday night. We don't know what to watch here. We could be Bootstrap using 5 things. It was like, what is like left or right
introducing wise.
Okay?
So that was really cool. I loved it. It was a great project done. Project. Thank you.
Okay. questions
questions project
as you're thinking about project and ideas mainly on slack email in person comes office hours.
Hey? Now, our team was thinking about this. What do you think?
I'll say that project sounds terrible.
So what I just told you is really great, because the idea is very simple. But they executed was awesome.
Okay. I would say, the best projects are the ones that are. They really address a real problem. So
the ones that are boring to me. Sorry I wouldn't say it. Are
any video game related project. I found a database of video game ratings. And it's like, I like this game where you shoot people and says we recommend other
or something before teams. We're gonna recommend teammates to do to something or characters to play with.
Sorry I find it super boring. We recommend this out
person and I go. Is that good? I can understand.
Alright.
And it said that I don't understand anime. But I've also seen lots of good animate. I have seen with anime.
Okay, the main thing is to make.
Also, I've seen a lot over the years which are like, okay, we're students. We care about food. So we want to find like deals or local food, or something like this.
And it's one of these big aspirations. And at the end of the day they haven't really delivered.
So I'm saying, is, you want to really think carefully about what's reasonable and doable in the time frame?
Okay?
And so. But again, this is where to me. If I would be.
I would find some way to get some tech Gpc. Or other Ll. Into the project, because suddenly unlocks the door to the walls
for examples, like projects. You could do right now. you know. you say, like you could go into someone's Netflix.
You do some like plug in on Netflix. And you say, here are the Netflix recommendations for them.
You do something with those recommended titles. You do some kind of prompting to chat Gpt to get explanations for why it's recommended them. And you build some interface like that.
Okay?
Or whatever right? Something like that
tech Gp gets you like gets you pretty far.
That's good at what explanation? Comparing things like, oh, Netflix is recommending like these 2 movies.
What's the difference between them? Chat Gpt? And I'll say, Oh, you'll like this one, if you prefer such and such like this one, you prefer such and such use that then, as the some interface design is
that would be cool. But again, you can do whatever you want.
Okay.
yeah.
The main thing is, do something that you want to do, not because I'm pulling your teeth to do it.
cause it sucks. And why? Why do you want to like work on something you hate for like a month?
Because I've been training by Texas and engineering hate myself right?
Is a chance like there's something you've always wanted to try. Try it, do it.
You're gonna do great. You'll do great
anything else on the project.
It's like great more more work to do. Thanks, thanks. 20% should be fun should be fun.
Okay. So, anyway, 11 days till your proposal is due back.
They're designed not to be killers.
but they will account for however much as like 15, I think of your homework grade is still out there. They're designed knowing you have a project coming.
But you can still do it all. Okay, so
trust trust trust. Okay.
thank you for that.
We're not done. We got more to talk about. We gotta finish Netflix.
Yeah.
yeah.
I started watching this movie on Amazon
faculty
called Totally killer.
It's back to the future meets screen.
There's a time machine.
I don't know. Check it out.
Okay.
where we left it last time was this. we're gonna go really fast here. This is gonna be fun. Cause I know you guys are suffering right now, let's have a fun class.
We have, how many minutes we have, like, 15 min, 20 min. Let's make it a fun. 20 min.
I'm gonna tell some stories. a little bit of math but 90% stories.
Netflix prize 2,006, 2,009.
Remember, this is Netflix is engineered solution.
We said we could do collaborative filter and he beat it. Shocking. We can do our basic factorization. We can pretty much smashed it pretty cool. But we can't win the big money.
Okay? So then one of the big ideas was this, remember the baseline estimate you're like, by God, I do. We just did in the midterm.
You already know this stuff. Well, check out this cool thing. Remember how we learned good parameters. Remember, we had that. We said, we're going to minimize over P. And Q.
And then we said, We're going to do this. Summation
of the R hat. Ui's minus the RU. I's squared for all the ui's
in my known ratings. And then we said, Well, are really we said, That's
EUQI, minus RUI remember that
these are the Latin factors.
Beep, beep, we get a number. This is an error. We minimize the error. How do we minimize the error? We pick the best P and Q to do it.
So we know how to do that. We use gradient descent and do all that stuff. Take gradients and blah blah, blah and all this stuff. Okay?
But what if we did it like this? What if we said. what if we said, instead of just learning like
the rating? Directly we said, Well.
I'm gonna go. Mu.
I'm going to use the baseline estimate. I'm going to say. mu plus, there's an item bias plus a user bias
plus the user latent factor, the item latent factor.
And so instead, now, what I'm gonna do is I'm gonna make
you can make you know you. You can leave it as the overall average want to. They say, I'm gonna learn Vi and Bd.
so I'm going to minimize over P and Q. But also.
I'm gonna learn the biases. So like, I'm gonna I'm gonna learn those. I mean, you can even learn mu if you wanted to.
And so the point is, we know how to do this.
So the point is, you can just change like, what does it mean to get your estimated rating? So now, what we're saying is, basically. this is now our estimated ratings.
whereas before we said, the latent factors have to exactly give us the number. P. Times. Q.
Now we can say, okay, the overall average is a threefive. This item is a little higher. This user is a little higher.
and then I'll do a little bit of interaction between user. And items do a much smaller like Delta here to get my estimated rating.
Okay.
we can do that. We can throw all sorts of stuff in there, and just learn it, too. So but the point here is, we're learning now all those parameters. whereas before, where did B, you come? Yeah, where did BU come from
vu was this.
We just calculated it once
from the numbers, right from the ratings. Now, we say, I'll let the model figure out the best to be you
that minimizes my error. That's the cool idea.
So I'm just showing you this is sort of unlocking. If you go back and read all of the work back during the Netflix Prize. This is what all these teams are doing. They're learning all sorts of parameters, trying all sorts of crazy stuff.
Okay? So they tried that
that still doesn't win the prize. Okay.
so I'll show you one more thing that they considered.
which is this is also kind of helpful for your project, which is, think about the actual data you care about. So what I'm showing you here is the X-axis I. The y is the mean score.
So this is like the average rating on Netflix over time.
the scores that are coming in. So what what do you notice here.
let's solve it.
So just imagine, this is like, I don't know the exact year this is, let's say, 2,000.
Here's 2,001 at some point. You know this is 2,000. I don't know what this is. 2,004 or something
scores, scores.
So just as time goes on.
the swords are kind of flat. and at some point the source started shooting up just in time. Nothing about the age of the movies, but just in time. Suddenly the scores started going up.
Why do you think that happened?
Maybe they change their system. Their recommender got better. People were being exposed to stuff they're more likely to like. and they gave it higher ratings.
Be
so? Some kind of there was some kind of big change here. Was it the recommender?
Was it the interface.
something happened. Okay.
so time is important, like, this thing is moving. Let me show you one more figure.
This is the age of the movie. Okay? And this is again, the average reading. So this is like, fresh.
And this is old.
So if you just look at the age of the movie. What do we notice?
In general, you can see there's definitely this.
okay. bold movies are better
that true. So first of all, let's explain, why do we think that happens? Why would old movies have higher ratings than more recent movies
at Netflix?
Let me ask you this, you mean, this is true in general
if I did a random scene of movies from 1970.
But they are better than a random sampling movie in 1980,
better than a random sequel to.
let me ask you this, which old movies are in Netflix's catalog?
Is it a random sample
movies that people like to watch?
If they played some good movies from the 70 S.
Star wars
cause even when I do movies in the 80 S. Yeah. Indiana Jones.
Those are fighting words, he said, his whole picture from the eighties on stage. Let's go.
Let's do the nineties. Sorry. Let's do the nineties good. Movies for the nineties. Full fiction.
Fourth
matrix is 99
more Star Wars. So
for those of us who were alive in the 90 s. Let me tell you what. though, there were a lot of other crap movies again. So what's happening here? Presumably the movies that are in Netflix's catalogue that are old are the good ones
because the crap ones. No one wants to watch. So why would you have them?
Okay? So there's some selection bias here. It's not like a true representation.
It just turns out to be. Notice what happens here at the very beginning, though.
What do you see happening here?
Up down? Something crazy here is happening.
There's a lot of something going on.
So I would say, just why do you think these movies are super high that are super fresh. but because they're new? So people are trying to watch it. They're new. So people wanna watch it. I'm excited. The fans are watching it. I like sci-fi sci-fi movie. I go watch it. I like horror. I go watch it
like Barbie while I party. I go watch it now. Time goes on. Hey? That movie's really awesome. Wait, wait, wait! Wait. Wait! Let's go see that movie. It's a month old. It's it's a year old.
It's not accurate. Okay. so I kind of interpret this like kind of fans.
You know, people who are like predisposed to want to watch that movie. They go out the first day love it.
And then, as time goes on, it's like a course correction, more and more people. Okay.
the reason I show all of this to you because all of this, like MF business we were doing.
There's no time in there.
There's no T. It's just like users and ratings
and all I'm trying to show you here is like. well, if I was trying to guess a movie's rating and it was old.
I would probably guess a higher rating than if it was younger, just based on time.
regardless of the latent factors, regardless of all this other nonsense.
And so that's exactly what people did. They went back to these formulas, and they started saying, well, we can start putting in a
bi and some kind of timing component and a vu with a time component different ways. You can define this, and and I don't ask you to know. but just letting you know that like, if you actually try to put time into your model.
you can do better. So what happens? We do that
in that.
In that, let's learn the biases better. In that. Let's learn the biases of crime even better. We're getting dang close.
But we're not winning. We're not. So that's the end of the technical discussion of the Netflix Prize. If you understand latent factor models, you got it. The rest of it is just fancy versions of it.
Storytelling.
Remember, the goal is to get to beat it by 10%. Okay, we know thousands of teams participating
the prize. Remember 1 million dollars
in your pocket.
So go back and talk about how it actually worked
plan for 5 years
every year they give a $50,000 progress prize so long as it was at least a 1% improvement on the previous winner.
Get some money.
If you win it. You have to. Sorry if you win the whole thing. You have to share your code and description with Netflix Netflix within share the description, but not the code. Okay. here's the kicker. Once the team does pass the 10% threshold.
you don't win the money immediately. Okay. they issue a last call, and they say you now have 30 days. and then the competition is over. Make sense. So just because you pass the
what is it? 8, 5, 6, 3, you pass it. You don't get the money, you pass it 30 days later. Whoever is the best wins.
Okay. so
2,006, let's go back. It starts October second by one week later, a team is already beating cinematch. Okay?
A week later 3 teams are beating cinematch by at least 1%. Okay? And by December we get our voice funk. That's the stuff we talked about. Yeah. MF,
so within what is this? 2 months we have funx. Svd, every team starts using it. Okay.
2,007, 2,008.
Hey, Bellcourt, this is at. And T labs
this core in his name.
So here, Korin, he's at Bell Labs Koran.
You have big chaos in Austria under
and the team start teaming up. So then you get Bellcore in big chaos they join up.
Because what's happening is it's like we're trying the best we can. But we just can't get the number down. So let's keep this partner up. Maybe we can share ideas. Okay.
so 2,007 progress prize. This is Bellcourt. There's Yehuda core, and there's the core. I guess I'm sorry, and there's Robert Bell.
they get by 2,007. They're 8.4 3 7% improvement. That's pretty awesome
by they team up. They're 9.4. So they're knocking on the door. Not after 5 years. But after what? Like a year and a half, they're almost there. Okay.
this is
this big article by Corin Bellinsky. All about what they did. Okay. Now.
June 2,009.
Bellcourt has teamed up with pragmatic team with big chaos.
They get 10.0 5% improvement. Okay.
that triggers. The last call we now have 30 days.
Clock is ticking. Okay. so what do you do if you're not Bellcourt? Pragmatic chaos
do something crazy. Team up.
do something crazy. We're gonna lose the money. So let me show you what it looks like.
You can see, these are the teams.
Here's pragmat theory, Bellcourt, in big chaos. Here's the chaos. Here's Bellcourt. What we can do
here developers pragmatic chaos.
This is the leaderboard. You can see it shows you the best score, the improvement until you submission time.
Okay, so this is on June 20 sixth.
You can see. Here's a June 20 third, the chaos. So the individual teams can keep submitting, but the collaboration can submit it.
They're over it. But notice, you also have gravity opera.
Bruce Lee. Excel, vector you have a bunch of these teams. Okay?
And so just think about it. Like. if you're grant Riceine.
Okay? So then, what happens?
Ensemble facts?
Okay, these are other teams on the leaderboard. They're shut out by bellcourse, big chaos, pragmatic chaos. They start combining models. They get 10%. Bellcourt keep squeezing it.
they quickly realized. Cause you can watch the leaderboard. The ensembles are popping up. Velcro is popping up
so they're like realizing this, is it? Okay? 30 days, 30 days.
Okay. both teams watching the leaderboard. Okay?
Remember, you had to make a submission
that then update the leaderboard with your result. So you only know it. So like, there's these times, you don't know what's happening.
Okay. last 24 h. Okay. you can only make one submission the last 3, 4 h. Bellcourse sees that ensemble is taking the leap.
Okay.
last 24 h. Imagine people losing their minds, super optimizing your parameters.
Bellcourse. Submit a little early on purpose. About 40 min before the deadline
ensemble submits their final entry. 20 min later
we wait.
Okay, submission. We wait.
That will.
Let's go to Leader board.
Oh, yeah.
The ultra pragmatic chaos, the ensemble.
You're actually
what's the tiebreaker
versus the next period. I submitted. 861-86-3820 min early.
Yeah.
5 day. And so the lesson is turning in homework.
Now. My understanding is
they donated all the money.
Let me tell you what they were already doing. Fine. And since this, they're doing great.
Okay.
these guys are all making big bank. They're all big tech companies. They're big, rich dudes. Now they're doing fine. Don't worry now.
Okay, now.
how did it? Really? How did it really work?
I'll just show you this is their solution. If you go back and read that paper I posted. this is the solution they so they show you
they did. They just do. MF and biases. No.
they developed.
But notice moving in there.
Baseline, you know that
it does make the track rising. But he did a whole bunch of versions user can in a whole bunch of versions. And he did like a big ensemble.
So you end up doing like 500 different predictors out of those recommender models.
So 500 different predictions.
Okay? Then they do some kind of blending.
They use some latent features. They end up doing something. They squeeze out this insane model that squeezes out the percentage. Okay.
now I ask you to this. I ask you this. So if you're Netflix and Bellcourt's pragmatic chaos is giving you their code.
Are you gonna put this into production?
Why would you not put this into production?
One? It may be hard to implement certainly implement at scale
2 hard to maintain.
So if you've ever been at a company, and they're like, Yeah, you're responsible. And they're like, yeah, the recommendations. Are screwy. Could you take a look at it?
There's 500 predictors you need to debug
what? What?
Absolutely, not. Another thing you can. So you start seeing. And so the Ads headlines. Netflix never used its million-dollar algorithm
due to engineering cost. And so you see, these kinds of articles came out. I have one more.
This guy says
what the failed Netflix Prize says about business advice. This, by the way.
this guy is a
clown.
sorry like that's like I'm running. It's my cousin. now, total clown show, so
he's saying is like, Look. they spend a million bucks, which again, how much is a million bucks to Netflix
for multiple years of huge, huge, huge, like worldwide engagement. All these scientists and engineers
launched all of these revolutions in terms of latent factor models that, let me tell you, are still used 100 today in Netflix.
Their particular approach definitely, not at Netflix.
Okay, yeah. This guy here
0
cause. He says it changed binge watching because it's right, it's true. So
too hard to implement too hard to maintain. True. the innovations definitely affected us. Yeah. MF, etc., etc.
What else happened to Netflix around the same time?
The business model change? Oh, yeah, they were mailing Dvds and became a streaming bug.
So let me tell you what the mechanisms, the specific mechanisms to do.
Rating prediction for male Dvds is very different from
what you do for a screening
the big takeaway to be is no, they never used it. Why would they use it? But the innovations change the world.
See you all on Friday? You're lucky.

Okay? Howdy, yeah. Welcome back. Yeah. We have a mid term on Friday.
just as a heads up on the homework, right? You had to submit a URL for the SEO contest. All we're grading now that you submit something, but not that you actually show up. It may take a while I'm happy to see, at least on Google. If I use the phrase query, we're getting at least 3 results.
Someone has bought has algorithms.com. It's funny cause I checked whenever we were when you were saying yes, and when I was doing
Peterson building.
and I don't know in the phone number.
And if that's maybe your phone number, it's not my right. This is not my number. You may want to call that number and request some albums.
You guys can decide if this is fair or not.
and you may wanna, I don't know. Can you report it? And you say, I think this is not a real place.
Oops.
Okay?
Alright. So we have a midterm. We have a midterm
in 2 days we'll post the solution for the homework to this act like a
in the hours when they get back to my office we'll also be posting the homework. One grades that should have come out already. I found that at my ta. But
we're basically done with the ratings. Get those out today.
midterm again. Logistically, what's gonna happen is there's gonna be all these people here that you've never seen before. Right? And we're gonna fill this room up. And so you're gonna be, you're gonna be all like blocked in.
and it's not great for me as an instructor. If I want to like, evaluate, you know how well you know the material, and you're kind of like sweating and bumping into people and eyeballs, or scanning and all that kind of stuff. So I think I'm gonna end up making like this soft script. Sorry.
Get into my head for a second. I'm gonna make like multiple versions of the midterm right to randomize stuff and change the numbers itself. But the idea is he comfortable desk
and doing your work? How it's gonna be like,
I started to make it. It's gonna have again a mix of what I call like quick, like a true fault, multiple choice at the easy way for me to ask lots of questions.
I will definitely have calculations, style question. So things that we know how to calculate like, you know, cosines, Euclidean distances, tfis cosines that Rms
precision. 3 calls. Those are all calculations. Style. Now I can't. You don't have to calculate it right.
and so I gotta do it such that it's like I simplify it.
But the point is, you have to sort of communicate. You know what's going on right? Here's 2 vectors. What's the cosine you're like, yeah, you do this, this, this divide by that. You figure it out first.
but you show me that you know you're doing right because I don't have the past. I'll say like, What's the Cfid effect here this document
and use for do one or 2. And you say you can see how I'm doing. You figure it out. Okay, that's fine.
But all the some students will like. No clue is very clear to the now
other than that.
There might be some short answer. Those are kind of hard to grade.
And so when I first answer I mean, I'll say this. It doesn't really matter. But like short answer, literally like phrase. But people like to answer like. you know what is what is? For example, I think, what is precision?
You know you could just write the formula done with someone. What was that was saying? Recall, app measure.
But in this context, when I'm trying to communicate, is that precision is an issue, and you go on on and on, and on and on and on, and then eventually you give me the definition for precision.
You also are nervous. You give me the definition for recall, and then you just throw it out there. You just like, give me some points, so we'll see how that goes.
and then I may have what I call kind of like my synthesis question, or like a harder question, which is.
I take the concepts that you already know, and I kind of remix them in a way, so you can't really prepare other than be ready for the class. Come to class. You've sent me the notes.
and sometimes like that last question that, like last 5 or 10 point question is something like that. Okay, given that.
Any questions about what kind of like the structure stuff like that.
But you wear that t-shirt right? So for you.
yeah, front and back. Anything
in literally anything.
One sheet.
one sheet, all you just one sheet and something to write with. Everything else goes away.
Sorry?
Yeah.
So
there is. Somehow, like, micro divided in half. And I have 2.
No, you can have basically like one like 8 and a half by 11, like plain front and back. You can't like subdivide it, or micro half by 11. This fast. We respect that we don't do a 1, 8, 4, none of that nonsense. I will measure no legal number in English
looking to do whatever you are
anything.
Okay? So another question about the structure. Then I do have all of my. I posted those old midterms and finals. I'm happy to go through those. Or if we had specific question, happy to do those? Yeah. Question.
Everything is fair game of everything. We've covered period.
I will focus on the fundamentals. So again, if I was studying, I would not read discriminative models for information retrieval by now. Apache, 2,004 in detail. If I didn't understand Cosine. I didn't understand. Tf-idf! I didn't understand. All was basics once I did that then I might go back and say, Now, what were we talking about? Why did we talk about that paper? What was the idea?
Yeah, spoken
cool. Okay.
So you guys want me just to go into something.
I have those old midterms or finals. I don't have anything prepared. This is not like, I'm not gonna just tell you stuff. So just give you the answers. So here's a midterm from many years ago.
Okay. following.
And so a lot of stuff. You haven't done. So. This is like a short answer, right?
This is a style of a short answer.
Hey, we talked about Boolean vector, space. What are 2 advantages of Boolean compared to vector space? If I ever ask you this
sometimes I'll ask you for, and let's take an advantage of one and a disadvantage of the other.
Right? You can't say Joe is taller than Bob Bob is shorter than Joe. That's the same point. Okay, I'm not gonna double count you.
In this case, I'm only asking for 2 advantages. And so it's just again, like, okay, where are some advantages.
some stuff. Yeah, all the stuff. All this stuff is on the modules under course resources.
But you can see that the style. Give an example of the difference between an information need and the query. So you may say, I don't know what that means. You know what that is that you forgot information need. So you better go study.
Does anyone? Can anyone give me an example of?
Are we all going to fail together?
It's like
when Frodo, is there a mount doomed like this?
There's the pit
buyer. We're all just gonna jump in together. I don't want that dude.
I regret nothing.
So what does it give me? A query? You might ask. A query is literally the thing you type in.
Meet me at midnight. What does the information need.
It could be that song. But you can give other kind of more clarifying examples would be like. I'm looking for good albums to give to my, to my wife.
my girlfriend, my whoever my significant others. So I might query, for you know, like
albums out! Get out of ideas.
So the query itself is like what I type in. But the information need is like the hidden state, the thing that you really care about, that you oftentimes translate into a query which is some rough approximation for the information need.
Why do we care about this? Because, remember, when we evaluate, when we do precision, recall, and all that stuff. It's with respect to the information need not to the query.
In other words. if you queried for, meet me by midnight. and the information need was, I want the Taylor swift album, that features that song.
and the result is just a document that happens to mention. Meet me, but meet me at midnight. It might.
you know, be relevant to the query like it has those words, but it doesn't satisfy the information needs. It would not be relevant. That's the whole point is, we're always thinking about it. With respect to what does the person really want?
Not just compare it to the keywords in a queries. Okay.
but anyway, you can see, this is like examples. So
what kind of question is this one? This is a good one, this this
sometimes. Yeah, this is one where I don't tell you what kind of question it is. You have to know what's up.
So I say, we have a bunch of Texas and web pages. The most frequent word for 300,000 times
was the estimated frequency for the third most frequent word. And why so? What kind of question is this? What am I asking about a little?
It's one of those rules or laws
we have 2. We have 2 laws.
Yes, remember we had heaps. Nope, not it.
I mean, it's zipz law. So this is Zip's law question.
right where we said that the collection frequency in position I is proportional to one over I.
And so in this case, we're saying like, the first thing is like 300 k.
so we're saying, this is like one over 1, one over 2, one over 3, so we can guess
100 K ish
very rough. I use Zip Zip's law as proportional to that one over the rank position. I'd guess 100 k, something like that.
Let's
yeah. So I'm gonna say, in park. So you say, oh, according to zipzer. I would do this. Yes.
Good. Good question. Okay.
Visual index. Wild card positional index is good for what kind of queries?
Nope, you could. I guess we did say that didn't.
Concretely. We talked about 2 kinds, which were a phrase
and proximity.
Okay.
we didn't really talk about this. I don't think so. I don't need to worry about it. But this is the more of like
not a synthesis question. But these are the kinds of questions I may ask you, like one of this, where, like, we didn't specifically talk about this.
But you would kind of say, well, I understand what an index is. And we did talk about tiered indexes a little bit. Remember that
it's like you have, like your high quality index first, and then eventually, you have, like your third or fourth or fifth index is like a bunch of crap.
And so when you're executing a query, you know, the idea would be like, you know, this is like a one awesome.
This is like, Okay.
and this is like crap.
These are, these are document indexes.
So the idea is like all that awesome documents like Wikipedia live up here
and all your ads algorithms SEO web pages live down here right? And so when the query comes in
like here comes the query. The idea is, instead of running it over all of them. It's like, No, no, I run the query over just the top one.
and if I can answer the query just by looking at my high quality docs.
I'm good right. I found all the Wikipedia pages. I found all the high quality stuff for you. If there's nothing up here, I fall down to my middle. If there's nothing in here like tabs algorithms, there's nothing here. There's nothing here. Then it falls down to here.
Right? So the idea of it's your index. And so this question basically says, typically like, these are smaller and they get larger as you move down.
So the question is like, Why would that be a good idea to have like kind of smaller index up top and bigger index at the bottom
over this
ideally search through a bunch of things. So just get the best.
Well, yeah, smaller index could be faster.
right? If we can satisfy most queries with the small index. It's really good. That seems better
right? Then, having like a really big index up here, and, like the craft, is very small. The hope is, we can answer everything with a very small, like compact index. So you could tell me some efficiency story or whatever. And I'm not gonna ask you a specific question. But that's something where it's like, we haven't exactly talked about that.
But I might ask a question like that where you need to think about it.
okay, this one is a calculation style. Okay? And this actually combines a couple of things. We never talk about anchor text. So you don't need to worry about that. So we never talked about anchor text. But the point here is, we're saying, like, Do a cosign of
whoops, I guess I update these.
oh, yeah, yeah, this is a little too complicated. So yeah, so we didn't talk about anchor text, so don't worry about this one, but the point is like you do a cosign. But notice what I do here is, I simplify, I would say, like
no ideas, no logs.
I give you software and lower case and more punctuation. So the idea is, I'm trying to make your life
a lot lot easier. I'll just. I'll help you see what's going on here. So this was like. Daca, what are we doing here, Tamu?
Basically, it's just like that.
All you care about is that stuff.
and then the doc for T to Texas.
So this is like really.
really
awesome. And this one is what
the longhorns. I don't really want to write this. really, we really, really. So just imagine, those are my 2 docs. Now figure out. Oh, I said, you have some stop words
that goes away.
You do some stuff. You calculate something. Okay?
Page rank. We never talked about it.
Pubs and authorities. We never talked about it.
Evaluation? Yeah. So this is I precision and recall life stuff. Right? It should be pretty pretty basic. The query is, Aggie football. We have our 2 different search engines, Alpha and bravo!
And you said, boy, I give you lots of numbers, and he's got to remember what's going on.
So Alpha returns 50, 20 irrelevant. So the precision for Alpha is 20 out of 50.
Bravo returns 5, 4 irrelevant, 4 out of 5.
There are a hundred documents in total that are actually relevant.
So the recall is 20 out of a hundred.
4 out of a hundred
cool.
so we can calculate precision and recall. Then we say which one is better.
Wow!
There aren't going to be very factors like steamers.
So this is one of the things where you could answer that way. So his answer was basically like, Well, we don't really can't tell. This is not enough information. There could be other factors like speed that are not here. So I can't tell.
I would accept that answer totally fine.
Okay.
If if you were, say, assuming all that is equal
and just based on these numbers. Could you tell me which one is better
probably started?
So the you can have many answers. My hope is, your answer would be again. I can't tell. This is one query.
I would never make a judgment on one query, that's madness.
right? It's also like, What's the What is the use case? Do I care about precision? Do I care about? Recall? I don't know. So this is one of those things where it's more like.
if you
you guys superficially know what's going on, you're like, well, precision higher, you know. Bravo's better.
And in that case it's not gonna be a great answer. I'm a softy. I'd probably give you a couple of points because you made some argument.
But really like, if this is like worth 5 points, you might get 1 point for that cause. The point of this question is.
you can't really answer the question
right?
Well, isn't fair. It is fair.
It is
okay.
when we evaluate a search engine. So the the point here is like, don't know who knows? For a lot of reasons.
Also, if you just say, dunno, you're not getting any credit either, so don't do that. When we evaluate search engine we have a choice of metrics precision recalling. Bcg, however, we have to have a relevance value.
Blah, blah blah. So where do these relevance judgments come from?
And so I said, 2 possibilities are like experts. Sit there and read the doc, read the query and then say, relevant or not.
or we could do like. what did our users click on? They queried. For this. They clicked on the doc. Okay.
so what's an advantage and disadvantage of each of those approaches? So what's good about using expert judgements, for example, is accurate.
Presumably it's it's good quality, because they know what they're doing
and close it.
It may not be representative, because these are like a few people doing it. It may not be representative of your your brain and what you were looking for.
Expert judgments could be expensive.
Okay, so let's do click feedback. What's good about click feedback.
That's right.
We one, we had to have users. Yeah.
doesn't serve way.
There's gonna be a tendency for it. There could be some additional bias like how you got the clicky bag. And so you could have like spurious clicks, you could have clicks that don't indicate relevance.
So it's noisy. There's lots of bias to it. What's a good thing about click feedback from users?
It's a lot. It's cheap. plentiful, but but but cool
switch.
So these are, I like these questions, but they're kind of nightmares to grade.
So basically.
you're building a next generation search engine for finding good streams on twitch. So in this case the stream is a document.
and the idea here is like you would design your own twitch search engine.
Okay.
those are fun for me. I mean the sense of like I think they're maybe not fun for you. I think they're kind of fun.
But the problem is is like I get a hundred different answers. like at a hundred different levels of of granularity. So I probably won't ask this kind of question.
But it's something. This is more along synthesis, like one. That one last question where I may ask a more pointed version of this is where I want you to think about like. Okay in twitch. So like, for example, if you don't know.
Sorry. But I gave this question before. I remember some students like, I don't know if that what you're talking about. So I hope everyone knows what's up now, but like some streaming platform.
And so, for example, like compared to like, you know, like our. I don't know our indexing albums
like, what's something unique about twitch? If someone comes on a twitch and they're searching for something. they're looking something they might be looking something live right now.
Okay? So like an albums. Presumably it's like we have this whole catalog of albums. and yes, new ones. New ones. Come on.
but it's just the same set of you care about the whole collection.
This search might be optimized, for who is live now?
Right? So you could say, well, in this case there's use case of people who are like trying to find like, just any good twitch stream like ever.
But I'm gonna focus on. People are looking for twitch screens that are live today. Which means, now, Oh, gosh! All of my indexing structures, all of my everything I do. My ranking has to be optimized for just was live.
That changes. Everything has to be a lot of yeah. It's like, who's online right now.
Okay, that changes everything. And so that would be like the motivation for like, okay? And then you would try to design something.
Okay?
Oh, that was a midterm. Yeah, I'm David.
let's go to the same year. Let's look at the final.
It's fun to go back and look at this stuff.
the basics.
Oh, yeah, I think I asked this one cause I got mad, because, like on the midterm, people couldn't do this stuff.
So it's called the basics. So this would be something like, I give you document vectors. And I just say, what's the Jacquard?
Right? So what's the tech harvest in these guys?
Set Union right?
Jacquard is like a intersect B size over a Union B size, so they have
one. 2 things in common.
The union of them is 1, 2, 3, 4, 5, 6
done.
Done. Euclidean. Distance
distance between Dock.
One talk to you! Oh, crap! I didn't write down my formula, Professor. What do I do
live on your cheat sheet. I forgot to write on my cheek sheet. Oh, no! I tried to print out my cheat sheet right before class, and the printer was broken. Can I use my phone and look it up? No, no. So if you come in the class on Friday and you're like, I forgot the formula, I don't have my cheat sheet. What do I do or have my teaching? I didn't write down the formula. What do I say to you
say anger?
You're going. Hello! Goodbye. I say, RIP. Brother.
so we just go. You know how to do this. I don't want to do this, the square root of 8 minus 2 squared plus dot dot dot. You do all that stuff.
Okay? Then we do cosign same deal.
You're like, okay, cosine, 1, 2
8 times 2 plus one times 2 over square root of something through to something. Check.
You only do this. you know how to do this. Yeah. you're good.
We don't now. So this is like a little tricky one. I say, now.
100, yeah.
So that's why lots of
aware of its proposal.
Look. I hope last time that there's a knob actually right.
it's gonna go. I don't know it could be. I've given these exams for like I flip it over. I split it over and give it out. People are walking out of 10 min.
Everyone's gone. Also I give the exam, and it's like 50 min. Everyone's so
don't know. So I don't know. My goal is. Look, I don't judge the old ones and worry about this. I'm gonna try to make it
doable. But I appreciate the concern
also, typically, though, like we have a 50 min class, we're gonna start on time.
So be here ahead of time and set up. Okay, we're gonna start on time. Yeah. One more preview thing is, I'm like, real like, I get really caught up in the like. We all start at the same time.
So it's gonna have a cover sheath. and I hand it out
and inevitably someone next to you pushing down that paper like trying to read through it. I can see the second thing
with ideas. I hand them all out so everyone's got it. Can we all begin same time
at the end of the class? Same deal. I'll typically depend on how it goes. Well, I'll give you extra time if you need. It's not. It's designed to be done in the time.
But you're still suffering. I'll give you 3 more minutes. But let's say I'm now calling him, you guys, we have 1 min left. Give 10 s left, and there's someone wearing the back. you know.
and I go time.
And the guy in the back is like, well, it's time for people upfront. But I've got an extra minute category down there, and I'm gonna keep on writing.
That's the kind of stuff that like I wanna pull my hair out and I'll start screaming, I'll go user, stop that! Yes. And then, while I'm not looking, you're still writing, and
this is this has happened. I don't want it to have it. I will take the exam.
I will rip it in half. I won't destroy it, because I want to grade it still, but I'll rip it in half to indicate. But this is a shame. Be upon this person, Shane.
Okay, no, I can't.
Here's another relevant, non-relevant thing
you know how to do this stuff precision recall is like
you can do this.
This is the same question
based on these results. Would you feel confident in arguing to your boss? It's like one of the versions?
No, I don't know. There's only one query. There's lots of other factors. Then I say, what would you need to do? What like? What more would need to happen?
I would need to consider all these other factors the same. I would want to run many more queries. I would want to understand my users all these other issues you'd want to do integrate.
So you look at that. No, it's not connected.
hey? We have 5 users and we have 3 movies.
Remember the baseline estimate. What would I guess, Alice, for incredibles.
So you have to remember
this. Stop right? And so hopefully, the numbers work out. 121-61-9124, like 728. Is that right?
So mu is 33, over 10,
3.3, the bias of Alice.
So she's a 2.5. So she's a minus point 8. And the item, what do we say? Incredibles? What is that? 3?
That's a point 3 below.
Is that right?
Something like this? I did really fast something like that
when you go to do, Darlene, do not use your estimate for Alice, remember. that's a guess. So you're doing these in parallel. Okay?
Why, I do all of these. I don't know why it's too much. This is why I won't do all these questions too much.
Oh, crap! This really got serious. Then I said. What if we use this thing?
And I gave you a table? Oh, no.
So this was they had more time. But yeah, but
yeah, they had more time. Rmse.
this is rms, need to tell the formula right?
So you just like the
what is the formula square root of one over N summation. R. Hat minus the r.
so you just have to do that.
So it's like
4 squared plus 0 squared, plus 2 squared plus 3 squared plus one squared. Where'd that come from?
That's the 6 minus 2 squared.
8 minus 8 squared?
Okay, so what is that? 1620,
2930. Is that 30? And we guessed for 1, 2, 3, 4, 5. So it's a square, square square root of one over 6, the 30
so nice. I think so. Someone check that out.
Of course, Rms is not sufficient. What are up to other factors? You might also consider when you assess a recommendation engine.
I might measure a specific thing like in the Cg. Because that considers the rank order. I might also consider the how fast I might also consider
the user interface design. I might consider
blah blah, blah, lots of stuff. Thank you.
Okay, here's a ratio classification
ratio ratio. We know how to do, Roshio, you've done your homework.
and so we have 2 tweets from me and 2 tweets from, and then we have a anonymous tweet that we're trying to figure out who belongs to. So it's a binary classification.
And we say, simple term weights ignore all words of links 1, 2, or 3.
So basically, you just have to say. C, one and C 2. And this is like a real pain. You have to go today.
Tough class exam, so this one is a 1 0 1 0
and the other doc is a tough
exam today class. So the calf average is 1, one over 2, 1 one over 2.
Yen
is oh, crap! I have more worries. Yeah. So she's got easy down here.
So this is 0. So yen is easy exam today.
Easy.
This one is class tough. Oh, crap! I'm
easy easy as twice exam today.
And the other tweet is class tough today.
So Yen's average is 1, one half, one half, one half.
And so now we basically have the cave, vector the in vector
and the test, you know, whatever the new, the new one is
exam. Tough.
6
is what tough exam is that? Right? So then you just have to say so. The distance between new and cav
the distance between New and yen. and you do some big Euclidean distance, and you figure it out
cool. No calculator, because this is all like simple stuff paths and ones
as you can to the point at the end of the day you get like, it's a square root of something and square of something. I don't know if that is, and my brain hurts. So you would say which everyone is closer is the one you pick
that make sense. Do we have it right on policy. you, if I ask you which who wrote the Tweet you got to give me, you got to say
here's something.
If if you don't solve it, which is okay, it's okay. You can just say, Yeah, whichever one of these businesses are smaller assigned to that class done.
What I'm trying to assess is not if you can multiply and divide.
But if you understand, like, how does ratio classification work?
Having said that, it's structured so that this should be very straightforward to do
so. You can't just say, well, I would find the centroid of those 2 vectors.
Show me. Show me the centroids. Okay, show me you know how to do that. Are you out scratch paper? I mean, like, you can write on the back.
Yeah, exactly. Yeah.
But again, what I would say is, if you're doing. I've learned over to. I've learned I've learned
which is
II can do these problems where I don't want you sitting there.
I'm doing 25, like on the homework. We had you like 25 million Euclidean distances, right? But that was a homework.
You could write a script you could do in a spreadsheet. You can do it by hand. You have plenty of time. So on this. It's not gonna be designed for you to be like hurting your hand writing out a bunch of stuff.
Oh, man, it's more.
hey? We didn't do that one
short answer.
okay, but oh, yeah. I heard the short answer, Oh, yeah. I tried to use like lines. And then people say, like. you know. feedback of of of paper.
And they're really messed up.
Okay, recommended system. Simply gather ratings, using explicit signals or using implicit signals like
clicks, views, etc.
Any interaction when building a web search engine often need to optimize the Pcg. Why do we prefer in Dcg, does it explicitly, you know, optimizes for rank and the discounting factor means like it rewards the higher stuff. More, it also has great irrelevance. There's lots of things we could say.
Check, don't write the formula. That's another trick people do. Well. Precision's formula is this, because it was on my cheat sheet? Indie CGI's formula is this, it was on my cheat sheet.
And see, this one has a summation. This one has a whatever. No, tell me the reasons. There's some queries where we might prefer recall in Dcg. Can you describe such a situation.
where we care about recall more than Nbc.
Patents legal stuff. I need to know everything. I don't want to read just the first one I want to know everything. I also gave you the example of like I'm going on a date with someone
or my child is gonna marry someone. I will read every web page about that person.
I want to know what is up.
I think for for you, for example.
if you were gonna go on a date. You're just like, alright cool. What's up? Let's go
to get
it just tends to.
I don't even yeah. I will say this, I think dating apps are good in the sense that you can connect to more people.
but also, like, I empathize, I empathize with you. Yeah.
When I was coming up. It was like, you wanna date someone you kind of had to. You bump in the same circles or like a friend knows somebody. So my whole dating pool was like what dozens of people. Max. Right? 3.
Your dating pool is like everyone within like 3 h up here in this age range, which is like a million people.
And so if there's 12, let's say there's 3 people in there who could be good matches for you. You have to suffer through like 900,000 rejections.
So my heart goes out to you.
Dear, science students, I go to
get your shot. Brother.
Okay, word to back. We have not talked about.
We haven't talked about this, really.
We didn't really talk about ethics yet.
Okay? And then this is again design question, which is,
you're working at Youtube. And you say we notice that they're saying conspiracy theory videos.
And they could be recommended alongside regular visits. And so your job is to make it harder for users to be recommended. Conspiracy videos.
However, you're not even sure what conspiracy videos there are, what topics they cover? How widespread.
So what are you going to do?
This is a big question. And again, these are hard to grade. I probably won't ask the specific question. There's identification
where I would want you to think about the concepts we talk about.
And so you would think in this case. Well, first of all. I don't even know what conspiracy videos there are. So like, what were the first
step, the line like, can I identify conspiracy videos? What tools do we know that label things we know about binary classification.
right virtual.
and one of the 2 classes conspiracy, not conspiracy. So the first thing you might say is, I would develop a conspiracy labeling method where I would first collect a training set of videos. I would have humans label them as conspiracy or not.
I would then identify features of the 2 videos. I then would try to build some kind of classifier, a binary classifier that could distinguish them.
Then I would apply that to all of my other Youtube videos and see if I do a good job on at least finding those.
Okay, that's step one
step 2. Given those labels, how can you incorporate that into your recommender? Well.
I could just ignore all those videos. I'd never recommend them
right.
or I could give them like a lower weight.
Or I have a lot of tricks. Okay, that's that was that.
let's go check out
the midterm from 670. This is more recent, so maybe gives you more clear insight into my feature these days. You can see how it changed. Too
true, false!
Why true, false, easy, brother, easy.
true, false also. Don't, don't be. Don't be this person that says that's a 0. you cross your teeth.
Lot of these kind of questions are easy, short answer.
Hey, we? We read this paper.
So they're talking about. Do we need to read the papers?
Well, we talked about this. It was on your readings, and we talked about it. So I'm not going to dig out a question typically on a paper that you know. Oh, I didn't read the paper the last paragraph. We talked about this in class, and we said that there were informational needs, but we also said there were 2 other kinds of needs.
transactional.
navigational, that's in the slides and talked about it. You gotta know it.
Case folding
what's good about it. What's bad about it.
A good thing is, it can decrease your vocabulary. That's a good thing.
Smaller vocabulary size, maybe a more compact index sounds good. What's a negative thing that should be like.
There could be some words that the capitalization is really important for, and it may be hard to disambiguate
acronym acronyms that also correspond to. You know, words. So, yeah, yeah, stuff like this.
here's a question where you have to identify what the question is.
I give you a bunch of emails, a bunch of bookab blah, blah blah, and we double
what happens? This is a heat's law question.
Thank you.
So if you didn't even realize this heap's law. You can't answer it.
If you knew it was each law you just plug in some numbers, and you say, figure it out.
You don't have to actually solve it. Okay?
Oh, this is a fun one due to recent budget cuts. We must conserve energy.
So we're going to throw away the original item ratings matrix for matrix factorization. You can only keep the much smaller. MF, but once you get it.
what's problem with that.
you see what I'm saying, like, normally, we have this thing. and then we have this thing. And so he's saying, Get rid of that.
they can't do.
Yeah. So what did I lose? I lost all the original ratings. Yeah.
so what does that mean?
All I have is estimated wrong.
I don't know. You know, I've lost my ability. Yeah, to really do this evaluation and do this update. That's a big problem. What's another problem with this
evaluation super hard stupid? I won't. I won't accept that. But you know, for any one user.
I no longer know what you've you've actually seen
right? Cause the original row. And the matrix told me all the things you saw.
Now, I just know, like you like horror movies. So you can imagine like, what do I recommend to you all these horror movies, and you're like you might recommend. I've seen all this stuff. Why are you showing me stuff? I've already seen
broad. You better. Do the research
don't don't worry about this one.
we haven't talked about that.
Here's a vector. Space one combined with a positional index.
The whole point of this is to make you remember what a position index is.
And so it's a Tf-vf. so basically, you have to go back and figure out like, what are the documents? So you have to go back and say, like.
like, what is Doc? One Doc? One is like
Aggie's twice right cause position. Those are positions.
It has R. Once it has the once. So I think that's Doc. One right?
And we said, Doc 2. So I said, Doc. 2 was Aggie's.
Peggy's are the
we? There's Doc 2,
and then you have to do the idf, so you have to go back and figure out what is the idf for all these guys.
Oh, man, this is a they didn't like me for this question.
So for the idf, you have to say, what is the Idf
for aggies.
So aggies is in how many documents
it occurs 4 times. But it's in 2 docs.
How many dots are there?
Okay? So hopefully, the math works out. So it's just gonna be, what is that? That's one. And you can do that for all the other words. Okay.
I'm going fast. We can do this on slack as well. If you want to post the sample solution.
here's more baselines. You know how to do that.
Then I ask you like a tricky question, which is you? Did the baseline. Now, I say, use collaborative filtering user user.
And I say, like, be specific. Show me a formula.
Here's I would say whatever I asked. I'm not gonna ask you this, but if I do, don't go write the formula that's in the on your notes. Right? Just say, remember, user, user, collaborative filtering is basically, I find the nearest neighbor. And I do something.
So just cook up the most basic version. Let's say I would find my one nearest neighbor and I would just take their their rating.
That's it
versus someone's gonna write down this complicated formula. Their hand's gonna hurt. Don't do that.
Oh, why do I keep doing this? This is pain. Another design question so again, Youtube.
But now we're doing speech-to-text, and we say that there is their error. So kiss this guy instead of kiss the sky.
And so you have spaces. You have extra noise, words missing letters.
Give me, what's the problem? How do you fix it? Okay?
And so there's not an an answer. But this is again thinking about the problems we've talked about. From a slightly different perspective.
Okay.
Rhoda.
what is it?
And then there's a final. We got more like more true fossils.
more of multiple choices. A lot of
yeah, how do you like that? One
is?
Yeah. How about that one?
I want to be able to grade it and just go straight to the answer and not have to look at multiple things.
Okay? So.

You know. Sick.
Okay. Howdy?
Yeah. Good. When this weekend giggle, etc., etc., you wouldn't. You're gonna ring. You bring people to Africa.
You're gonna you're gonna get
anyone wearing a ring right now. Yes. Okay. Please. Sorry to cancel class or something.
Okay, yeah. Big week for us folks very big week.
matrix factorization.
What's up
homework do
yesterday? Yeah. As late as tomorrow.
Lot of activity on slack. I like to see that questions, questions.
questions. Okay, homework. 2. Good practice. I hope. Everyone sharing stuff on on slack. That's great. Check your answers.
So homework 2, we're on top of that. Okay. Midterm Friday.
midterm, Friday tomorrow. Wednesday, we'll do a review session. So by review.
So here's a couple of things to consider. Number one.
I have not written the midterm. It does not exist.
That's my assignment. Just for you. I'll also save this.
Typically, whenever I make exam. there's a knob, we turn right?
The knob. It looks like this. you guys know, like, sigmoid looks like
it's like this. Okay.
this is typically, this is the difficulty.
So there's this knob. and it's either gonna be easy or impossible.
And what I'm trying to do is tune that knob so that it's like right there.
But the problem is, I tune in a little bit to the left, and the exam is really easy.
I turn it a little to the right, and it's impossible.
So
when you're taking the midterm, if you think it's incredibly easy, good for you.
if you think it's incredibly difficult and you studied. it'll probably be difficult for everyone else
if it's incredibly difficult. And it's like you've never been to class. You don't know what's up. Well, I mean, that's on you. Okay. So just be aware that, like, I do my best to try to make it like in this middle room. But oftentimes
it's just hard. Okay, main thing
focus on fundamentals.
It'll probably be these kinds of things true falses. I haven't written it yet. On Wednesday. Come with your own questions. I had those old exams. I posted you. Wanna go through a problem on that. Let's do it. But I'm not gonna bring like a big set of materials to go over. Okay. So on Wednesday it's more like, Hey, cab? I don't really understand how this works, or hey on that midterm. What's that about? Or that old exam, what's that about? And we can work through it. Okay.
but I'm not gonna come here and just say, lay it out, because again, midterm doesn't exist. I don't know what's up. Okay?
Questions we good.
I would say. The main thing is this.
I like seeing your faces.
That means to me. You can come into class.
You've been doing the homeworks you're doing. The quick quizzes. Brush up on that stuff, make your cheat sheet.
You should be 5.
Thanks
if you happen to come into class, you're not watching the videos.
you're not doing very well on the quick quizzes. You're bombing the homeworks
for having done the homeworks.
You gotta study. But there's time.
Okay.
we cool. Okay.
congratulations again on football things. People are happy.
The weather's gonna get a little cooler. people even happier. It's good. It's good.
We're gonna finish. My hope is we're gonna finish this MF, this Mfing topic
right now.
this is a big day. This is one of my happy days. Okay, again, will this be on the midterm high level? Conceptual questions? Yes, technical details. No, because you haven't had a chance to do it on a homework. That kind of thing. You will see this soon, and it does lay foundation for the whole rest of the semester. So I'm glad you're here.
Okay? In fact, there will be a question based on something we've talked about, at least on Friday.
on the midterm.
Okay, matrix factorization.
II know, I keep showing you this today, we're gonna we're gonna finally fix it and finish it. Okay.
big matrix sparse, we do the factorization. So we need. We don't know how to do this right?
Where'd my pen go?
We don't know how to go from the big sparse matrix to the 2 dense
factorize matrices.
But if we could do that, then we go back and we can do dot product of every set of, in fact, for each user times each item, and we can walk through it and fill in the whole matrix. It becomes dense. Now I have estimated ratings for everybody. That's amazing.
I gave us our ratings. They look like this. We had 27 users, 13 movies.
These are the ratings look like. Ha, ha! Ha! Look at those silly ratings your classmates gave.
Okay, Waymo, we take that, I go through my matrix factorization. So now we end up with that. I use 4 latent factors, not 5 as in the figure, but it could be a 27 by 4, and then a 4 by 13
latent factor matrices. There they are.
Okay. So when we talk about. We're typically going to call this thing P. And this thing. Q.
So this is P. This is Q,
4 by 7. It's hard to read. But it's like
4 by 13.
Okay, so our whole goal is
like, you know how to find P. And Q.
I just gave them to you automatically. But these P's and Q's are somehow derived from these original ratings.
So the idea is, I'm going to give you a different set of ratings, a different matrix. And you're going to find the best P and Q that do this.
Okay? And we use those P's and Q's.
we go back and we use. This is this, users latent factors. This is, this is latent factors. We dot product. Add them up. You get a number
notation wise. We'll use a little P,
that tells me it's a vector and a little. Q, that's a vector it's just the U, the user u latent factors.
that thing. Item, eyes leading factors. That thing
vector dot and vector dot product
boom done. We did it. We got a rating. And I showed you last time we had Justice's ratings exact same thing. He had his leading factors.
We have the notebook, latent factors. We dock them. We get 3.7 one wamo.
So this is just like PU. Or P for
justice. And this is QI. Or we can cause it queue for the notebook.
Okay, so everyone should be super cool with this. Okay.
I've given you the examples. Even you don't know what's going on. You forgot matrices and vectors, you can eyeball it and say, Yeah.
P's and Q's, PQ. And it's all good. Okay, now. And we did that. I showed you this last time. We could do all those dot products at every pu and every qi.
and fill in the whole matrix. So now we have a number for everybody.
Amazing. Now.
how do we do that? How do we do? How do we do that?
How? How do we do that? And so we said, Oh, you could use Svd.
And I showed you an example, last time, I said, Svd. However, has this problem, which is. it treats the empties as zeroes.
And so it's trying to find latent factors that would guess zeroes. which means it screws up big time because we don't want empties to be zeroes. We want empties to be unknowns.
So funk
folks Svd is ignore the missing ratings only focus on the known ratings. In other words, can we pick a P. And a Q
that best recover those original ratings that I do know about. If they do, we think we did a good job. and then we'll use those Ps and Q's to fill in every other entry.
That's the big insight. So where we left it last time was.
Rmse. Oh, yeah, this will 100% be on the midterm that I have not written
note to self self.
Rmsee, cause it'll come Friday, you're like, where's the rms? And you like, have
I didn't lie. I just forgot.
It happens.
Let me see.
Okay. And remember, we said this, P. You, this is where it's confusing. I'm sorry I shouldn't use this notation, because we really would say, That's that's our hat.
So you don't confuse with the other piece. Don't freak out. Okay?
We said, Okay, yeah. Yeah. We have this R. Hat estimated rating. actual rating.
And so we said, Okay, well, we can get rid of the we can get rid of all this stuff because it doesn't change it.
And we just say, this is some like kind of measure of the error.
This is like the error again. R. Hat.
It just says, you know. I guess. 5. You gave 3. I was wrong. I guess. 2. You were 2. I was right.
Okay?
And so the idea is, we're going to do this. But for every ui in like our known ratings
and so let's do a quick example
okay, so let's go back to are known, this is, gonna be hard. Okay, this is gonna be really hard for me. But
I'm going to highlight.
We'll just do justice. For a second. He gave a 5 and a 4 and a 5, and I'll go down to Elvis. Gave a 5, a 1, 2, 3, and 3. Those are known ratings. We know them
right? So what we can think about is, what is the error of
like? II picked a P in a queue, and then I got an error.
What did I say? Got? This is so impossible to do. He did shrek, Barbie beat, or miss to one do do. And then, El, what did Elvis do? Elvis had
all the first 5. Okay, sorry. This is like crazy 1, 2. Okay.
so let's go see what we guessed
versus the actual. the actual rating. And so, in other words, we have like a 4 point.
You know what?
You know, what? Big brain? Yeah, I'm a copy paste. This book. Yeah.
And that crazy
big brain.
No. why does it do this to me?
It's cruel.
Why, why would it just give me the first layer?
Thank you.
I'm just gonna do the first 4 copy.
We'll go all the way back.
Why did I do this already? Not gonna be stupid? We're gonna erase all that.
The first 4
stop it going way back.
we can. You can. You can paste image.
Okay? Thank you. We did it.
Okay. And then one more thing. What does our formula look like? I'll just grab
group
copy that
Facebook. Thank you.
You see this. This is my curse. You guys
fix it.
Copy. Dad, you could have done this ahead of time. Oh, it could have.
But I didn't. Okay, so
and again, that's R hat. Okay. Finally, people here we are.
So all I'm saying here is we're saying of the known ratings. So that means this one.
We had no ratings, had no ratings, so I'm not going to use those. Those are gone.
you understand. They're I didn't know them, so I can't compare to them. So all I'm going to do here is, say.
what is my error? Well, my. I guessed 4.3 8 5 8,
and I guessed 5
squared, plus
I guessed 4.1 7 6, 4, and I guessed 4.
But I don't consider these these few, because there was no rating to compare to
right. I had zeros, but Zeros was just missing.
and then I go through, and I add in well, I guess 4, 8, 3, one minus 5
squared, plus
3.1606 minus one squared, plus
3.1043 minus 2 squared. plus 3.0 6 2 9 minus 3 squared. And that gives me some error. Okay?
And what I want you to notice is you'll notice some things we did. Great top, we guess 3.0 and this 3.0 here, we guess 3.1. And there's a 2 here. We get 3.1 into one. So we're kind of far off there, because the point is, we're doing this like compression. We're taking, like many, many dimensions and smashing them down. So necessarily, we're losing some of our like ability to recover the exact numbers, cause we're doing like a compression.
So the whole point of all this is, this is my error for this choice of parameters. These are the P's and Q's that led to these numbers.
But what if I had different P's and Q's
different latent vectors like latent factor matrices? I would have different estimated ratings, I would get a different error. Okay, so the whole idea is, the error is like this proxy for the goodness of our choices of P's and Q's.
So if we pick if we pick bad P's and Q's, we get bad estimated readings, meaning we get big errors errors up problem.
Okay?
So all of that is to say, we now have objective function, the objective function. Let's minimize that thing over our choice of P and Q of this summation of whatever the iheart ui and the actual
squared. So we're gonna try to minimize this thing. And so the whole point is, we said that our hat.
That's just PU times. Qi.
we're cool effects. I showed you that before. You all nodded your head and said, That's easy.
That's just this user latent factors. This item, latent factors, dot product. I get estimated rating, we said, 3.7 one for justice, for the notebook. That's all that is
so. This estimated rating is
which came from the link factors, the link factors dot product. Give me that number. Okay.
But the whole point of this is like we're trying to pick the best Ps and Q's to make our error. The smallest basically
minimize our error.
That's it. Okay. Now.
how can we do that.
How do we really do that? Because so far, I just gave you the answer.
So one, the first bad idea would be, let's randomly guess. P. And Q.
Remember when I say P. And Q, I'm talking about, that's P, that's Q, like, let's just fill it in with random numbers.
Okay, let's do that right now. In fact, I'll I'll pick the random numbers, which will be
I'll make them all once.
Remember, there's 27 by 4, and there's 4 by 13.
So my first guess is, I think, the best. P's and Q's the best latent factors are going to be all ones.
Okay.
so let's use those and see what happens. So
let's go back to
Oh, man, is this gonna work? Let me go back and get this thing hang on, hang on.
Let's go get these first 4.
Yeah. Dummy
copy.
Okay? So again, those are my actual
Rui. Now, my estimated, are you, Ruis? Which is just pu times. Qi.
Well, I guessed all of my p's and q's are ones.
So what would we guess this would be.
or we have that.
So what I'm saying is. oh, you mean the PUQI. That's just 1, 1, 1, one.
1, 1, 1, 1 one times, one plus one time, one
that's just 4. We cool with that. Oh, so I'm gonna guess 4 for everybody.
Okay. So in other words.
my, this is the actual. So then my estimate is again, we we ignore those.
This is my estimated
when all latent factors R. One. So now we can say, what is the error here? It's just gonna be 5 minus 4 squared, plus
we nail that one plus 5 minus 4 squared. Plus
okay? And so I'm just basically saying, you know.
And so we get some number here, which is like 1 0 plus one. That's 2, 3, 9, 11,
plus 4 is 15, plus one is 16.
So I think this is 16, I hope.
Okay. But what if we went back and looked at the actual ones we got here?
Less than 16?
So it seems like these are better choices than
this, this choice of all ones. Okay.
so what do we do? Instead, we say, well. so we got an error. 16. Let's try a different set of random numbers.
We fill it in. We measure the error.
We do it again. Have you guys ever heard of? Bogo sort. Yes.
Bogo sort my favorite sort. What are your favorite sorting algorithms? Quicksort merge sort? Where is your other favorite search or a sorting algorithm
support these.
So how does Bogo so so sorting algorithms are quicksort. Right? You have your pivot.
And it always didn't make sense like the pivoting in your what it's like what and merge sort and bubble sort.
So given a collection of things, how does Bogo sort work like, for example, given a deck of cards, and I want to put them in some order.
Bogo sort says you throw them up in the air, and when they land you look at them, and you say, are you in order? No?
Okay. Well, let's try that. You take those cards and you throw them up in the air, and then you look at them. And you say, are they in order? No. So it's just random. Okay, that's what we're doing here. We're basically just trying random things
and seeing what happens? Does this seem smart to you
that seems really dumb? That's why I say, the bad idea.
We don't want to do this. So instead, what we want to do is
what if we started from some random spot like what we just did all ones?
Then we measured the error somehow. Use that error to figure out which direction I need to change the P's and q's.
So then we can sort of like take steps towards a better solution.
so we can like iteratively, do that and say, Well, they were all ones, but really based on this error, I should probably move that one down and this one up. And but but and we could kind of move the numbers around.
And we do kind of an exploration or a search like that. Okay. that is exactly gradient descent.
Okay, we're gonna intelligently search the space
until we find the best P and Q's, that minimize our error.
The idea in general, you've probably seen some curves like this is like, here's some function. And if we're trying to figure out what is the the value, let's say, of X that minimizes this function.
and all we can do is pick random. X's
right, that's what we're doing. We're picking random P's and Q's. So the idea is, what if I picked this X here.
that's like my X 0.
And we would say, Okay, you are. What you are.
All I can do is pick X's well to be a minimum, right? We don't know, actually, right now. But we can see there's some kind of we take the derivative. We can see we can move in that direction.
So the idea is, we say, Oh, let's jump that way. And then we tried this X.
Well, that seems lower. That seems good. So we go this direction, and then we hit end up here, and then we go this direction and we end up on the other side.
and then we come back. And then eventually, hopefully, we find the X that minimizes. Okay. So what we're doing here is basically doing some intelligent search of this space
in in this case, just in this X dimension. trying to figure out what minimizes F.
Now let me ask you this, we don't have one X. How many parameters do we have that we're trying to minimize over?
What are my parameters when I say parameters?
What are we? What are we trying to find?
No, no, but like what? What are the things we're trying to find? What's the notation for the things I'm trying to find?
No, you said something. The something. Vectors.
These are the parameters. I guess. 1, 1, 1, 1, 1, 1 one. Those are bad. I showed you from my code. I did before I do the better numbers.
But the idea is I'm trying to figure out those numbers. So I say, what are the parameters I'm looking for? I'm looking for these parameters.
so I don't have one. How many do I have?
I have 27 times 4.
I have 4 times 13. So really there are M. By K, plus n, by K. Parameters. that I need to figure out what they are not just one X.
But instead all of these things.
this is where this is, where your heads explode and you go. What can you can't do? It's impossible.
So this is where I'm going to hold your hand
of metaphorically. there's no harassment happening in this class.
my professor said. I had to give him a hug or something. It was very weird, and we're holding hands. No, no, but I'm gonna hold your hand intellectually
and do it. So what we're gonna do is
we're going to learn how to do gradient descent. Okay, gradient descent is a method to find the minimum.
We're trying to minimize the error.
You want to minimize error. How? We want to choose
good P and Q that make the error small.
Okay?
So first, you have to ask yourself, what's gradient.
It's the vector of partial derivatives. You're like man. I'm A. Cs major.
I don't do derivatives
as a high school that was like Cal. I don't want to do all that.
I didn't like that class. So I'm going to remind you.
Oh, yeah. And here is gradient descent. In practice
we have a function. XI just showed you that function. X, it looked like this. This only our function doesn't have a single x. It has
27 by 4 p. And 13 by 4 cues. And it's like the super complicated thing. But the idea is we find we pick a point whoops.
We pick a point. we take the derivative.
We figure out what the derivative is, then we move.
So, for example, we walk down here and we keep doing that until we convert. I just showed you what kind of gradient descent is like in a picture. That's what we're trying to do.
The key here is, we need a gradient, which is again, what is a gradient. It's a vector of partial derivatives.
So just as a reminder, I got some examples.
Let's do it. Check it out. Here's a function, not of one thing, but of 2.
We don't have 2. We have
4 by 7, 27 plus we have, but we have a lot more. But here's 2
is tenx squared minus fivex plus 4 plus y squared.
So the gradient of F is just the partial of F. With respect to X partial of F. With respect to
why so what is a gradient?
The gradient is just a vector look. It has 2 elements. That's a vector
partial derivatives is the partial derivative of this function respecting each of the variables.
x and y. So then, I ask you, well, what is the partial of F with respect to X.
And what is the partial of F with respect to Y. You guys remember this stuff.
The 2 comes down 20 x mine has to lose the x.
so it's like 20 x minus 5.
You guys remember that.
And then the partial with respect to Y, remember, you ignore everything else except the Y, bring the 2 down.
It's 2 y. so what is the gradient of F,
it's 20 x minus 5, 2. Y, okay.
So then, if I said.
not using gradient descent, but just using your big brains. And I said, What are the X and Y that minimize this? So if we want to minimize
F of XY, what is like the X star and the Y star, I would choose
what choice of y makes the smallest
go out.
Well, think about it like this.
We take 2 y. and you can set it equal to 0.
And so y would be 0, because we're saying, basically, if I pick any y. if I pick a negative line.
you square, it becomes positive everywhere.
We're trying to minimize it. So how do I pick a Y that makes this whole thing small? I would think y is 0. Any other choice of y makes f of XY. Bigger.
What would I choose? My X to be?
1, 4.
Oh, you mean 1, 4. That would be 5, 9, 5, which is 0. So you can go plug all this in
and you get some number and hopefully should minimize F of XY, you pick any other X and y, it's bigger.
Okay. you cool with that.
Now let's go solve it by using our gradient descent instead.
Okay? Because we're gonna have not just 2 variables. We're gonna have
a lot or at Netflix scale. We're gonna have
half a million times 50 plus 18,000 times 50,
a lot.
Okay? So we're gonna do gradient descent here. So how does gradient descent work? Remember, we first, we guess.
okay, we'll just start with the X's. And so let's say, we guess X is equal to 5.
Then we update. And so we say, Okay, my, my new X is basically my old ex
minus this work gets a little tricky. There's some like learning rate.
and I'll say the learning, I'll say that's equal to point O. 3, just to make it clear
times the partial derivative with respect to X
at the old.
Okay? So what was, let me go back here and show you gradient descent.
evaluate it, move in some direction. Is the new function of the old minus this gradient at that, at that Z point.
So what are we doing here? We're saying the new X is just the old X, and then we want to move in the direction of the gradient.
which is the partial of F with respect to X
of the XO. And I said, this is a learning rate. So you have to figure out like
you're going to figure out what this thing moves, and you're going to move it to some fraction of that. Okay, this is some parameter. Don't worry about it right now.
But remember what was Df. Dx. We said that was 20 x minus 5.
So we said, 5 minus point 0 3.
What is
20 times 5 minus 5. Right?
This is just Df Dx, which is 20 x minus 5 20, Am. I? Said, old X is 5.
It's like 95.
So if you work this out, it's 2.1 5
I was at X is equal to 5.
I moved in the direction of the gradient. It's now 2.1 5.
Let's do it again.
A new ex new.
It's just the old 1, 2.1 5 minus 0 point 0 3 times
df, Dx of 2.1 5
if you plug that in.
and I want to do that. No.
it's 1.0 1.
Have a blank slide. No, I don't
got a blank slide. No, I don't. Let's copy.
How do I move cut
based?
So what we said? We said it was 2.1 5. Then it was 1 point O one. If you do it again, it's 5, 5, 4. If you do it again, it's 3, 7, 1, 6. You do it again. It's 2, 9, 8. You do it again. It's 2, 6, 9. You do it again. It's 2, 5, 8, do it again. It's 2, 5, 3, eventually, it's 250,
one quarter. You told me it was one quarter. This works. If you didn't follow that. check yourself.
This is derivatives, man. You did that in high school. So let's go back and remember what we're doing. Let's go back and remember what we're doing.
how to chick, pick good P. And Q that minimize my error.
we said, use gradient descent.
define the P. And Q. What are we gonna do? We're gonna randomly pick a P in a queue. Then we're gonna take the gradient. What's the gradient?
A vector of partial derivatives? Then we're going to move along in the direction until we find the minimum.
So right now, I didn't want to do P. And Q, because there's too many parameters and hurt your brain. So instead, we did
just 2 x and y.
So this is our our gradient vectors of partial derivatives.
And we just said, Oh, I can just take in. I can guess X,
I can do these steps. boop, boop, boop, boop, and eventually I'll find point 2 5, which is the choice of X that minimizes the function.
Okay, we do the same thing for Y, and we'll find 0.
Okay.
so what I'm hoping you get away from this is, you know how to do gradient descent.
If you have a function, you just take the partial derivatives, and then you just start evaluating it and doing these steps.
And eventually you find good P's and Q's.
Okay. Now, however.
in our case, our function is not that simple?
So in our enough world. we said.
we don't have a function of X and y.
Nope. we have a function of
Oh, crap!
It's like
P. One times. Q. One.
p. One times. Q. 20, so that's how I would hang on
so wait, wait! What are my parameters again? So that's like
P. 1, one, and that's like P. One K. And that's Pm, one that's pmk, so really I have a function of
p. 1, one, p. 1, 2, p. 1, 3. In our case. P. 1, 4.
Let me have dot dot dot dot dot all the way down to P. It's not M. How many n's did we have? We had
27, 27, one dot dot, dot. P. 27, 4. And then we also have the cues. So we had. Q. 1, one.
q. 1, 2, q. 1, 3, q. 1, 4 dot dot all the way down to Q.
13, one Q. 13, 4. So we ended up having M. By K, plus N. By K. Parameters.
which are those things.
So we have to do the partial derivative of the some. The function that is all of this with respect to all of those
I don't want to is 2 Dagmini. It's too much.
Oh, yeah. But what was my function? F, do you guys remember what our function was?
It's the error.
So we said, the function we're trying to minimize was like F of all the P's and Q's, was that summation ui.
like the PU. Qi. Minus the RUI
and squared. You can't see that.
That's that's the function. That's the error. So we're trying to minimize this thing.
We got a whole bunch of parameters. We gotta do the gradient.
So we gotta find a gradient. So yeah, crap.
So now, I gotta do. What is the gradient of this? F with respect to P. And Q.
It's the partial of F. With respect to p. 1. One. the partial of F. With respect to P. 1, 2, all the way down to the partial of F with respect to the partial. With respect to
Q, in k, or 13 4.
Okay.
So again, that has m by K, plus n by K.
Parameters. Okay.
are you with me?
That they might?
You do this to not be heavenly minds per second?
And so what's that and move back.
Now.
yeah, maybe you've already gotten since the ease, as I keep repeating with computers.
think you can really always keep your eye on where we're going. I went.
Yeah.
we have the
matrix of racism. You're trying to break it in these 2,
like, in fact, the basis has been gained.
All I want to do is find the error with respect to. I'm going to make guesses for these and Q's, and then measure the error with respect to the actual rating you gave me.
I mean that they're small.
So we're gonna do now is we do gradient descent. We're gonna make an initial choices of Ps and Q's.
Then we're going to find the direction that all of these parameters move.
We're gonna measure some error. And then we're gonna update all those parameters. So they move a little closer to making that number smaller.
and then we'll try it again. We'll see our errors, and then we'll move the parameters. A little more. and then we'll do. We'll keep doing that over and over and over until hopefully, the error is as small as we can get it, it'll probably never be 0.
Then it'll be small
and the small. Then we say, I think these are good choices of these and queues. I'm done.
We've done matrix factorization. Now, we do all the fun stuff where we go. Guess everyone's raining. And we do all that stuff where we guess the unknowns and we're happy.
Okay, so
you guys in the last 10 min, do you really want to see how this the story ends? You know.
did you guys when you were kids. Did you ever read the lady or the tiger?
There's a key, and there's something where there's like, there's a door.
and behind one is the late, you know, the Princess. and the other one is like the tiger that eats you.
Okay?
And so this is long story, and you know whatever.
But at the end of the story there's like your hero is there at the doors and is gonna choose which one
and you don't know like, does the king know? And he's placed it a certain way, or does the does the hero know, or whatever? And then, as the hero goes to open the door.
The story ends. and you have to complete the story in your head.
and if you're like me, you're totally dissatisfied. You wrote the story. What happens, Lady Tiger, which one
so
rather than ending the class here. And you can just wonder
I'm going to show you that it's truly a tiger that if you really like this, it will consume you
and each you. But you'll be better for it now. Okay.
I had to write this stuff down, too, because I get lost. Okay, so now. this is our partial, we're trying to remember, we're trying to
minimize
pick P's and Q's
that minimize our for our choices of, I'm gonna write like this, PU. Qi, minus our ui, remember, that's my R hat. Ui, that's my guess.
That's the actual
squared. Okay. okay. you can also think about this all as this is also this is the error.
Okay, we know this is the error, right? I guess something. It was actually something else. There was an error. Okay.
so what I'm going to do now is I'm going to rewrite this. This is where it's going to get fund. I'm going to rewrite this, minimize P and Q
of the summation of Ui.
PUQI minus RUI all squared.
We're cool with this right?
Well, I wrote this in vector, notation. But really, we have like 4 actual latent factors. So I'm gonna write the dot product. I'm gonna write it out.
So we said, I'm going to write as summation of Ui.
Okay, now, I'm going to do a summation of Imagine. Our case like X is like one to 4,
right? So I might just say, What is the PUX times the QIX. So I'm just walking down and doing the dot product
all of that minus RUI,
does everyone buy that. So we did the dot product right? We're just doing those
thanks. So I'm just saying, just walk X is one to 4. Just pick, you know the first one, the first one, the second, 1, s, third, and fourth and fourth. Okay?
So remember, we call this our error. So we're gonna do. Here is we can take the partial of the error alright as error squared with respect to P
you one of those X's.
So remember, this is just like PU. For some user, the first latent factor, the second latent factor, the third latent factor. So if we were taking the partial derivative of the error
squared.
we have to figure out where are the PU. X's to figure out how to do that.
So what do we end up doing? So 2 comes down.
You believe that 2 comes down. So it's
2 times the error that doesn't change.
I'll write it as ui.
This is.
the 2 comes down. Still, the same error for you on okay. minus. We need the partial with respect to PUX. Of just the error.
Okay.
it's a partial mistake.
Okay? So
that's 2 times the error. Ui.
okay, are you guys ready for this thing
partial of PUX. Of summation. Good gravy. I'll do a different X, so you can tell. UX. Prime.
QIX. Prime minus RUI, paren boom.
so I have the particle this 2 times the error.
then we have to this partial with respect to the error, like the Us.
So I'm going to use a different X, I'll put X prime. It's PUX. Prime times, QIX prime minus RUI,
okay. And so.
what is what is the partial of this thing with respect to PUX. Like, how does it move with respect to PUX like, how much does it move?
But just move moves. Qix. This is like a number like 3 like this is partial. With respect to like Threex. You would just be 3.
So which is going to fall out of this out of this whole thing over here is you're gonna end up getting QIX.
Sorry this is not a.
Did I do this right? Calculus people?
So this is all to error. Ui. QIX. So
check this out.
So the PUX. Prime. The new PUX. If we use gradient descent.
is just the old one. We have a learning rate.
That's a learning rate.
and we move in it the direction of the partial derivative, which is 2 times the error of ui times. Qix.
2 times the error. Times. QIX, that's the partial. So the update is just like when we did before, with like F of x squared or F of XY, and all that stuff.
It's just my old gas for P. Ux.
I just move it by some function of the error in the QIX,
okay. So they do this trick where you can basically like this, this learning rate in 2, you can like fold together because it's like 2 times a number. It might as well just be a number.
And so you end up getting. These are the update rules, and I'll show you how kind of cool they are if you do it for everything. So you'd find that PUX. Prime.
It's just the old pux minus learning rate times the error of ui times, qix. And if you also do the qx. The qix, it's the old Qix
error ui. But with respect to the PUX.
Okay.
If you didn't follow all that it's okay for now we're gonna get a chance to play with it in the homework. And really like.
really get what's going on.
But high level. What's happening. I'm trying to show you this.
You can go back and you can find
for this insane minimization. We're doing
where we have all these parameters, P's and Q's and Blah. At the end of the day we can end up finding a gradient descent update rule that, I think, is very kind of elegant.
Okay? So in other words, I guessed all my p's and Q's
right. I measured my error. Now I go back, and I say, now, I gotta update every p and every queue.
So for, like p. 1, one, I say, well, what was the old p. 1 one, that little entry in the matrix. I say, whatever it was, I moved it by some function of the error that I got for that user for that item. So I was way off.
So this is going to count for a while times the Q that I had. And so we're gonna move it a little bit.
And so we sit there, and we walk through the P's and Q's, and we update every P.
Here we go for the queues. And what's cool about this notice? Let's say my error was 0.
So I would say, What is the new Qis? Well, it's the old Qis
Beach. It's an arrow.
If I said, what if the error was 0, I guess it exactly right. You're basically saying, Oh, don't update the queue at all. It's exactly the same you did perfect.
But the point is, if every time I'm making an error, I'm moving it a little bit. Boom, boom, boom, boom! Boom!
Okay. now go back to what we're doing here. Let's turn the crank.
We guess P's and Q's. We measure these errors, we move all the parameters, we measure the P's and Q's.
We get our error, we move them again, remove them again. We keep going until at some point the error starts to converge and doesn't change much anymore. And then we say, Tada, we pick the best P's and Q's. We did it.
Okay.
Now, here's kind of the big joke about all this.
which is, I think this is really cool. But if you were to do all of this.
we did all this hard work.
We still don't know him.
That's kind of a joke. However.
the ideas
are so fundamental. Every method you do now is a variation of this.
Okay.
so this is, you're supposed to go. Ha! Ha!
Like all that pain, and then we still no money, no money. But you can get a progress prize. You get a little bit of money, but you don't get the 1 million bucks.
Not yet. Okay. I know this is a lot.
Yeah, I know you wanna go offline and think about it. Don't think about it. You got a midterm on Friday, what do you? What?
However, don't think about this? This is not on the midterms.
Well, since you came today I may ask you something like, Hey, we did all that. Ms.
Do we win the money?
No, say, no. But I'm not gonna ask you anything super technical about this at all until after the midterm. Okay, we'll do on the homework. We'll have fun later on. Okay.
with that good luck. See? You all on the flip side.

Houston. Wow.
hey, friends, let's get started. So let's let's hang on. I got some kind of computer issues here. Hang on 1 s.
let me see. Okay, I think it's working. Okay. So let's let's get started. Today is a great day. Very exciting day.
Homework
homework one. We posted a sample solution so you can get a hold of that. Take a look at it. We will be releasing the grades later. Today.
so grades are coming soon.
I would say we were relative lenient.
depends on how you feel about it, but, like, you know, we tried our best to find points for everybody. What I would say is, you know, definitely. Take a look at it and make sure you know what's going on. Homework. 2 is out now.
It's due when Sunday, Sunday. as late as Tuesday is the latest.
and we're going to release the solution on Wednesday. So you have it. For the one part.
questions concerns comments about homeworks. Anything else.
Also, the swag is blowing up in our slack. I'm seeing a lot of really great stuff. I'm gonna basically put out kind of a last call. And then we may go for a class vote or some kind of class design cycle where we try to figure out what we want.
But my goal is to kind of get that done so we can get those things ordered. I don't know how long they take, but it would be great together. Yeah, okay, it's gonna be great. It was gonna be really funny. Is there gonna be a few of you who don't, and then we'll you don't wear your shirt.
It's gonna be awesome. Is anybody going to the football game tomorrow?
Congratulations have fun. It's an early kickoff. So when when are you guys going
in the morning?
Honestly, you're leaving today, be careful.
Have fun. And hopefully, we'll win. Right?
You're gonna today, folks.
we are one week out from our midterm.
the reminder. We are one week away. It's gonna be in class. There will be people here who never seen before.
Maybe your roommate. Okay.
I have posted old exams on canvas.
If I were you I would look at them. What a resource. Okay, now there will be questions on those old exams that don't relate. Ignore them. But you'll get a flavor for the style of questions. So you try to understand, not just the
content of the question, but like, what are the kinds of questions I ask? Okay.
remember, you get 18 key front and back anything you want on it. The questions again. Look at the old exams. They'll be in that style.
The topics. Where would I study? Everything is fair game. Of course you want to focus on fundamentals. And so when I say fundamentals. But but give me 3 fundamentals, you got to know
TFID. F. Amen, brother.
yeah, I don't know. Fundamental
indiscd. There's 3 of them
almost guarantee you'll see all 3 of those on the midterm.
So for you to get on the midterm and not know how to do that again. Shame on you!
Easy! You should know it now.
So what I'm saying is, you want to kind of layer your study. which is lockdown the fundamentals.
That's 80% of the things are then go back and worry about like, well, what if you ask some really weird edge case questions? Okay.
don't start on that stuff because you're going to spend all your time in the weeds. And you're not going to get cosign, or you're not going to get tf id
any questions as that were a week out that you want to talk about.
Yeah.
no calculators.
no calculator. Would you guys want to like on? Let's say.
Monday or Wednesday, like.
do like a review, go over old exams something like this seeing lots of this? I'm seeing a lot of this, some how about? We'll do a review.
When do you guys want it? Monday?
Okay, we'll do a review on one of those days. Okay.
your voice has been heard. Now, any other comments concerns questions about anything. Yep.
Where is the cutoff of material.
so good questions.
certainly. Yeah. And we're always upstate. This is fair game. I think I mentioned this before, which is.
I may ask you about matrix factorization that we're gonna talk about today. But I'm not gonna ask you a super technical question because you haven't done it on a homework. You haven't really sat with it for a while. However.
you had 2 homeworks, all those concepts guaranteed. You're gonna see a lot of that stuff. So but I may ask you something about the stuff we talk about today, but probably not super deep technical.
So you still want to study it. But like.
that's kind of understand what we're talking about. I'll give you some examples as we go along, but this is the kind of stuff that'll show up on the next mid or so on the next assignment after the midterm, and it'll certainly be on the fine side.
But yeah, all this is fair game as of today. It depends on. Maybe if I do the review Monday.
then maybe, as of today, will be the last day. So good. Question, very important. Anything else. Great questions love it.
Yes, let's see, the
this is like literally any anything.
anything one person could make a cheat sheet, copy it, and share with everyone else.
However. that would basically go against the principle of it, of the cheat sheet. You know the principle of the cheat sheets
by making the cheat sheet. One does not mean.
yeah. by making the teaching you don't need. You don't need it.
Okay? So if you just take someone else's cheat sheet, you haven't done the process of thinking about the material, writing it down, etc., etc.
So you're kind of short-circuiting the whole point. Make your own cheat sheet.
You will need it. Cool.
cool, but yeah, anything you want, type it handwrite it. Take slides.
strengthen down, put them in whatever. Okay.
bring a monocle.
Bring a microscope.
bring a telescope.
Didn't matter. Oroscope, that's good. They don't help. Okay. Anything else.
These are great questions. I love them. I love them now
today is like one of my favorite days. Why? Because 27 of you have revealed your souls to knees in the class
by giving me ratings.
and your name will show up on here with the I will. What's interesting. I noticed Shrek, I believe all of this 27 responses, all 27. I think polar express, maybe all one as some polar express.
So I imagine these are like standard movies. When you're a kid, your parents put them on. We love donkey
we love Fiona
Polar Express. I'll I'll be clear it has been on in my house. I've never actually watched it, cause it just seems horrible and boring.
That's just me.
Just me. Okay, so here's what we're doing. We're gonna make it real. Because so far, this is the sort of abstract idea we have users, we have items. You have rated something. Remember, that's like a rating.
And the idea is we're somehow gonna take the same break is 2 matrices that encode our latent factors.
So
the hope, remember, is that if I took this user's latent factors dot product, then with this user's Latin factors, it would go back and recover the original rating.
Okay, that's sort of the idea. But if you eyeball this, you're like, I don't really understand. So what we did is this and say, Look.
our ratings. We have 27 users. That's 27 of you.
We had 13 movies. and I ask you to rate them. And so this is what our matrix really looks like. It looks like this.
So this is Shrek. And so Elvis gave track of 5. Okay. wheat. The age threat also applies.
Okay.
everyone for judging and looking. What did you you gave? What
if you walk across? It's hard to match up a little bit. So only one person who says.
not only one person is not seeing polar express. Okay.
but the point is, these are my ratings. So this could be Netflix. Right? It's not 27. But it's like, say, F, 517,000 movies. Okay? So we have a big matrix, we have a bunch of ratings. Okay, I want you to notice, I put zeros in there. You guys see the zeros
like, there's a 0 there.
The zeroes are. Basically, you did not rate it. So it's not really a 0.
So I put this note on here.
The 0 indicates no rating. However.
this is, gonna be super important in a few minutes, because they're not really zeros. They're not. They're missings.
So don't interpret them as like a low rating. It just means you haven't rated it. Okay?
So given that big matrix. I applied matrix factorization.
And I broke it into 2 submatrices. Okay? And so the idea is now, what I'm gonna do is say, I have my 27 users. Now, I'm gonna get the latent factors here. It looks like 5. I gave the 4 latent factors. Okay.
similarly, my 13 items also have 4 latent factors. Does this make sense? So I applied this. Oh, thank you.
Okay, great. So I applied, if and here's what it looks like.
and we'll talk about how I did this in a minute, so don't worry about where it came from.
So what we're looking at here is. so that first row is, who's the first person under
justice? And Elvis, okay? So like.
oh, undo
undo. Come back. So that first row that is justice.
That second row that is Elvis.
who's our last person on here. Ltm.
that is Ltm, so let's just take a look at those for a second.
Ltm. In our original space is the last one. 3, 2, 4, 2 pertinent together.
Wow! Did not rate. The notebook did not rate Barbie, you know, 4 to be movie. It's hard to see either. 5, 4 app has not seen the room, dot dot dot dot. Okay?
And so we've done is we take in justice. We take in Elvis. We've taken Lcm. Everyone else.
and we squeeze them down 13 dimensions to 4,
and when we do that.
Now, notice in the 4 dimensions is point 5 8.7 9 1.2 1.2
Ltm. Is 1.4 5 1 7 3.4 0 point 9 one. So if we compared Ltm and justice, you see, are quite different. The numbers are quite different.
Okay? Question, yeah, we're gonna do that just a second. Where they come from. We'll get that to that in a minute. The point is, now, I've got those numbers. Okay? Similarly, what we're looking at here, it's hard to see. But basically
it's hard, because basically, it's it's forward. It's like, it's 4. It's 1, 2, 3, 4 latent factors. and that first one right there is for what was our first movie Shrek.
So it's hard to see. So shrek
trek is that 1.4 1.6 1.3 0 point
for 3,
and then let's do a different one. Let's do
1, 2, 3 is Barbie should use pink for Barbie.
so we'll say
so. This is Barbie.
and so
I'll just highlight Barbie as a 1.4 1.0 9.4
point 7 3. So just in the 4 dimensional space.
Barbie
and Shrek are very similar in the first dimension.
a little different, the second dimension very different than the third dimension. And you know, whatever the fourth dimension. So somehow they're they're sort of different. Okay?
Now, a natural question is. what do those meet. And so before I always give you examples. Well, you know that that's horror. That's comedy. or you know, for music this is like British pop or metal.
The issue is the only way we can interpret these is after the fact.
like my method, gave us these factors. Now we would have to go back and really look at all of the items. Look at all of the people, and then we might with our eyeballs think about oh, it seems like
the third dimension, or Sorry. The
yeah. This third dimension corresponds to something to see how different they are. Shrek versus Barbie.
But I didn't do that right now. So I'll leave that for you guys to try to figure out if you can. Eyeball like what the differences are. Okay, boys, we have our latent factors.
Is everyone cool with this?
Yes. remember, remember.
in theory, this is sparse.
Is it really sparse? Well, there's zeros, 0 0
zeros. 0 0 0 0. There's lots and lots of zeros in here. Okay.
when we did our matrix factorization, how many zeros do you see? 0? There are no zeros. So this is now dense.
Right? So we took these big, this in theories, big, sparse matrix. And now we have these 2 super dense submatrices. Now.
where were that last time, I said, Oh, if we took those latent factors, this user latent factors, I didn't show you
this item. Leading factors dock them.
He will tell me the estimated rate.
Okay, so this is the cool bit I can go back and like densify
that ratings matrix, I can fill in all the blanks. So what I did is, let's go back and look at our buddy justice.
So justice has not rated. I have a 0 0 means not rated. Okay.
what I would do is speak just as 4 dimensional representation. I can take the notebooks for dimensional representation. dot them because the idea is they're the same
4 factors. Okay. so the idea here is, if you look at justice. Shrimp is
Hi, I'm sorry. Well, this is the notebook.
The notebook is 1.0 8. And just this is kind of low in the first dimension.
Okay. so let's see what happens. So we take the user vector, we take an item latent vector. we dot product them. We get the estimated rating. That's what I did for justice.
This is in vector, point 5 8 point 7 9 point. Remember that point 5 8.7 9. Where'd that come from
point 5 8.7 9?
And then the notebook, the notebooks for?
And so if you do 1 58 times 1.0 a plus point 7 9 times point 7 4 plus if we dot product, these guys, you get 3.7 1.
Okay. so the idea here is. even though justice has not rated the notebook.
We believe you know he likes these 2 dimensions more than those 2.
And this one is pretty high in this third dimension, right?
And so we kind of think, yeah, he give a pretty good rating of 3.7. He's not gonna love it because he's pretty positive. Okay?
And so by doing that, we get 3.7 1. So that's my guess for justice, for the notebook. Okay. now, we can do that for every entry.
So if we did that for every user item error. this is what we get. So now, I've totally filled in the matrix.
Okay?
So again, justice. just to focus in
before he gave, I think, 5 to strength. Notice the predicted rating for Shrek for dozens is only 4.4. What's that about?
We did not guess. 5. We made a mistake.
you know. He made him say, yeah, he he really believed it was performed.
Hold that thought we filled in frequent 7. We filled in. Oh, not one, Barbie, very much relative to the book. I didn't know I was. You guys love B movie.
You walk along and see all of the estimated ratings, not just for justice.
But again, we could go back to. Let's go back to the original ratings.
Does anyone here want to like stand up and claim their role?
Say, your name?
Yeah, it was, he was. Actually, it was really long. And so
so let's take a look at each
fact. I'm gonna do this and see if this will work.
Okay? So there's yeet.
So ye in here has some latent factors. We can take yeast latent factors.
That's not. It's not like a slur or something, is it? Okay? It's good. Okay?
Cause I will. Basically, you type it. I will say, it's not okay. It's like Bro.
that didn't work. Hang on folks. We're having some issues here. Yeah, let me erase
that.
Eat
happy. It sounds vaguely like a scandalous. But it's not
okay. Sick.
Can I grab it. Now, how do I grab it?
Where's my! Where's my grabber?
I don't like
that. Let me. No folks, we're having some technical difficulties here. Me do this.
Oh, my gosh! This is embarrassing. So
let me good gravy.
It was the base of the phone. It it wouldn't be okay. Let's copy all this for Yeats
the things we do
or eat
it's hard to see. But let's just see that. Okay.
so let's go take a look at what's happening here. Right. So a few things to notice right? So yeet
I'll just do that one like, look at those 2.
We're guessing. It's point 6.
We're guessing at 3 one. Okay, if we compare
our latent factor predicted a 4, ye gave 5.
Okay, ye gave a 3.1 to the notebook. We guessed a 3. How about the 12345678910.
Whatever movie that is, you gave a 5, we guessed
for the bloody, bloody block. We can fill in all the things. Okay.
I can't tell those. Is that how it matches up? Yeah, okay.
So do we have any. What are some highly rated movies for? Eat? Well, how about this one? This is very low.
We're guessing very low for the room.
But are we guessing very high for anything. The highest is this one, maybe inception.
Have you seen inception? So our recommendation is you should we think you're gonna like it.
The class stays behind it. Okay, we don't think you should see the room. We think you're not.
Yeah. You see, some higher than 5. Right?
Okay? So
we'll talk about where this came from in just a second. But I want, does everyone follow along like you? Give me sparse ratings, matrix. I apply magic matrix factorization. I get
those 2 matrices somehow. Now I go back, multiply them all out. I get ratings fill in all of this.
We're good to go now. I can use this to make my new recommendations. We can go in and say for you of the movies you have not rated
here are the ones that are most highly rated. We said inception we recommend inception. Should he see the room? No, we think you'll give it a low score. Don't see the room.
Okay, and we can do it for everybody in here. And you can find this online. So if you're in, if you're one of the rows, you can go find your own recommendations. Okay.
this is the power of matrix factorization.
Now, the challenge here again is, we would like to go back and interpret those factors. Okay.
I didn't have time last night to do that. but I'll leave that to you if you want to spend some time. Try to find some pattern that makes sense of the 4 dimensions. Okay? Another question you may have. Why did I pick 4?
So when you go back to this
picture, like I'm saying, there's only 4 of the compacts.
It could be 5, it could be 2. It'd be 10.
So that's a choice hyper parameter. We have to choose that we think best explains what's going on. Yeah. So great question. If I have more factors like it better accuracy. Typically, yes.
the the problem is, you kind of overfit. And so what we would like to do is say. you know, right now we only have 13 items
so like it makes no sense to have more than 13 factors, right? Or even have like 11 factors, really.
with only 13 items, really, maybe just 2 or 3. The idea is. there is sort of
signal. That kind of connects these movies in 2 or 3 factors versus like 10. Okay, but more factors, I add, the better we'll do it predicting. But the problem is, it may not generalize out to new movies or new users.
because basically, we're kind of like overfitting to some like, well, this is a movie. It happens it's animated is one factor. And it was based on Jerry Seinfeld.
B-movie.
or like, you know, the characters were like hats. It's like some factor doesn't really matter.
But because we kind of like have too many factors. We learn these kind of spurious connections. Okay?
So everyone. Big picture, you got this. This is cool. This is super easy. It's super easy, guys. I gave you the factors. Right? That's awesome. like. I'll give you code, most likely, for your next homework. I'll give you the code to do this.
and you can play with it. Now.
what are the latent factors? We said.
there's 5 that corresponds to these human labels.
We don't know. Okay, all we know is, if we go back here. you know, we said, for what justice
all we know is, the notebook is higher, this dimension and lower these 2 dimensions. And just this is in those particular dimensions. So it's like justice's preferences for these dimensions, and these are the dimensions that correspond to this item. But beyond that I have a hard time, at least at this point, telling what they mean. Yeah, so
does this mean like, are you putting in
values for each of these features. Or are you being like? Okay, the machine, the algorithm is figuring out.
That's the big question. The big question is, when's these numbers where they come from. and where do they come from? We're going to learn them. And I'm gonna teach you how how to learn them. It's gonna be super awesome.
Gonna change your life. Say, we're latent factors. Here's okay. Yeah, here's a bunch of leading factors. I just isolated them. You could try to figure out what they mean so like random is negative in this dimension.
What's that about? It's like, I hate that dimension.
whereas
like in contrast, styre is like super high in that dimension.
So if you went back and you looked at their ratings. maybe we would see some pattern. I don't know.
In fact, let's go back and look at Styre and Brandon real quick. Let's see if we can see something here in the original ratings.
So here is Steyer. And now this is hard to see Steyr and Brandon.
So it's like, Okay, yeah. You can see, Brandon did not like that
thing thing.
Everyone loves track. Great design
not rated, not rated
like fired love. Barbie
Brandon is Brandon here today.
If you're here, you need to get out.
We love carbon
best picture.
What's my guy's name. Yeah. Ryan Gossen, best actor.
Oh, go, go, go! Go! Cost the best. So we notice.
Haven't rated kind of close kind of close.
Okay.
my hunch is some of these early movies. So it's a big disparity. Okay? So that kind of gives you a signal that, like again. Here.
somehow, Steyer and Brandon are quite different. And you have to do some more analysis. Okay, I don't know. Okay, but in theory, to do some analysis and try to understand what's happening now.
Okay?
And again, this is in the item space. Again, it's hard. It's hard to look at, because it's it's folded over. But it's like again, it's 4 dimensions.
just to mention 1, 2, 3, 4, and then there should be 13 movies down.
There's 4, 5, and you should keep going. And again, if you look through this, we could try to make some interpretation. I'll leave that to you to figure out. Now
you can think about what matrix factorization is doing is kind of like dimensionality reduction. Each user is represented by a 13 dimensional vector of their rings.
And I somehow am president of 4 dimensions.
Each item column is represented by a 27 dimensional, vector
with 27 students who rated and we smashed those 27 down to 4.
Okay, so it's kind of like reduction. I showed you last time in the original matrix. We, you know, for the Netflix prize. There were billions of entries. But in the factorized matrices they're only millions.
So we took this big space and we slammed it down into a smaller space. So
the intuition of how we do that imageality production. We're trying to pick where those factors come from. We're trying to pick those numbers
that best explain the data. What's the data, your actual ratings.
So we're trying to pick factors
that when we multiply it all out gives us back your original ratings. Yeah, this is where it gets real.
Okay?
Okay? Again, what are the latent factors in practice? We don't know.
We can look at them. We discover them. We just apply some algorithm. It learns the factors after the fact, we can eyeball it and then say, Oh, this looks like, you know, popular kids, movies.
or edgy comedies
or crowd pleasing action movies. I don't know.
Okay. now.
how do we do this? Really?
How do we find latent factors? All we have in the Mini is your radius matrix.
That's what I did yesterday. Go into my Google form. And it gave me a spreadsheet. And it was just a bunch of names and movies and some 0 or some fives and fours and threes.
I got to turn that. I got to factorize it.
Define my leading factors, Craig. This is the this is the hard part. How do we do this? Okay, are you ready?
The same venture to end? Do you guys have class later today? If you do, you should even go. Let me explain why.
what
exams are just the man.
Free yourself.
I'm kidding. Take your exams. Do? Great.
Yeah. Your parents are gonna call me
my kids like failing out of college and stuff. They're like gonna graduate.
And then, like, they're not anymore. No, but
the.
But our first question is.
yeah, we're gonna finish. We're, I'm gonna lead you out today. But I hope with something that should be very inspiring to you.
Because if we can get through this. this is gonna carry you. This is the foundation for, like every modern big machine, learning, deep learning, large language model chat. Gp, all that stuff.
If you can understand how we do this latent factor business. you basically have the foundations for everything else.
Okay, like the specific formulas change. the specific derivations change. But the idea and the methods that we use
are absolutely foundational. And people do this every day. So if you were at Openai today. they would be talking about these topics. Okay.
school.
Now, how do we do this?
Let's go back and do a classic method called Svd. So you're no, we're not actually gonna do this
called singular value decomposition.
Okay, if you took linear algebra
say, Oh, look, if you give me a big matrix, I can decompose into these 3 submatures
we only have 2.
But imagine this. This one sort of gives us the projection down to K is 5. But the idea is
we can do this decomposition. Okay. so what I did is, I went and go. I ran Sd. on our ratings data. And can I show you what happened.
He gave me this.
Does anyone woo? Has anyone seen any problems?
What's that about
manoeuvre justice? By MF. Poll? 3, 7, one.
And then for the notebook, it was like a 3 something. Now, it's negative, 22, almost 0. Here's a 0. Almost.
Here's negatives.
What's that about?
Don't watch this movie. So what's happening? Traditional Svd. this method has been around forever. You give me ratings. Matrix.
don't worry about how it works. I apply. Svd, this is what you get. You know the what Svd assumes. It assumes. The empties
are really zeros. It's trying to say, oh, I think justice would have given the notebook is 0. I think you would give a Barbie a 0. So the point is traditional. Svd, not smart.
It treats the missings as zeroes.
So then, when you do the learning, it tries to learn factors that would predict 0,
which is madness, a 0. I said. It doesn't mean a 0. It means you just haven't seen it.
So the whole point of this is
Svd. Traditional Svd. Not designed for missing values. It treats the zeros as real ratings
example. Here again.
You know all those. all those
those you know. what are we saying? Treating 0 as 0, not as missing.
Okay? And so you can find it throughout this. So the point is, Svd does not work. So go back to the context of the networks prize
you want to get.
what are you guys doing? Monday signs of money? Hit that bag, shake that bag. get that shedder.
We're other.
Give me some Moola.
You want the 1 million dollars.
and you say I got a big matrix. I go try Svd. Go try it, and you will like a total dummy.
You learn a whole.
Don't know
why, because you're guessing zeros or negative numbers.
And Netflix is like, are you? What are you doing? You're guessing negatives for everybody.
You can't even rate negative.
But I wanna, II think you really hate it. So the point is, we did does not work. Okay. So then what happens?
This guy funk
function. Svd, that's a blog post. You can go read it. It's on our readings.
This is just a guy sitting in his house.
I think, in the Bay Area, and he's like, I know, that Std. Svd. Does not work.
Could I adapt it to basically ignore the missing rates?
So pick the Latin factors that basically
give me the guess. The closest to your known ratings make the smallest error. But we'll just ignore all the misgivings
right?
So this was funks. Id, so people typically call this funks svd. or you'll read all these papers in this time they talk about. Oh, I used Svd singular value decomposition. But they're not doing the original. Svd, they're doing this functified.
Svd, okay. funky. Okay?
So you guys remember our Mse.
you're like, I better know it cause that's gonna show up on the midterm gear deep. Gare gare a garon
guarantee
midterm guarantee or your money back
alright. I think it's the channel 6 explanation.
And we need
we just say. for this user, I guess 5.
They really gave it a 4.
There was, I was all by one. I squared, it went. And I sum that over every user. Every I
okay. so what we're gonna do is this, we're gonna do
over the non over the known ratings.
Let's speak clicks standard error.
You may have seen this in different contexts, something similar.
So let's yes question.
I guess, like
last time we chose. This was Netflix's objective. This is this is their metric correct?
Exactly. Okay. So this is the thing that the next surprise optimized on.
So if we do our in that. Why don't we optimize on the same thing? Because this is the case where
optimizing on Rmsc means winning a million bucks. Yes, sir. Okay. so let's say.
basically, go back to our matrix of all of your ratings.
Okay, I'm gonna assume. That's my full training set. And imagine there's some hidden ratings somewhere else. But don't worry about that. Okay.
it's it's here
I could do is I could basically say, Let me look for every user and every item
that would mean
I go back here, and so let me go. Look at my original thing. Where is it? Way way back here let me have to copy this.
Where is it?
All the way back? Here.
let's copy.
Let's
very least.
I said over all the known ratings. so I would say this just as he applies to shredding.
and suppose that my guess was
3.7. For example. this is not a good one. But suppose that was the example. So we would say, Okay.
3.7 is what I guessed, minus 5 square it some error. Then we go to the next rating and we say, well, justice gave B movie afford
that maybe our model guests 3.8. So we would say
we got 3.8. He gave a 4. We were pretty close, square, and pretty close. If you walk through every user item
where you compare the guess and the actual.
And so you agree? I hope you agree that we do that for everybody. We want this number to be big or small.
What's the dream that this number would be 0, the dream would be 0. We always guess exactly the rating you gave
in practice. That's hard to do. So. Instead, we just want to make it as small as possible. We want to minimize. We want to minimize. That sounds like optimization.
I know, like, I'm seeing a lot of jaws like this. What did you say that high 5. Can we do that? Yes, we can.
So here's what we're going to do.
Let's take our rmse. yeah. So you know this business.
Here's the other little trick. This is. You do a lot of these kinds of tricks.
Well, does the square root change this?
But making. We want to make it small, the square root being it like change, really
kind of scales it, but doesn't really change it. How about dividing by n. it scales it, but doesn't really change it. So in in practice, what we do is
we ignore all that business. Okay?
And we say, let's try to take just this business
copy.
What
stupid thing?
Copy
paste?
Okay.
what do we want to do here? We want to minimize something.
Remember, this is our hat.
Okay.
so
what do we choose like, what are, what is our algorithm drinking bar?
We said, we're trying to learn yesterday to greeting right? But where does the rating come from
like? Where does? What is our
r hat? Ui is equal to one. Where does it come from?
Usual?
No, this is our. This is our guess.
Remember, are you?
Our ui is actual grading
of you. Where does our hat come from?
That's the problem.
Let's go back to dot product.
Let's go way back here. Remember this.
I showed you justice, and I said, Where did our guest for justice come from?
Oh, I took justices, latent factors, and I dotted them with the items. Latent factors. And I get a number.
Yeah.
in in matrix world.
If this was a big P matrix, this is a Q matrix.
I just took the P vector and for that user and the Q, vector.
so where did it come from? It came from that vector, times, that vector, that's like, PU,
that's QI,
they're just vectors. What are the vectors?
That's to you. That's
QI, what is the vector, it's that vector, what is the vector, it's that. Vector.
so my whole point here is this
for this P, and Q,
remember, we don't know any of these numbers you were saying, where do the numbers come from? And I say, I just gave them to you.
But instead, we have to learn these numbers we have to. We have to figure out what these numbers should be.
And so I'm saying is, we want to pick the numbers that go in all of your
that minimize the error of my predicted rating versus the actual rating. That would be the the dream. Okay.
I could think about this. What if I put all of my random numbers in here.
and what the random numbers in here.
And so suppose I did that by P. And QP. And Q,
just keep the hand right there. So you get well, right?
Yeah. But like, if I just put in random, would I get a low?
So we want to do is PQ. What is RUI. It's just PU
times. QI.
So really, I'm saying is, I want to pick the P. And Q. Matrices. The maker's really small.
That's the dream.
Does this? Does this hurt your head or zombies?
You're taking your head and heard your head.
Your head is hurt. Did you have breakfast? She always had breakfast. very coffee dinner, did you?
Don't drink that stuff? I only drink stuff that's been on earth for over a thousand years. etc.
No, help me dead. The blind stuff!
My drinks, the coffins.
Doctor T.
Probably been around for that long.
I don't. I drink. Only drinking better is a hundred. Okay, I've been around for a hundred years.
No monster interview rates. Thank you.
I don't. I don't dream. Asian food reads
wines, beers, spirits.
Do I drink? Coke? 0.
No way. Okay.
okay. okay.
My feeling is this is challenging your brain a little bit. Okay.
what I want to. I'm trying to convince you it should not.
And if it does, it means you're weak.
All of the materials to understand this, which are
logically, and we'll do this on Monday logically. You know where every piece comes from.
I'm just, I'm tell you, do
so right now. It's a little Fuzzy, we will iron it out. What we're gonna do
is basically take this. We have objective function.
We want to make the best choice of latent factors. We gotta do optimization. Guess what that is.
That's gradient descent gradient descent is just finding a minimum.
What's a gradient.
It's just some partial derivatives.
So we're gonna show how to do this
simplified so that we can find the best P. And Q, which gives us our
factorize matrix. See you all on Monday have a great weekend gigaem.
etc., etc.

Start. Video.
I'm good.
Howdy, hey? Friends? Big weekend, huh? Good weekend. So let's give it up. You're gonna say, what? What? Let's give it up
by saying, I'm ready. What is wrong with my thing?
It's not. Let's give it Andrew and Ezra, are you? Are you here?
What what's your name, Ezra. Why am I calling you out? Because he nailed the score of the football game.
which is what? Plus 40 plus 42. However, Andrew's name is, first, because he actually got his end earlier than you did so, anyway, high 5, Andrew, wherever you are, out in
and let's give it up. Give it up.
also, previous. Quick quiz, I asked you, for, like you know, what are you looking forward to this semester? I have read all those comments, a lot of really interesting stuff. So I do read all of that. I'm not gonna do my more like like exciting data analysis, because, as you noticed, people could post after the game was already over, that was just me being a dumb dumb. I'll try to fix that in the future for something more interesting. I love the quick quizzes. Did it take you long to do? Yeah.
no. Were you stressed out about it?
No. Did you get them? All right? Maybe you don't know.
Okay, so don't worry about it. Just keep you going. Also homework. One is due next week. You'll notice the preamble was copied places from an old homework where you still use e campus, of course, on canvas, of course.
also remember to rename your submissions. So it makes our life easier when we grade it. So we don't get a whole bunch of solutions that say.
Cse. 4, 70 underscore homework. One underscore Spring 2023. Rename that your uid underscore homework, one dot whatever. Submit that it makes our life much easier on homework, 0 a bunch of people didn't do it, but we still gave you credit for it.
How about that? So please do that any questions concerns about this or anything else going on in the class.
I think.
in the top 25, maybe the first time in my will, hey? And it's unbelievable feeling. So. Anyway, I don't think we'll stay there. But you know who the coaches Duke is.
He's a former Aggie assistant, I believe. So. There's connections, people there are connections. Kill you. Okay?
Again. I don't know what's going on with my display here.
Okay? View settings seamless. No view settings.
something. Goofy's going on anyway. Big data day today.
So we're gonna go the smaller version. So anyway, our goal is to rank albums. Right? You come to cab's algorithms. You search. We now give you a ranked list of albums where we were last time was. And we're sending the foundations. We're gonna do vectorspace. We'll come back into probabilistic model. And then by the end of the week, maybe into next week we'll be talking about machine learning and doing other cool stuff. Today, we're gonna lock down vectorspace model. Very important.
It's key to homework one. So if you haven't gotten onto that, get onto that also on the homework. I posted this on slack. I'm not seeing a lot of activity there, but you are welcome to post like the outputs.
so don't post your solutions, but you can say like, Hey, like for this one. I parse all the the lyrics, and these are my top 5 tokens, and these are the counts.
and then everyone else will say, no, those are totally wrong or mine are different, and you guys will quickly, hopefully, I would love it if everyone kind of arrived at the exact same answer. Well, that makes sense.
So you're welcome to do that. And like on the part where you're doing ranking, and you have to generate scores, you can say things like, Hey, my top 5
albums are these 5? Here are the scores I got. What did you get?
People will say I didn't get anything like that. So then you can say, Well, here, my intermediate scores did these scores, and that way you can really like kind of sandy check with everybody else. That's totally fine. And, in fact, if you're one of the first people to post that kind of stuff because you've already solved something that gives you like extra special bonus in my in my mind.
Alright. So I'm kind of figuring out what who was a great participant, because there's also a risk right like, if you go first. You're like giving away the goods to the rest of the class. Okay, this is what I got. So to incentivize you to that, I make a special note in my little book next to your name.
Okay, cool.
Okay. So we're doing scoring is the basis of ranked retrieval. So we're trying to generate a score for every query document paired. That means when you go into Google. And you query something in theory. We're gonna score, every you know, one trillion documents.
Okay, now, in practice, we're not really gonna do that.
But that's kind of the mental model I want us to have. We have a query. We're gonna take all of our albums. We have 10,000 albums. We're gonna score every album. With respect to that query and give us a score. Okay? Then we can rank by the score
that gives us a ranking. We're happy. Okay? So where we talked about last time was Jerry Salton, father of Ir, introduced vector. Space model. I showed you some of the original work
I showed you this big picture. And the whole idea was that documents and queries are just vectors in a high dimensional vector space. And we went through this. And we figured out, what are the actions?
All of the
the stop. The words, yeah, yeah, these are just words. Those are just words
from our vocabulary.
So if we considered vocabulary size could be a million
or 100 k. Or whatever. But the point is, when we talk about a high dimensional space, we're not talking about a 3 dimensional space we're talking about like 100,000 dimensional space or a million dimensional space. Okay.
but the big idea here, this is pretty cool is to say, well, we can just create all of our documents
as a vectors. We can also treat our queries like vectors. And then we can do some kind of vector distances or vector similarity, or some sort of vector, math to compare those vectors to give us a score. Okay, so that's what we're gonna talk about today. We're gonna lock that down. So last time I gave you this example
we have a query, Hi! There!
We have 4 documents. Hi! Hide! Go aggies. Go go aggies. Hi, Hi! Hire! Yeah. I gave you a new document.
There aggies. Okay, we only have 4 words, Aggie. I go there, so we don't have 100,000 words
100,000 axes. We only have 4.
Okay, I use 4, because that way you can't just plot cause you can plot like the twod, or maybe even threed
4 DI don't know how to plot that.
Do you know how about that?
Maybe
maybe use some kind of time
travel something. Speaking of which this movie, the flash. It has a time, travel aspect to it. It's free for me on my hbo, Max app.
I watched it within 30 Si was like, oh, oh, this is terrible! I don't know if you like. Maybe you like that movie. But you know what I hate. I hate watched it.
I'm still watching it. I haven't finished it yet, but it's really really bad. anyway, the DC.
The and then the main actor is some kind of like a criminal right? And again, spoiler. It's like, well, he's a really annoying like character. And they're like in the movie. We're gonna give you 2 of them.
There's like one's even more annoying than the other one.
I don't understand. Okay, so we have 4 dimensions. Aggie's 5 over there.
We have queries in documents. Okay? And so if we were to think about this, we could in theory, plus this. So when I say plot like those 3 dimensions, would be XYZ,
you know, XYZ, you can somehow plot it. Now, we're in 4 dimensions. But it's still a vector, in a 4 dimensional space. Okay? And so now, what we want to do is once somehow compare queries to documents
right? And so like a natural idea is to use Euclidean distance
right? So Euclidean distance. So, for example, if we, I'm going to draw a version of this
which again, I can't really draw it. So I'm gonna draw like a 3 dimensional version. You can imagine. Like.
if I had this. Vector so suppose that was like A, I'm just gonna write document one. And suppose
this vector, was, let's say, document 2. And then I asked you like.
what is the the distance
between? Document one in document 2, are these 2 vectors? Well, it's just
it's that. That's the distance. Right?
That's the Euclidean distance between them. Okay?
And so we can use Euclidean distance immediately as a way to measure how far apart they are. Okay. So notice, sometimes we'll talk about distances. Sometimes we'll talk about similarities
or scores. It's way to be careful, like is higher, better or is lower, better.
and sometimes I'll kind of use them interchangeably, but like a Euclidean distance. what do we want for our highest ranked document? Big distance or little distance.
little little distance right the dream would be.
you know. Imagine my query was.
Here's my query.
And then we said, What is you know the distance of the query to d one. and the distance of the query to D 2,
this is, gonna be small.
This is maybe larger, because that distance is that and that distance. Is that
right? So the idea is, the query is closer to document one than this is, document 2. So it's a smaller distance. So that would be good. We'd rank it higher.
Okay? So
so you guys remember a Euclidean distance? Right? It's just the component-wise. Is this the square root of the sum of the component-wise differences. So if we come back here and we did like, what is the distance between the query and document one.
So let's go here and say, what is the
the distance between the query and document one?
So here's the query, here's option one. We just go component-wise and take the difference and square it. So with the difference, one square
difference. square it still one one one.
so one plus one plus one plus one is 4.
So we just did. We did the differences. squared it well, the difference is squared it. What's the difference that squared it? So in our case. It was we ended up getting I'm gonna write it out. Just so everyone's really clear with us. 0 minus one
squared. Plus sorry. Do this to you. one minus 2
squared, plus
what was the 0 minus one and then one minus 0
0 minus one squared plus one minus 0 squared
and we square root all of that, since that was just square root of 4, which was 2.
Okay, Wimo. Now we can sue do distance of the query, and let's say, document 2,
and we can do it for all of the documents. So we did a distance between the query and document 2. It's one. That's where someone remember that one
one squared also 1, 2 squared, 4, 5, 6,
7. So square root of 7. You who did this? That's square root of 7.
Now we can do the distance between the query and Doc. 3. We'll just go ahead and finish this out distance of the query.
and Doc 4. So let's do the query, dot 3 0 9 0
one minus 2, 2 squared, 4,
0, one.
5, 5
square root of 5, you can tell. I just click these numbers up, because otherwise, like on an exam. By the way, if we do this, it should all work out nicely square roots of 9
square root of 16. So if you're on an exam hint and you get like square root of 17, you probably screwed up. So check your math on the last one. First? 2. Okay, so 1, one
so square root of 2. Oh. so now we can induce a rank order. So what is the best rank.
1, 2.
So if we're gonna rank these documents for this query, what's the number one document? We returned off Doc 4, and then one, and then Doc 3, and then Doc.
2. Everyone buy this. This is the smallest. Doc, 4 and smaller hint stuff for is what we bring first. Okay? And then, whatever Doc 2 was, and the biggest is bring blast to.
Oh, that's fine.
Years ago I use Marker sometimes to make sure if I have a marker that I don't draw in here.
because I think these are my marks
from previous semesters. I do that sometimes. Whoops.
Does this make sense.
It's cool. So we're done. We said vector. Space model was, all we need is a way to represent our documents and queries. We can do that and a way to measure the distance.
Now, there's a problem with using Euclidean distance. And in fact, we don't do this.
Why might we not use Euclidean distance
problems.
Let me give you another example. So imagine you're back in our 3 dimensional space, because again, I can draw it and imagine there is a there is a query.
and we're gonna compare it to 2 different documents.
okay, so there's Doc, one
be another color.
Can you see that?
So which document is closer to the query in Euclidean space
in this euclidean distance, which one is closer to the period Euclidean, what do we do we measure
that distance, so that would be the distance, and then the other distance we measure is
that distance? So call that that that's the distance. The query to d one. That's the distance, the query to D 2.
And so it turns out, though like. But if you were to look at the words used. the idea is, this document is exactly the same words as the query, it just has, like more of them.
And so let me give you an example. So what was our query before? Hi, there!
So you can imagine, like Doc Tube could be something like.
Hi there! Hi! There! Hi! There! Hi! There! Hi! There. Hi, there. right? Whereas, Doc, one
is just some other words, you know other words.
So this idea that like what's happening is because of the repetition you're getting a much larger magnitude.
right? So when you do the Euclidean distance, it would be something like, you know, this is represented as like 1 1,
and this is represented as something like, you know.
whoops
5, 5. And when we take the distance between those, it looks like a really big distance, whereas if we compare some other documents much smaller. Does this make sense?
Okay? So that's why, in practice we don't use Euclidean distance. So we go back here. you know. No.
don't do this.
Okay, so don't do this. Don't use Euclidean. It's sort of like a straw man showing you an example. It makes sense. No, it doesn't work, not smart.
Okay? So the whole idea of the classic vector space model is we're going to again
take all of our queries and documents, put them into our high dimensional space. But now not use Euclidean distance, but use the cosine to instead measure the angle between the 2 vectors. Okay.
so instead, we're going to use the cosine. What's the idea? Idea?
We're gonna measure the angle
between the 2 vectors.
We're gonna measure the angle. So the idea would be. you know, something like that has a small angle
that has a larger angle. So this one is preferred to that one. Okay.
and back in the example we just gave, it would be like, what is the.
you know, if we're measuring that angle versus that angle, even though this vector
is like way longer.
The angle is, you know, like 0, or like very minimal. So like the angles are much closer. Okay?
So the idea is like whichever direction. You're pointing other things that point in that same direction. Ish should be ranked higher.
Okay? So in cosine, when the angle is 0, what is the cosine.
just like in trigonometry or geometry. Jump whenever you took that stuff one. And so it's high, and if they're perpendicular, or like, say, orthogonal
with the cosine there. 0.
So now we're doing similarities. So notice this says similarity, this is not a distance.
So we're going to rank now by like how similar you are to how dissimilar! So it's going to be higher. Scores are better, lower scores are worse.
So this is the whole point. I don't see you confused, because before we were doing distance lower is better. Now we're doing a similarity. They're like, kind of
2 sides of the same coin.
Okay. cool.
So let's use cosine.
Oh, the cosign between like a and b cosine is the measure of the angle. Is this vector dotted with that vector over this magnitude over that, Mac, you guys remember all this stuff.
You forget all this. Some of you forgot this.
Who remembers this? Who forgot it? Yeah, that's my people right there. Yeah, yeah, yeah, yeah. I mean, it looks familiar. Right? I mean, like, I've seen this kind of thing.
So we'll talk. We're gonna talk kind of philosophically about this. We'll do a practical example.
and then we'll talk about some like implementation details, and how you really want to do this. But what I want you to notice about is philosophically is like
A is just a vector and B is a vector and so the numerator is just the dot product. And we'll go. We'll go through the steps of that.
But I want you to notice is and we're dividing by the vector. Magnitudes of A and B,
that's the vector magnitude of a vector magnitude of B, but what you'll notice is. you could also kind of look at it as like.
it's kind of like that times that.
So a over the magnitude of a.
if you would know what that operation does. You know.
it projects this big vector down to a unit vector now, what is a unit? Vector it's a vector that has a magnitude of one. So what's happening here, this is like a normalization step.
And so in our Euclidean distance world, we had a vector and we had another vector
they were really far apart.
But if we divide them by their own magnitudes. We pull them back down to the same magnitude, and they live on the unit hypersphere.
Okay, so what's cool about this, let me see if I have a picture.
So back in our original example.
me, some colors here.
What happens is, imagine this is the unit hypersphere. So any any factor that lives on that has magnitude one. So all we're doing is saying, basically, like, if I have this, vector
and then I have. That's a terrible color. Be a better color.
And if I have, let's say that
when we do that normalization step.
this thing becomes this and this thing becomes this.
And so the point is, we're we're sort of getting rid of those magnitude differences and pushing everything down to the hyperspere. And then we're only operating on that. Okay? So the key step of cosign is, it has this normalization.
I will do an example. I will make it very concrete. So you have no no concerns. Okay. so
let's go back. Let's do a cosign. Let's go back and do.
I might have to copy this stuff. It's too too hard for me to follow. Let me find.
Hang on.
No, no, no, no, no! Have too much power
copy.
I don't have a piece of I need.
Let me insert a new page. Cool. Let me paste
cool.
Okay, cool. So
now, what we're gonna do is let me get a different color. Let's say, we're gonna do the cosign
between the query and Doc one.
So what do we need to do? We need to do the dot product between these 2 guys. And then we're going to divide by the vector magnitudes.
So the dot product. Remember, go back to the formula. It's just we walk along all of the elements. Remember all of the terms. and we just multiply the value in one times, the value in the other, and then we add them up.
So let's you, you guys wanted you me to copy that
oops wrong thing.
Let's copy that. Let's paste that
and pasting apparently nothing.
Let's try that again.
Why did that copy not sound? Slider
did. But it's on the slides. Sorry
we try one more time.
Copy.
Yeah. So second.
okay, so you can see that kind of it's okay.
So
let's do the cosine. Let's do the dot product of the 2 vectors. Okay, so we just walk along one to N, well, that's this, 1, 2, 3, 4. We just walk along the terms, the dimensions.
And let's multiply the value of the one factor times the value of the other. Vector, so it's just like, okay, fine 0 times, 1 0. Oh, it's a summation. So then, one times, 2,
2 0 times, 1 0, one times 0 0. Add them all up. And so we end up getting. Let's write it out 0 plus
one times 2. I'm gonna write it out so you can see it 0 times one plus one times 0. That was 0 times one
over something. And that's gonna end up being 2 over something. Everyone's cool with that.
Okay? Then we can see the vector magnitude of A and the vector magnitude of the B, how do we do that? We just walk along the in dimensions, and we sum the squares of those dimensions. So what is the magnitude of the query?
0 square plus one square plus 0 square plus one square, I'll write it out. So we have no concerns.
0 squared plus one squared. Make sure I'm doing this right cause. I'm I get kind of confused sometimes. and then we'll do the other one.
One squared plus c squared is one squared plus 0 squared.
one squared plus 2 squared plus one squared plus 0 squared. That's 2 over
square root of 2. What is that? 4, 5, 6.
Is that it? Is that right?
Okay, cool.
And you can simplify. And you can decide who cares? Okay? So this is, someone can calculate this out is some number.
Someone do that for me. By the way.
so this is gonna be some number.
Alright.
Yeah. we don't have any human calculators.
It's one spur. It's very rare.
which is just just give me a number
3. It's not. It's not very nice.
Do I have to do this
through, you know.
Thank you. 0 point 5 7 number done. You did it. Okay. so what is the similarity of the query in the document? Point 5 7.
Okay.
we can do this again for every other document.
shall we? What was our number one document last time.
The document 4. Let's do. Document 4. Okay.
one time, 0 0 times 1 0 times 0 one times 1 1.
So the cosine
of the query, and Doc. 4 is one divided by something. Well, we already know the magnitude of the query. That was this thing. We did it once already, which was square root of 2,
and the other one's gonna be square root of 2,
which is 2, which is 0 point.
Wait a minute. It's a little different. Oh, now, document, one is better than document 4.
Let's go back and remember what those
documents were. This query was high. There.
This one said Niagara's
Hi, Hi! Go!
And this document was
Aggie's there.
So somehow the first document is a little higher than the fourth document. Why, even that is.
it has 2 highs. And somehow that mattered.
let's do.
Let's do. Sorry we're gonna do is out of order, because now you're with me, I'm gonna go ahead and do Doc 2 and Doc 3, just so we can do a quick comparison. Compare and contrast. So the query and Doc 2
0. That's a 1 one that's 20, no. Sorry.
0 0 0 0. Oh, you do better. So it's 0 divided by something. Guess what? 0. And let's do. Doc, 3,
the doc, 3, 0 0
3, 3 0 0.
So this will be 3, divided by square root of 2. Because that's the vector magnitude of the query never changes, and Doc three's magnitude
is 3, which is one over root 2, which is what
I don't think I want to know. Point 7.
So we now have a different rank order in our Euclidean space.
We rank them deep. 4, 1, 3, 2. Okay. In our new world we rank them.
3, 1, 4, 2.
Okay.
So our our top dog is, Doc. 3. What's up with that? The query was.
Hide there, and our best document is high, high, high.
Oh, is that 3 is a lot, and it doesn't have any other words. It's only about saying Hi!
And the query was about saying, Hi, there! And so when we worked it out, it turns out the high High High is our best document.
Okay?
Whereas in our other world it was aggies. There was our best document. This is kind of a goofy example here. Well, it could be, I mean, who? Who knows? Exactly. Okay, we're gonna get at. It's like there's something different happening here. It's emphasizing more on the these commonalities.
but also taking into consideration the size and the other words that show up. That's the magnitude. Fifth. Now.
we did it
this way right? Alternatively.
we could normalize the vectors and then dot product them. Okay, that's when I was saying like.
Oh, like what we did was we did this one get a number divide by this number, divided by that number? Alternatively. we could sort of normalize the query, normalize
the document and then drop them. Okay? And they're equivalent. I'm not gonna do that.
But you could do that. Okay.
So one thing, a practical issue. I want you to think about right here.
which is
in order to do this in practice. What do you need to know?
Like, here comes the query.
you have a bunch of documents.
So the first thing we think about is like in the numerator.
What conditions have to hold in the numerator. For this to be like nonzero
austerity is the same.
You have to have something in common. The intersection needs to be not empty.
so a trick you can do, and it depends on the implementation. They're not saying, this is always how you do it.
But if you cause, you know, really, we know this is not 4 dimensions. It's hundreds of thousands, right?
So what you could do and some of you will do. And people historically have done is, they'll say, Okay, I got a vocabulary of 1,000.
Let me start with element one, and I'll walk across, and I will do the dot product. I'll multiply them, and then I'll add them up 0 times 100,000 wrong, and then you have a number, right?
So there'll be faces like. Which was the case here.
like Doc 2 is 0 0 0 0.
It's gonna happen
zeros all the way. And you're going to spend all this time. Okay, what's the next? Still, zeros still zeroes.
So one thing to keep in mind is
the numerator is only nonzero, if there are words in common.
And so again. There's lots of tricks to do this like, if you're using python, you can just do some set intersection. For example, you can do some sort of intersection, trick and figure out? Do they even have words in common?
So really you don't need to loop over one to n
just instead.
Look at the intersection.
Only the words that are in both of them. We'll add anything positive. If the word isn't one, but not the other. Is it still a 0.
Okay, so you only care about words actually in the intersection. So again, what's the intersection of like? V. 2. And and the query was not
right. What is the intersection of you know the query? And this one? It's only high.
Everything else is adding zeros to it. So you only need high to do the numerator.
Okay.
the other thing to think about is this, denominator never changes or excuse me. The components never change.
The magnitude of of you know the document
is fixed
so you could pre-calculate this.
You can pre-compute if you want to.
It never changes right. The stop in their teens.
and the query may come in and you don't know it.
and you have to calculate the magazines of the query.
But this never changes. Okay.
Questions concerns freak outs over any of this stuff.
Let me blow your mind.
Yeah. Yeah.
What if the query came in? And here's the the real query. The query comes in and says,
Hi aggies Aggie's rule.
Hi aggies Aggie's rule.
Okay? So we have. Here comes the query, Hi aggies aggies rule.
Aye. that means
who? Oh. what do we do? We have a word we've never seen before.
We have many options. What could we do if we got a query with a word we had never seen before. What's the easiest thing to do?
What's even easier than ignoring? Not good for your user, but easy moving.
You guys are smart. You're too smart. What's even dumber
Aggie? Error? I can't help you. Bro. Thank you. Thank you.
Hi! Aggies to Aggie's rule
can't help you. Bro. Can do it. You said something. I don't know what it is. I'm done. 0, no results.
Okay? And that's
again, we're
in theory. We're not really gonna stress test about homework one. You're just getting started. But that's like a classic edge case, right? It's like, Oh, we didn't mention it. But like I used your code, and I tried to query it, and you didn't know the word. And you just crashed.
Okay.
which most of you, if I hadn't said this, that would happen for most of you. Right? Because you hadn't thought about that
now, alternatively say, well, you could just ignore it right. The query was, what a Hi aggies aggies, rule will just treat it like, Hi aggies Agnes.
So this becomes. what is it like?
2, 1 0 0 and just write it like normal.
What else could you do?
You can add it to the dictionary, you can say, well, now, we've had a new word rules. It's one here and zeroes for everybody else.
That would change the magnitude of the query. Right? This would be a little different. The calculation be a little different. Okay.
different options. But this just keep this in mind, like in practice, you gotta think about stuff like that. Okay.
so this is vector space model with cosine.
That's it. That's it, however.
so weird. I don't know why this is
represent queries and documents and vectors. Each axis is a term we measure similar to using. Cosine.
higher is better, lower, is worse. That's it.
The key, though. Notice. the weights are so so important. So which weights have we been using? So
just? The term counts just the raw count. If a word occurs more.
I high high 3
aggies aggies 2,
we just count the number of times the word occurs. That's the weight.
Okay? What?
Remember, if we somehow had bad weights, we would get regardless. If we're using vector space and cosine bad weights, we would get bad results.
good ways, we get good results.
What am I getting at here? Well. imagine the query is a different, you know
there are some words that are more informative. There's some words that are less informative.
We talked before a lot about like the stuff. So imagine my query, was the aggies.
Okay? And I had a document which said
Da da, da.
and had another document that said Aggie's rule.
which one is more relevant.
Probably Aggie's rule, because it's talking about the thing we care about aggies. Whereas the other document, the talking about the nobody cares about the.
And so there's this idea that just using the simple counts is missing something right? It's not sort of capturing the informativeness of the words.
So this is when we talk about the vector space model what we really mean and what you're doing on your homework is typically
a documents
and queries are vectors. Each axis is the term measure similar to using the cosine
find weights
using tf-idf.
so the weighting system we're going to use is classically tf-idf. and which we'll talk about now. So this is when we talk about the vector space models. the vsms.
we typically mean that that
and that.
Now again, those are design choices. And there could be applications where you don't want to do it exactly this way. But this is classically what we mean, okay? And so basically, we know how to do this. We know how to do this right now. It's gonna work on the waiting scheme.
So
tf, ideaf classic waiting, you'll see it written sometimes. Tf, dash df star lowercase. Tf, dot
I
don't like. Put this on your cheat sheet and forget about today. And people will write it down as tf minus idf. And they're like, yeah, I get the tf, and I subtract the idf, no, this is just notation.
So it's just tf, dot ivf, it's just a don't. It's not a, it's not an actual operator. Okay, I say this. And yet every year someone will know why.
Click on my cheat sheet. Okay? So again, we think of vector space. We typically think of tf, idf weights plus cosine. Tf, term frequency. That's what we've already been doing. Count them up. Count the words
idea. Inverse talking to frequency. What?
So? Tf.
2 components
score each turn by the number of times it occurs. the more word occurs, the more valuable it is
you say aggies twice. Tf, is 2.
Seems more valuable.
Is that hard? No.
is that easy?
Yes, simple. It's what we're already doing. Just count them up.
Yeah. now in practice. And you're doing your homework. We do. We don't just do the the raw term frequency world-term frequency is just what is the term review for this term in this document.
and we'll write as Ftd, it's basically just count the number occurrences
of T in D,
that's it. Sometimes we'll do some logarithm scaling to it. There's lots of other variations.
This basically is saying.
well, if the document occurs,
one time.
And for let's say, we're gonna do log base 2 for simplicity. You can do whatever there's another trick we have to figure out
which base we're doing. But the idea here is saying, if a word occurred once. what is this
log base? 2 of one plus one log base, 2 of 2 is one.
So basically, we're saying, it's like that. If if the
sorry, let me erase that
if the F is one, this tf would be one if the F was 3.
The log is, 2 of 4 would be 2 if it was 7, 3
for dot dot dot, we're basically saying, it's like.
Yeah, if you say add means like a lot of times.
It counts for more. But we're just not going to give you all the extra bonus for saying it so many times. We're gonna kind of log in just to bring it down. We're still saying, the more you say the word, the more valuable it is. But we're trying to say you can't really spam us by just saying the word over and over again. We're gonna log it down.
that's all it's doing. If the intuition is the same. which is the more a term occurs, the more evaluable it's it's just we kind of don't let it
grow linearly. And
that's tf
ideaf, on the other hand, like, Yeah, tf, courtesy of this guy who worked at Ibm
Hans Peter Lewin.
Fun. Fact, I like to give you local fun facts. You own your credit card. When you were a kid, you tried to buy stuff with your parents credit card, and you typed in the number. And it was like this is not a real credit card. right? Because there's a checksum.
So he invented the checksum on your credit card so you can't just type in random numbers and buy stuff pretty smooth. It's a real person. 1950. S. Thinking about this.
what?
The idea, this is the other. This is sort of the rarity of the word. So Tf is within a document. Idf is across all of the documents. Ivf says, basically the rarer a term is the more informative, the more valuable it is.
Okay. So it's kind of in conflict with tf. Ts says within a document, the more you occur the more valuable you are so. In other words, if I have a hundred documents, this document talks a lot about the thing that's good.
Ivf says if I have 100 documents. And only this document talks about it. That's really good.
Okay.
So the point here is that, like a word like the that occurs across all of the documents is going to be non-informative, not very valuable.
a word that only occurs in a few documents much more informative, much more valuable.
This is due to. I'll give you an example, Karen Spark Jones, and in the invented this in the seventies, British Researcher. And now in the ir world there's like the Karen Spark Jones award for like a top top contributor to the field. Okay, so real person. She just passed away a few years ago.
also kind of put a human touch on all this stuff.
The New York Times ran an article about her in 2,019, when they were doing obituaries of people who had been overlooked at the time of their passing. So it turns out, you can go read this very fascinating article. Her husband
was like a reasonable computer science person. He got a big obituary.
He was just a regular dude.
She basically, you know, pioneered computer science, you know, foundational work that led to Google and Whatnot.
She didn't even get a mention.
Okay. So the late years later, it then came back and wrote a very nice picture. You read it all about her? Very cool.
yeah.
So anyway.
her idea back in the 70 S was this idf idea. And here it looks kind of crazy. But basically it says
the idea for a term and a document. And we're gonna just log version. This in is just the total number of dots.
Hello! Sorry I
I've lost my ability to write
very interesting.
Come back
alright.
love it! When that happens.
My, let's do this real quick.
let's get that
unplug that let's do that.
Come back here. Stop sharing. Let's redo this
lost the connection.
Now let's go back here.
Let's share screen. Let's start.
We're good. Let's come back. Let's go.
You seen and forgot my my pen.
Good search
7. It hasn't been
okay. Well, anyway, I've lost my PIN. That's weird. So the point here is this is someone right on their own notes, total number of documents.
So like, if we have a hundred documents in, is this a hundred?
And this is just basically how many documents have this term in it? Okay.
so for there's also a logo again. So basically like this.
Let's do. Log base 2. Again. Suppose I have of
256 documents, 206.
Your term occurs in all, 206 documents. the log of 2, 56, over 2, 56
long ways to it. One is 0.
So a word like the that occurs across every document is scored. 0,
there's no value. Let's do. I have 256 documents. and this word occurs in only 2 documents.
2, 56, over 2, 1, 28, with log base, 2 of 1, 28,
6. So, 7, 9, so, 7, 6, 9. Bro, this is
the. This is failable.
yeah, I can't ride. This is really annoying. I'm gonna use a marker on my ipad or on the screen. Oh, no, this is a this is a disaster.
okay? So I don't get to write it after this is the idea. Okay.
so what we're gonna end up doing is basically putting them together
where we just say we talk about Tfids score. If you find a tf, the term, the document and the Idf for the terms across all documents. Okay, so you end up giving. The weights are not just pure counts. They're going to be some combination of the count of the document and the rarity across all documents.
Okay, with that we are done. We'll see you on Friday. Good luck with your homework, Gigun, etc. Etc.

Increase in all.
It's only
oh, yeah.
Oh, yeah.
good.
It'll only make our bonds stronger together. We can uplift our backup. What's the name
John.
the football quarterback. I've heard of the game last week.
and then now they diagnosed him, whatever it was. Maybe it wasn't just the game, or to announce like the whole. That's all I know we'll overcome. I know it's tough. We'll make it
fun day today. Fun day. first questions, concerns, comments.
homework. One solution. We're gonna post soon.
Homework. 2.
Some students ask for the like, the vortex source.
So if you know what I'm talking about, you're welcome to use a Lotte editor to put in your answers. You don't have to.
You can just do your own.
However, whatever you want to do, and just submit your Pdf, okay,
I would suggest this on the homework. We say, show your work right? So you can get on slack, and you can see people are posting the answers right? It's like a mirror. They never had a class
that's great. But we want to see how you got to the answer on the homework. So give us a little structure, hey? Step one. I had to find this. Here's some calculations. Step 2. I had to find this. Here's some calculation. Step 3. Here's my final answer. Big circle done.
Okay, just and see your steps. You know you're communicating to us that you know what you're doing.
Any other questions. Yep. we won't go to.
So let's say you wrote a little function to do that for you.
you know, II can't imagine you wrote very much code. So put it in your solution.
But this is how cab. This is, how I did it. This is my function here it is, and this is the numbers they gave me.
Something just shows us that you. You know what you're doing. That's the big thing. It's all good.
but I don't want you to do is not show me the code and say, cab. I wrote some code here. The answers, I don't know what you're talking about
other concerns.
and just somehow take a screenshot, put the numbers, but somehow exposed to us.
What you're doing. you know what I mean. So like, hey? I use a spreadsheet. Here's the formula.
But here are the numbers they gave me. I'm cool.
Yeah, something like that is good. Alright. Any other questions. Not. Is that the homework about anything?
No. Self reflection. Yes, please. Today's. hey, Google, twenty-fifth birthday. Wow, wow, yeah, yeah. Yeah. Yeah.
So you know, separately, I have a Google hat.
and there's like the internal like they. Yesterday, I think yesterday they had a big 20 fifth anniversary, like they call it Tgif.
So historically on Fridays they used to have, like the whole company would get together, and it would depend announcements, but then they had to move it, and they kind of canceled it. But anyway, they had that, and they had a bunch of the people from the olden days talk about some of the key developments very exciting times 25 years. Wow!
Remember, like.
again, there was a while we didn't have this stuff.
Google wasn't a verb.
It was hard to find stuff so big ups.
let's give it up. Yeah, Google, 25 woop. Wot?
Very nice. Oh, also, yeah. Yeah. The T-shirts.
So if you're on the slack, there are some amazing designs already happening.
And so I'm getting close. We need a few more. And then we may put to a class vote. We're gonna refine it. It's happening. There are some really exciting swag. Yeah, yeah.
yeah. Let's what does this call like, let's bash it, you know. Let's okay, cool. Now
do a little bit of business here. So remember, we have a ratings matrix, right? We know how to do user. user, cf, we look at. we look at the rows. We find users that are like our users.
and then we have some tricks to figure out what are the items to recommend. Then we also talked about. Item, item. cf. or we look at columns same deal. We can figure out which items are like each other.
And then we have different techniques. I showed you some approaches where then we could fill in particular users, but we think their rating would be based on items like the item that we're interested in and doing that kind of inference so cool. This is all. Cf, we know how to do this.
to wrap all that stuff up the cool thing about all this. We only need those ratings. In other words, we can apply all of these methods to
any kind of item, movie song, album friend, job
news.
just the matrix. The set of the items are now those things. But we don't actually have to analyze the text. We don't have to listen to the song.
You don't have to look at the video or the image. It's just ratings. So that's really, really cool.
Same ideas work everywhere.
Okay, we will talk about content-based analysis.
maybe next week or a little thereafter. So that will come. We've already had people make some kind of nods towards that. Hey? Like, what if this album? This artist has like works in a particular style?
We can use that if you like. This artist, you can find other artists or other albums of the same style that would be content. Right? So we'll get there. And that's definitely a trick that we're gonna need to know. I just want to summarize with the big drawbacks. Okay, some of these we've kind of talked about already.
But like cold start. If I wanna launch my recommender accounts algorithms. Tomorrow. I have an abuser have no reading.
I have no content. I have no collaborator. I have no recommender. Can't do it. So like this matrix.
it's like, it's empty.
So what do we do? Right? So we've talked a lot about this. This is the key problem. We got to somehow. Bootstrap, the thing
and pure cf, can't do it.
Okay, number 2. And this is part of what we're gonna deal with today. And and next time is the sparsity, which is
that ratings matrix, you know. When I show it to you it looks fairly dense, but in practice it's super super sparse meaning a lot of times, you know, you may have like users or items, in which case it's hard to find neighbors there, just not enough ratings in common
can't do it. Okay, new items kind of like related to these. But basically, when new items show up, what do we do? And we have some ideas right? We know we expose them to people and sort of
rate this thing, you know, we try to and induce some ratings. We can do other tricks like content based. But basically in pure. Cf, we're kind of stuck.
And the last one is just sort of unique taste, which is, if you're a real like oddball
and kind of your ratings, distribution is really weird.
There may not be many close neighbors to you.
and so it's kinda hard to find. It's hard to find. Okay. cool.
big summary, you got all this stuff. These are classic midterm questions, hey? In collaborative filtering suffers from the
warm start problem.
You don't know, can, it's cold start?
Hmm.
big brain. Okay. okay. Now. where are we at at algorithms?
We're great. I mean, we have swag. Okay, we're gonna have swag.
And we're all gonna wear it like, yeah.
we're gonna be like this crew like rolling through campus. People are gonna back off. What are they from a high school visiting?
If someone says that to you
I was, gonna say, you have license to commit violence against them. You don't you don't. But you can tell them, no, actually, I'm in college.
This is my club. This is my class. We stick together.
Okay, we know how to do nonpersonalize recommendation most popular, most recent, that kind of stuff check.
We know how to do the baseline estimate. Remember, that was
that was that like VUI equals mu plus VU plus vi. We know how to do that kind of right.
We know how to do collaborative filtering user. Item, Ida.
we will come back to content-based in a little bit where we're going to focus on now, latent factor models. This is basically
this is like the modern foundation.
Okay? So the all the time we've done before we can definitely use it. And we'll inform what we do here.
But these latent factor models are like that kind of supercharged kind of modern recommendation. So we're willing to spend a lot of time really understanding this, because once you have that, all of these pieces can connect together, and you have kind of like the one model to rule them all.
Now
we're gonna do a little. I need your help.
So I need movies. So I asked on slack, for example movies, I'm going to ask you guys to rate these movies.
So this is what was suggested to me, so far, Shrek.
hey? Don't tell me
you guys have heard of Shrek? Yeah, you watch shrek all the shrek's.
Yes, that's why I have the notebook.
You guys know the notebook. Yeah, romantic. Okay, Barbie.
B-movie, you guys know. V, movie. Oh, my God, that's amazing. A,
can we put more ad on that is that still like acceptable these days?
I don't know the room.
And the room is like the really bad movie, right?
Like, Hi, Mark! Oh, Hi, Mark, you guys know the room, some of you do.
I need a few more movies. I need some movies, and I need them right now.
So go into your place. Think about movies that you think other people have seen, but maybe not. Everyone has seen okay. And also, maybe that not just polarized movies, you know. Don't give me shrek 2. Okay, or Shrek the third.
So who's got some move? I'm serious. I need some movies. Yup.
Well, we can do, Oppenheimer. Of course, that's kind of with Barbie. Okay, so we'll do. Oppenheimer. Give. Give me another one.
Hang on Polar Express, but they have the dead eyes. Right? I'm sorry there was one over here. The sponge Bob movie.
is there someone here who hasn't seen any of these? In which case recommend? Tell me a movie that you'd like to have up here or not, many of these. So I'm sorry you said the Sponge, Bob. It's like the movie something like this.
I need like 2 or 3 more. Yeah. Oh, a classic. Have you guys seen rear window?
Hitchcock. Jimmy Stewart, Grace Kelly. Roman birth?
Go see that who here is. See rear window?
Only a couple. So I'm not gonna do it, even though it is great.
Yup, you guys seen Forest gone.
He also like the matrix.
Okay, I need like, 2 more. That's unbelievable. I heard inception. Lord of the rings
whiplash! Have you guys seen whiplash? You guys seen whiplash. No interesting. So hang on. I'll do whiplash
inception. You.
Christopher Nolan, is genius. You guys in sessions like the greatest movie of all time.
You know, the top is spinning. It's really deep. You guys, it's good. It's good. There's one more over here Specific one.
We'll do fellowship. That's a lot of movies
extended addition. Only right? Yeah, yeah, I got the blue rays. Okay, cool. So I'm gonna right, when class ends or I'm gonna basically put all these into a Google form. I'm gonna send it out to you. And I'm gonna ask you to rate everything from one to 5.
You had to put your name. You don't put your real name, so if you want, don't want to put your name to it. You can say, you know
whoever you can put a fake name. It's fine.
and if you haven't seen anything, don't rate it. Okay. But I'm gonna ask you to rate everything one to 5.
Okay? And then next time I will have hopefully. If everything works, I will have done some cool stuff.
And we're gonna see all the these methods that we talk about today. And next time kind of come to life. Yup, I think I have some annotation in here. I say, yeah, something like.
it's the best to the worst.
okay.
all right, we good.
But hey, what do you guys do in class today?
Did you talk about movies? Not really. We just named for an hour. Name some more movies. Okay? Okay, cool. So
little bit of history lesson. And you're gonna see how all this stuff fits in. Okay, I showed this to you before. But now I'm gonna kind of come back and show you where we are. Okay. So basically.
whoops
predating. This is kind of like, the web is like 93, 94. The web is even in. We don't get Amazon to 95. So kind of predating even Amazon.
There were academic. There were universities working on basically the forerunners of modern recommenders. Okay.
Amazon launches. I'm going to show you this. It's gonna like
you're gonna love it. Today. We're gonna talk a lot about this Netflix prize 2,006 to 2,009. Okay, just to give you a context like these are like academic conferences. So what does that mean? The point is, it's not until 2,007 that they even existed. So before that it was more like in practice. People were doing it. But it wasn't like a coherent area where people were really kind of study. So it's fairly new, like, by way of contrast, like the first AI conferences were meeting in the 1950. S.
Okay, the first like database conferences like early 7 min.
So this is a fairly new area. Okay? And then later on, we'll get to all this stuff. That's kind of later in the semester. Okay?
but just to go back in time.
So, Xerox, Parc, this is like a industrial research lab at the time. But in 92 they were thinking about the precursor to kind of like the original ideas of collaborative filtering
and so they're saying, basically the context of email. I change the picture. This is great. This is
normally like the worker at actual desktop giving lots of minutes.
Okay? Then they say we could have distribution list like you sign up for the I'm interested in this topic, or I'm you know I'm part of this distribution list, and you only get mail from those lists, you still get flooded. Okay?
And they say, well, you could set up some kind of where you could put like, I'm interested, like in all the emails that come from Tammu.
But I only want the ones about jobs.
You put your own little filter on it. But I see you still get a ton of emails.
And so then our idea was
collaborative filtering. And you have a personal assistant. Bring you a little stack of only the things you want right? So just to give you a sense of like this was back in 92.
And at the time, you can see, they were basically trying to define like this language where you would say like. if
is inside coming to Joe, and it asked for. Yes, you had to, at least, Joe, and a name contains Bill.
So you're having kind of these rules that would somehow help you take advantage. And the idea is you can share these rules. And somehow it would allow you to do this sort of like smarter like finding what you wanted. Okay, in 94 Minnesota. And I guess with Mit, there's group lens. This is still around.
So if you Google for group lens, you'll find they have lots of great data, sets, lots of papers, a lot of big ideas. something similar. But here they have this idea. You have these distributed like news servers.
You're like, what is this?
But the idea is like, people were rating things so like these are like 5 stars, 3 stars. And this is more of when we think about traditional, collaborative filtering. It's like, Oh, this guy rated this thing. 3 stars, this one rated 5 stars, and these distributed servers would share information about who rated what? To then identify the things that you might like, okay, just to give you said stuff has been around for a while. Okay, 95. I told you, Amazon launches. This is
This is what Amazon look like in a netscape. I think a mosaic browser.
Anyone here use mosaic? No, I did. Yes.
and check out those kind of interesting.
the spotlight for this date. These are the books we love all over the Amazon. So prices spotlight moves every day, so please come off it. So what is this? An example
like you were to click into that
requirement?
It's like a non personalized recommendation based on. They curated it, and they changed it every day.
Okay? But they also say, here's the explore one. We make our personal notification service. We think it's very cool.
So even the very beginning, they're thinking about, how do we somehow narrow this down to your personal interest?
Now, a few years later, in 2,003, they published. Item item, cf, how they're doing. Item, item, cf.
and the reason I want to show you this and I didn't give you the I don't think I need the article to read, but you can find it. But II pulled out this little poll quote, okay, just to show you like, this is how Amazon is doing this, they said.
Okay, for every item.
for every customer who purchases it right for every item. I, too, also purchased by a customer, right?
Compute the similarity between i. One and I 2.
Oh, how do they compute the similarity? Oh, I don't know.
Each vector corresponds
to an item. We can use the cosine measure.
The vectors. M dimensions correspond to customers who have purchased that. That's exactly what we talked about.
I know way right? You can't believe it. Take over. Take over Amazon. I show you this just because
this is why
Amazon been around, for you know, 8, 9 years. You feel they're so far ahead of everyone.
And what are they doing?
And cosine columns?
That's it.
Just give you a sense like, not that fancy. Okay.
And you already know what was kind of, stated the art of like industry. Leader
back then. Okay, pretty cool.
Yes. Wait. Yes, wait. It's real. It's real. Okay. Now.
now we're focused on Netflix prize
2,006, 2,009. Anyone here heard of the Netflix prize
you have? Did you participate in the Netflix prize? No search?
Who did
and what was it? Was the context about Netflix? Price 2. Maybe
there was a privacy issue with Netflix Price, too. I don't remember if it was privacy. Or this
cool? Okay, cool, even better. So
this is a big, big deal. Okay.
big, big deal. This is like Earth shaking like industry shaking like major major stuff. All the stuff that comes out of here super super influential. Okay? So basically, Netflix at the time is not the Netflix, you know today. Netflix, at the time is mail a DVD,
okay, different business models, not streaming. Okay, mail a DVD,
so they're recommended to recall cinemax. And they basically opened up to worldwide competition. They say, if you beat it by 10%,
okay.
you win
1 million dollars.
Okay.
so what happened was teams formed all over the world. Right? So the idea is, Netflix maybe have some engineers, 10 to 20 engineers. How much do they pay? What is like the fully loaded cost of an engineer at Netflix. Would you guess
it's like their pay, plus the kind of the overhead, you know the building, the air conditioning, the health insurance.
I mean couple $100,000 at least, at least. So let's just take $200,000 to be like small
PIN engineers. It's 2 million bucks.
So if they had 20 engineers or even 30 engineers, and they cost even more. This is nothing. A 1 million bucks is nothing. Okay?
And the idea is, they induce literally thousands of researchers all over the world basically to work for free. Okay, this is cool. We can win a million bucks.
So you guys know the Kaggle competitions.
This is like kind of predating all of that. This is sort of one of the original like, let's get amateur people. Let's just get regular folks to engage in like big science. Very exciting. Okay?
So the impact of this one, it popularized these big crowdsource competitions. If you're familiar with Kaggle. It shared a big data set.
really, really big of real user item ratings which we didn't really have before. I mean, we had some. But this is really much larger and also sparked boatloads of technical innovations
that we're gonna get into. Now.
here's how it works. They give you train duck, a training and test. Right? They give you a hundred 1 million ratings
that's like, couldn't believe it. 100 million ratings. Okay. half a million users, 18,000 movies, okay. the testing data is the last few ratings of each user.
And the way they structured this is, you don't actually see the testing data. They hold it back
all over here. I cheat. The way this is structured is you could basically say, like, I'm gonna train my model, get my best model ever.
Then
I go make my guesses. I get it to Amazon. I'm sorry I get it to Netflix.
Netflix ever tells me
the actual ratings. They go calculate some aggregate number, and they come back and say, Here's your number.
Here's the metric.
and I can only look at the number and say, Well, it was better than it was yesterday. It was worse than it was yesterday. I don't exactly
where I made my mistakes. Okay, so the idea is very pure. You don't know what's going on. Okay?
So the setup is something like this.
big matrix users movies, hey?
Training. They held out a bunch of tests. You don't get to see it.
Okay?
And then we got to measure something. And so the little way they measured it is using something called rms, E.
This is how good they decided your recommendations were.
rmse root mean squared error.
So again, we say, predicted rating. Sometimes I'll write it as like
R. Hat Ui.
just to make it clear our hat. So it just says. I guess this user would give this item a 5 a fire.
They actually gave this this user actually gave this item a 3.
So 5, minus 3 is 2, 2 squared is 4.
If I only made one. Guess.
3, 4 is 2. So my rms, E was 2. Okay? And so it's like an error over. But all the guesses you make.
for example. remember, the actual
actual is like RUI. And my guess
is this R hat Ui also call it R hat.
So, for example, there was a movie, there was a 3. This is a one, a 5, a 5, and a 4. If my guesses were 4, 2,
4, 3, 3.
So I made 5 guesses right? And so Rmsd is basically saying, kind of how far off am I?
And so well, that would be 1, one squared, 1, 4, one. If I'm doing the squares 1, 2, 3, 7, 8,
so it would be. The square root of. I made 5 guesses.
of 8
is some number 1 point something that was my Rmse.
This make sense pharmacy.
So they basically said, no, we talked before about what is the thing we're optimizing for?
It's so important, because whatever you optimize, for you're sort of hoping it sort of corresponds to what anyone care about.
So let me push this back to you for a second.
What are some good things about art in this
for us to optimize, for if I'm Netflix, because, in other words, I'm saying is this, if my Rms. E was 0,
I always guess. Hey, matrix 5, I guess. 5. Say.
the room you said 3, I guess 3. I always nail it. Imagine my rms is always 0. That seems good.
So other. This is good. Right? Yeah. Can you make arguments? Why Rms is maybe not
what we really care about. But this horrendous flow of
all kinds of issues. Let the zoom right now.
Okay.
my rms is awesome.
0.
But when you go to Netflix.
you know it. You know, we talked before a little bit about like self presentation, you know.
Oh, yes, yeah. I wanna see this 4 in like art film. I give it 5.
Do I want to recommend it to me on Friday night?
Maybe not. Maybe I'm you know.
Might. We're growing down, and I want to watch.
What do I really give the room
parallel and give me like a 2?
But it's fun to watch.
Okay, so part of the issue here is Rms is a really great metric, because it does optimize, for you know
the kind of the error in your guesses.
But remember, at the end of the day it's not the numbers we care about? 5, 4, 3 twos and ones. It's like, what do we really show to our our users?
Okay, so keep that in mind, this is a good metric, but it may not be like the perfect one. Okay, in any event, we're going to use it
now. So here's the competition.
Can you improve on Netflix match by 10? They currently have Rms point 9 5 1 4. Okay? So they're about. They're about a point off right on average, you know, like
you wanted you, you think it's a 4. They guess 3. They're about 1 point off.
literally, thousands of teams compete.
Next time we'll talk about the big conclusion about what happened. Okay, thousands of teams can beat. Now let me show you to blow your mind
if I just guessed the global average.
Not that far. Like. In other words, I always get the global average of like 3.5.
What do I think you're gonna give it 3.5. I always get 3
to me. It's like, that's not that far. If I only guess your F,
your average is 4
matrix. 4. Barbie, 4. Oppenheimer, 4. Your average is 3.
Matrix. 3, Bardi, 3, Oppenheimer, 3. If I do that, I do better than the mobile.
And you say, well, what about the movie average
matrix average 4.2, you 4.2, you 4.2. Everyone gets 4.2.
Barbie global average Barbie movie average 4.5.
I guess you'd give it in 4.5. So on and so forth.
So to me, what is like, very like a kind of damning. About. All of this is Cinematch is the product of years, and
dozens. If not. I don't know 100 engineers.
and they barely beat.
Have you tried taking the average to me? That's like totally shocking. I don't know how you feel
totally shocking, so like what that tells me
if you were going to launch something from scratch.
Just take the item. Average buttons
don't hire anybody.
do.
thought a Bg or something. That's a tip.
Okay? And you're you're almost there. Now, this is a big engineer solution of the brain.
Then you say, wait a minute. What if we did this? Our basic collaborative already talked about.
You paint cinematch.
And in fact, that's what people did in the very early days. They took all the data. They tried all the stuff you know already.
hey, we already know about collaborative filtering. Take some of those formulas, try them. and you're very medium.
And however, however. we gotta get an 85 to win the the big money so still a long way to go. So a couple things to look at this would be sort of depressed.
You say, my collaborate filtering like all that fancy stuff.
it's better than what they've got. But it's so far from the grand prize. We'll never get there.
Okay. The way the thing was structured is, I think it was going to be open for 5 years.
Right? People made such fast advances, you know it ended within 3 years.
Okay? And each year, as they went along, they paid out progress prices.
So even no one had gotten to the Grand fries you had. You know, you had chopped into here $50,000 direct
$1,000. So they're still paying out. getting closer and closer. Okay?
So the big insight to get there. the winning approach.
And what's motivating us? Is it combined
stuff we already know
what stuff we gotta learn about? So what is the stuff we already know. So first of all, global.
look at the average user rating, the average item rating. So do like a baseline estimate. So this is basically like
baseline
Star Wars is rated higher than average. Eva rates movies lower than average.
Take those factors into consideration to get your kind of global perspective.
Okay, local focus on how individual movies relate to each other. That sounds like collaborative filter. Oh, the Empire strikes back is like star wars.
Think about the column space.
So if you like this one, maybe you would like that. One
seems cool.
But in all the discussion we've done so far that you already know these 2. What's missing is kind of somewhere in between.
which is you? You can think about it like as a genres or groups, or as we'll call it like latent factors for sort of regional like considerations. So like
we had no notion of like sci-fi fantasy.
Oh, you like you like sci-fi fantasy movies. Here's other sci-fi fantasy movies.
even the even if the individual movies are not alike
or horror.
But it's not just like these are like predefined. You can imagine, like, Oh, saw, the movie is labeled Horror.
A Nightmare on Elm Street is labeled Horror.
a Haunting. I don't know what movies you guys watch a haunting on
some street horror. But it's not just that. So the point is, you can also discover like good movies for a Saturday night.
So there is no like tag on the movie. It's just through the actual like, user behavior. We find these movies that all kind of go together, even if they don't really connect on like a genre or content level.
Does this make sense
cool? Cool? So this. this is going to be our discussion of what we call latent factors. or sometimes we'll call it. we'll do. Matrix
factorization.
exclamation, exclamation, exclamation. This is the hot stuff.
Okay?
Okay?
Latent factors, hidden factors. How do we get the latent factors? We're getting something called matrix factorization.
If you can crack this. that
okay, once you have, this is to get there, we're gonna have to understand matrices. Vectors.
we're gonna do.
We're going to take gradients. Yeah, yeah, all that. All that stuff is coming back. We're gonna do optimize it. We're gonna do the whole bit.
Okay, in such a way, though I think everyone will understand it, even if you forgot all that stuff. Okay at the end of the day. Once we have this, we're golden. Now, one of the readings I gave you.
Maybe you made it flip through, and you saw. That's a kind of blog post from like the 2,000. And like, what is that about?
Well, we're gonna get there. Which is the cool thing about the Netflix Prize
is who's working on it? People like you people like me cause randomness. It's a random guide
was like, Hey, what if we did this matrix factorization thing, and he just posted on his blog about his ideas.
and they work so well. Everyone else copied it and used it. And indeed, he was not one of the people who won the Netflix prize, but his people who took his ideas I mean.
they're well known ideas, but he connected them to the Netflix Prize in sort of an elegant way. It's just a Rando, did it. People took it. They went off. Get paid.
Okay?
Now.
here it is. This is what we're trying to get to.
Have you seen this before?
Not the specific figure. It's like, yeah. What is this?
A minesweeper.
Have you seen this in the context of this class?
Yeah, that's it's a matrix.
Did I tell you, when I was in grad school
there was an Italian lady in in my grad school.
and you know she called this. She did not say matrix, you know, she said.
called it the matrix. So then whenever I think about the movie.
oh, did you do you like the markets?
This sounds much cooler matrix.
You got the matrix. No man.
Okay. So have you seen this?
But of course, yeah. users. items, very experienced brief. We did this before we do this every time.
Okay.
matrix
users. Hi, this retain
matrix user. I I made it much larger. Okay, we have end users in items
in the Netflix Prize. How many users were there?
I just told you guys that. And I've already forgotten
480,000 users. So let's say, 480 and 18 K movies.
So this is like
480,000. And this is like 18,000.
Okay, so it's a big matrix
cool.
So the idea of matrix factorization is also this thing is sparse, right?
And so that's like a 5. That's a 3. Here's a one, a 5. These are just rates.
Okay. the argument in matrix factorization is
somehow there are these hidden factors, or they call them latent factors.
And in this case they're saying that there are 5. So they said, K. Is equal to 5.
And so in the context of movies, these latent factors, let's just presume what they are.
Latent factors are. Let's say we'll just make them up action.
Or, you know.
sci-fi
romance and other.
So kind of the idea is, we don't know those names, but we kind of presume all movies fit into one of those 5.
Okay, if that was true, we could take this big matrix.
You break it into 2
too dense matrices.
And I want you to notice the size of these things. So
how many users is this?
Thank you. 480,000.
How many items do we have?
18,000? But now we only have, let's say, 5 latent factors.
So if you think about just like, how many numbers do we have if we have to record
18,000 times 480,000. Let me do that and get at my auto calculator.
18,000 times 480 1,000.
That's
what is that? That's 8,640 million
kind of entries.
Okay. now, I said, you know now 5 by 480,005 times 480
1,000 plus
5 times.
Oh, crap!
I can't do. I can't do calculations here times 5 plus 13,000 times 5.
So instead, we have 2,000,490
1,000 call them entries.
So the first thing is.
we went from 8 billion.
3.5 million. So it's much smaller.
Okay. we went from a really sparse big matrix. You can see data
big, a big matrix
to 2 much smaller matrices. Relatively speaking, you're like, let's look 2 million.
But it's not 8 billion. okay, much smaller, but dense. And so what do I mean by dense?
I mean, here, there are lots of like empties.
Right? This user? Are they seeing all 18,000 movies at Netflix? No, they seem, maybe, I guess, 500 maybe 1,800.
So 17,900 are empty.
But here, we're saying, is.
each user is now just a 5 dimensional. Vector.
and each item is just a 5 dimensional. Vector. these are just vectors.
Let's go back to. I'll explain what's going on. Just a second. Let's go back to our album world tabs algorithms.
Okay.
we're going to think about the latent factors as corresponding to
these different genres. So on the user side, imagine we're back in our like album space, this user has 5
latent factors. They kind of correspond to different kinds of genre.
It means this guy likes British pop a lot. But it's not like metal very much. It's much smaller.
Okay?
On the item side. it's like. this item is mainly British pop. And it's not really metal at all.
Okay.
does that make sense kind of?
Yeah, this is sick. This is gonna be cool. It's all gonna come to life on Friday
after you give me your ratings for these freaking movies. I asked you about?
Yeah. because then we're going to discover
your latent factors.
Mine. Yes, yours.
You may not even know what your latent factors are.
but we will discover them together. Now
go back here.
The idea that we hear them.
this user let's just make an example. User
Joe.
If this is Joe.
Joe gave a one a 3,
a 5, a 4, a one, a 3. Joe rated some stuff.
Okay.
if we come over here. That's Joe.
Now, Joe is like, you know, 2.3 1.4. There's a bunch of numbers in there.
Okay, the idea is, we're kind of going from this very
sparse super long vector of Joe.
And we're kind of compressing it down to
5 dimensions, that kind of encode. Who Joe, is
for those of you who took a deep learning course. Maybe he'll like.
But these days everyone talks about
we call these like dense representation. or you may have heard this word embedding.
and if you haven't heard it now you have.
And so this is like the
the lingua franca of these days. This is the language. If you're talking to someone working in this space. And you said, Oh, Youtube. And you said, cool. And you said.
Do you have a good user embedded?
Can you say, do you mean a dense representation of the user that captures their interest and preferences? It would say, Duh, you'd say, well, cool, high 5. I'm with you?
Or if you look at a column, you say, Oh, do you have good item embeddings? Good dense representation of the items?
And it would say, Yeah, yeah.
And then a secondary question might be, W. How did you find these embeddings, these representations?
And they could tell you some fancy method.
you can say, well, did you try? I don't know
matrix factorization. And they would say. Course you did say, cool, I know what's up, too. Yeah.
Post a word. Representation with features.
Yes, so what's interesting about it? Very good, very good, very good. So
we could. Are these latent factors? Are they latent features?
So? Yes, we can. Very good point. Very good point.
A subtlety is oftentimes, when we think about features.
we think about kind of human divine feature. Like
number of views. Today feature
counts of the use of this word feature.
These are, derived features. Some algorithm will find them.
We may have to interpret them. So every time I wrote down like Paul.
or Horror, that was me after the fact like saying, that's what that looks like
that. I didn't. Priority defined it. So I would say, Yeah, if you call it like, oh, these are the latent features. Everyone would know what you're talking about.
But if someone said, well, you mean you define these features and say, in the middle, they fell out of my algorithm that found these things. I when I inspect the features I could put labels on them
for my human brain, but a priori I don't know what they are.
Very, very good, very subtle, very, very good.
This all connects back to all the stuff we've already been talking about features representations. Exactly.
Okay.
But the dream is today. Right now, this is just get the vibes. Okay. next time I'll do example, we'll walk through it all. The dream is this.
wherever this number is here. 5, the dreamers in the latent factor space. If I put this vector.
and I dotted it with this, vector.
he would recover that number.
So we're gonna say, Joe likes this. If this is our example, like a British pop like this user here, it's like Joe, that's British pop. This album is British pop. They multiply the big number.
Joe doesn't really like metal. This album is not really no, we multiply them to hold number.
We add them all up. We get a guess. So the idea here we're saying. you know, this user likes British pop and hip, hop. this album is British pop and hip, hop.
So guess what
if I got the user latent, vector the I/O latent. Vector
I get out the estimated rating.
I'm gonna leave you with this. This thought, this is gonna blow your mind. And it's gonna require you to come back on Friday
because you will not sleep tonight. You will be thinking about this.
You will be thinking about this.
Here's the cool thing. I haven't told you how we find Mike. That's hard. Okay. But assuming we did.
the dream is this.
if I had these latent factors, which are dense.
I can now turn around. See? Suppose I have all this? I can now multiply or do my dial
this user time. This item gives me the guest of what that user would get to that item. Correct.
So this user does. This item would tell me
that
this user time, that item, to tell me that.
and I can walk all the way down and fill this all the way in.
So what kind of matrix is this? Is it sparse? No, brother, it's dense.
it's dense
meaning.
If by some miracle I had these factors. I can fill in this entire matrix
meaning I can make a guess for every user and every item. What rating they would give
know, I know right.
See you all on Friday. Bye-bye.

I will.
Howdy, random stuff so.
But I like is that we have kind of a prototype right here can show. The this is for different clubs and agen album clubs. But it's kind of minimalist record.
You see that. So
yeah, that's the challenge we consider that that's like the Beta version, we gotta do better. Okay, we're gonna continue our discussion of collaborative filtering. And as we always do. Are there any questions, concerns, comments.
worries.
homework. 2 is out. Of course
we have. What a midterm is it next week? Yeah. midterm is end of next week. Well, time really flies. So I remember talking about the midterm was gonna be weeks away.
But now it's just next week.
We'll talk more about the midterm, probably like on Friday, I think. but this is a preview. Remember, you do have your one cheat sheet.
which means you can write like any notes you want front and back formulas, example problems. You can do all of that stuff.
the style. I still need to work it out. But historically, there may be like a little true, false.
There may be a little multiple choice, but I feel like you're in college. We shouldn't really do that, even though it's easy to grade. But usually I do some kind of like short answer.
There'll be some calculation questions there. Maybe it'd be a harder like kind of. I call it like a synthesis question.
Where you had to make it. Put some concepts together in a new way. Would you have I posted. And or would you like to have sample exams? Yeah, you would. So let me post
let me post and sample
exams. And I'll post those. If you're willing to work them out, you can post solutions on on slack, and you talk about it.
There will be questions I gave you from all previous classes on topics we haven't covered, so don't be the person who says, Oh, I need to know this topic, and never ever talked about
but you can also pull that and say, Well, I never read about that topic or came to class. So you can ask me about it.
We? We did that really
cool any other concerns, questions, comments again.
Are we in a good mood
so rainy? Cooler? We like it. We beat Auburn.
That's good right. Who do we play next
South Carolina? That's here, Arkansas.
You don't even know. What am I asking a bunch of computer science majors about the football game.
You don't even know. okay. so what we're gonna do today is basically finish up clatter filters. All the stuff you need to know. Basically for the homework
right now, you can already do like 70% of the homework. Here's the last little bit. Okay.
Wednesday and Friday, and probably Monday. We're gonna move on. Talk about the Netflix prize and matrix factorization, latent factor models, stochastic gradient descent. All of these like heavy hitters. Now, the issue is.
you may be saying to yourself, self
those concepts are very difficult. They're coming right for the right.
So typically, what will happen is on the midterm. I'm gonna hit you with things. The hard questions are going to be on the things that I know. You've seen
tf ideas cosines
all that stuff show.
And on the final, of course. just in terms of like, how do you structure your your studying and whatnot? Okay.
user, user? Cf.
we started this last time
I spent a lot of time going. If you know this, there's this website called the Noun Project.
where you can get really nice icons.
So if you pay them like a few bucks a month, you can get these nice icons. That's how I got all this stuff. I didn't make that.
but I had to go find it.
But the idea is, we have this old hippie. We want to find people like the hippie, and so these are the other icons on the noun project that I think or like, in response to the query, Hippie! And so like, Here's this guy, here's this lady. Oh, here's you know.
person.
And the idea is they're somehow alike our primary user. And so if we could figure out stuff they like, we can recommend it to our old guy. Okay.
Now.
where we left last time was, how do we find those users?
Right? So we said, we can define some kind of similarity score in, like the vector representation of the user space, like the rows of our matrix.
And then we can go find the nearest neighbors to our user. Okay?
And so question, we kind of where we left it last time, which is, what's the right way to measure similarity. We said, we can do Jeff hard. We can do cosign. We could do a Co. Like Pearson correlation, and we kind of leave it as dunno
Duneau. but we can always use our our offline
like, train test data
to find a good choice.
Right? So we had this. Presumably we've played some offline data. We can go use it. We need, of course, metrics, and we'll talk about that probably next time the particular metrics. But the idea is we can go use our offline to try and figure out what's the best choice? Then we go deploy it.
Same thing like, how do we choose? KK. Is like, how many nearest neighbors do I consider. Is it just the closest person to our old guy? Is it the 3 closest people.
whatever? Whatever same deal? Okay, same deal.
We can use our online train test validation data. Try to pick a good K, then we deploy it. And we hope we do a good job. Okay? So we're not gonna do the cosign because you know how to do the cosign. But the whole point here is we're trying to figure out like the row based
similarity, the act Bar Vba similarity. We can use the cosine
right? So just to be clear here, I can say, like, what is the similarity of Akbar to Bb, 8.
Well, it's 5 times 4 plus 4 times 4 plus 0 times one plus dot dot dot dot all that stuff over the vector magnitudes of Akbar and Dv, 8. At the end of the day, you get something right? So we're just doing our regular cosines. But we're doing on rows.
Okay? And so if we did that, and I went ahead and calculated it for you.
So if we're trying to figure out who's closest to Akbar, we may say, well, Vba is 0 point 8 9. It's very similar.
right? Whereas is point 2 2 much less similar. And we have these other users who are kind of in between Darf and and the Emperor, and if you just eyeball it, it kind of makes sense. Because if you look at Akbar mediate like, they have a lot of things in common, right? And you look at Akbar, and
they've only rated anything in common, and the rate even those ratings themselves are not that close?
So it kind of makes sense that, like Vba is closest, chewy is farther apart, and the other 2 are kind of in the middle. Okay? And so the point here is like we've used cosign. But we don't have to. We could have used Pearson, we could have used jet something else. Okay.
But now we have the most similar users to Akbar. So now.
okay, how do we find the users. Great, we did. Now, how do we figure out what you recommend? So what I want to turn it back to you is, say.
I'll just show it here. Given
these similarity metrics.
I want you to bring some, some, some possibilities on how we should use these similarities to make recommendations back to Backbar. In other words.
what are we going to recommend to Akbar? And how?
Okay? And I want a lot of ideas here.
And so
you guys want to talk amongst yourself, and you want to sit there
doing.
Are we doing
tired?
Want to sit back with your friends, for?
Let's do that. Let's say 2 min.
etc.,
you know. Talk to your neighbors together. If you don't know your name.
Yeah.
So semester.
Would you guys like your name?
How about this?
would you guys be okay if we all were named tags on campus? You're all like.
okay.
Joe, what's up? Yeah. Yeah.
Okay. So here's the scenario. We have a matrix and rigor and practice. It's much bigger, right? It's not just the 5 4 neighbors. It's like thousands and thousands. And we have lots and lots of items. And you're trying to think about ways to find good items to recommend to actbar based on this user user collaborative filtering idea. Number one is that one
find very similar
people
and then find highly rated movies. So you want to find, let's do it like this fine. I'm gonna make it most similar
user. So that would be Kn
where K is equal to one. And then do what? Okay? So we're gonna pick the highest rated movie from from that. And we're only gonna do. K is equal to one. So right now, you can say, Bd, 8,
and you'd say it's only some movie. Sorry it's Alan's class or exile. You would recommend exile. Okay, pick a highest rated album.
In this case it would be exile. Okay, great. Give me another approach, we can do
another item.
yeah, similar to that. But like, so you start out where you kind of go through everybody like. So you bring them. So you don't have the similar areas for
similarity. So yes, let's do a kind of a weighted average. Let's do like that.
Let's do a weighted average rating
of all of all neighbors.
something like this. So you would say, like, Okay, so for we only have akbar has only not seen these 3 or not heard these 3, so we would do 2.8 9 times this plus 1.4 9 times this point 5 3 times this, divided by the sum of that, so like a weighted average rating of everybody, so the more similar you are, gave a lower rating kind of counts more than the farther away people.
Okay, that's an idea so weighted average. In fact, let me just sort of do that really quick. So like if we did that for just for the clash.
we would say
because he hasn't seen it. So it'd be like point 8 9 times one plus
point 4 9 times 2 plus point 5 3 times 3, all over 29, plus point 4 9 plus point 5 3, which is something.
Okay. And we could do something similar for exile for 20 items this and get a score. Okay? And what you're doing there is not just considering the most similar. You're considering everybody.
But you're saying that people who are closer to me kind of count. More people who are farther away count less, but they still they still count for something.
Okay? Idea. Number 3.
Different idea. Who's got something?
I mean.
you might be a nice person.
Breakfast one. No, I didn't have breakfast
awesome. Yeah. Okay, so
you add a little similarities together and make
digital header.
And then you get the relative or even of each of these 7. That right? And blue
people themselves abortion.
you pick one of the people.
Yeah. So you're gonna do some kind of, I'm gonna call some. I'm just gonna not gonna write all down some weighted random sampling
idea, something like this. So you don't want it to be, I think sort of the static is always the same number, but instead, he'll sort of you'll sort of randomly pick the closer people, more likely, and you'll pick their higher rated albums, more likely, and that's how you'll pick the thing to recommend something along those lines. I love it. We can do that. How about this?
what are you looking around right now, we're saying like, I like people like
like a different way to look at is, say. I like things that people not like me don't like.
So that's the same idea that you can say I'm gonna go take Chewy, who's the farthest from me. and find the thing that he likes the least. So. for okay, so he hasn't seen the clash.
Oh, he's he's he's. I'm sorry he's heard exile and funeral. He like exile more than funeral, so we would say, funeral
is the one we should recommend, because it's the person farthest away from me and the album they like the least.
It's an idea. Do you guys know these albums? By the way.
yeah.
so this is the Beatles. This is Bruce Springsteen, the flash. This is the doors.
rolling stones. Arcade fire. Yes.
I I've never listened to it, but I was like I was like albums that begin with F.
Not explicit albums that begin with F.
It's very hard. Okay. we can also do so. So we said, not not this. We could say, pick the like. Disliked
albums
of people not like me
something like this. And you can also do variations of these. So the first one, we said, pick the most similar user where we could take the most similar to users.
and then take the average of their scores or so on and so forth. Okay. my point is, there's sort of a big design space in how we go from purely finding these similarities to actually making the recommendation. Okay?
And indeed, a lot of these cases we've given like averages weighted average, and I think alluded to this before. But you could also replace the averages with like we could do Maxes. We could do average we could do. Min.
and I want to talk sort of philosophically just for a moment.
Suppose we had many meetings. It's hard to do on this figure, but I suppose we have many, many users, many, many.
and I go around my neighborhood if people like me.
and I think all of their ratings for the for these different albums. And we said, We're gonna guess the Max is the store to recommend to me
versus what we've been doing is the average versus the minute.
So when you, Phil, something, what does that mean to be like a Max versus a min. So this group of people, they're all like me in some sense.
and I take their Max rating as the rating of the thing
versus this group is very much like me.
but it's actually their minimum rating, as the rating to recommend is to be.
what does that? What that mean? I can practice.
Okay.
do you have free like this? Who you asked me, where should we eat?
I mean, like U.S.A. 5 for it. Where do we?
There's 1 one person you guys try this place. It's a little bit easy.
5 gotta go.
You're gonna go try it versus the degree than the min.
hey? Where should we go?
So you can do this other sorry
other restaurant, and the minimum is only a 2,
and you give it a shot versus the one
is able to make a difference. You care
Matson's versus.
Think about it for your own experience.
You do the Max
versus, but I'm taking it from other people.
right? So what I'm saying here is like the the Max. It's kind of like you're kind of like optimistic, I think. right. The average rating is a 3. But you have one friend who says it's going to change your life.
So you kind of have to. You're like, Hey, I'm gonna give it a shot. The min is more like the average is a 3.
The men is the one avoided the average of the 3. The minimum is a 3 kind regret minimization. You're like, well, it's not gonna be too bad. but it may not be great.
so I don't want to go any deeper into that. But I just want you to think about it like, I think, a lot of times certain mechanically. Think about like, Oh, it's plug in averages, or or maxims, or mittens, but like it really affects the experience people have
in terms of like, if you're being sort of optimistic, you're being pessimistic. You're sort of like being a regret minimizer
or not. Okay. Anyway.
you need to do this in the homework. We need to get this approach.
which no one's for whatever reason came up with. Okay, but it's very similar to what we already been talking about. It says, Here, this is again.
This is like estimated
reading
for user U on item I,
that's the average rating
by you. So we basically say, like, what rating do we think you're gonna give? I mean, you're gonna give the average rating to this new thing, this new album. You've never heard it. But you're pretty optimistic. So you're probably gonna give it a 4.
Okay?
And then we do is move it around by how much your
how much other folks being around you. So we're doing here is we're looking all these other use in a neighborhood around me.
So this is like.
these are users near me.
And so you have to define how big is this neighborhood? Okay?
And what we're doing is we're basically saying, here's the similarity of me to that other user. So this is like a weighted, we just did this. This is like a weighted average. This is the similar to me.
And then we basically multiply it, not by their actual rating, but how much their rating deviates from my rating on average.
Okay.
we can do an example in a second.
Okay? But in practice this is like, sort of empirically. This is like a reasonable approach. Okay? And on the homework you get to play around and see what this does.
So, for example, when we talk about this neighborhood set, I could say, the neighborhood set is just K is equal to one. So like the neighborhood set
is just bb, 8, it's just the most similar neighbor to me. So what we're gonna do here is we're gonna say, like, what is the rating that Akbar would give to
the clash?
We got to take Akbar out average rating, which is, what do we say? That is 4 and a third.
Okay, plus.
we're gonna say, well, what is the similarity to Akbar? And whoever's in the neighborhood set. Bb, 8.
That's a 29
times.
What is
Bb, eight's rating of that item? So Vba gave it a one.
hey? And then, we say, move it around by. In this case, what is the similarity of the user to the other end user, which is point 8 9.
So I don't know. I gotta go back and look at how they do this formula, because I think
that may be mistake. I'll check on that just a second. the idea here, we're moving it around by this one minus 4 and a third. So minus 3 and a third time, 29. So we think we're gonna get very low rating. Yeah, that was our
or you can apply.
Sorry? Sorry. This is that is the average rating that that user gives. So we're trying to do is basically say it's like.
Are they rating relatively low for them on average.
or above what they do on average. So sorry? Good point. So we said, Vba, on average is 8 9 plus 6 15, divided by 5. So on average, there are 3.
So we're saying, if they give a one. They're about 2 lower than they normally are.
Okay. And so we're gonna end up doing here is basically saying. Yeah, so our standard deviation.
basically, it's trying to say it's like, Are you above or below? And so, even if your rating is lower than my rating, if it's above your normal rating, it's considered like a positive.
Okay. And even if your rating is above mine, but it's lower than your average rating, you're gonna demote it a little bit. Okay.
And so in this case, and I'll double check. III
I think this is kind of oh, sorry. Sorry. I think you're doing that it's like this is, this is this case, I'm sorry. This is yeah, it's not a, it's not a count. I believe this is just an absolute value. In this case.
I believe this is just absolute value.
But the idea here is it'd be 4 and a third
minus 2. So we would guess 2 and a third.
Right? So we said, my average rating is 4 and a third. A vba is much lower than me by super within himself by 2. So we're going to demote mine by 2, 2 and a third.
Now, by the same token, we could consider the neighborhood to be everybody right? So we're doing like a weighted average of everybody. So the same thing again, here would be
4 and a third. But now we would say, Okay, what is point 8 9, and we said, what do we say? He was?
So he was one minus 3. But then we would also consider chewy
right, and we would say, What is Chewy? Chewy's average rating? But Chewy didn't review. Sorry Chewy did not rate the clash.
so they don't consider Chewy. He's gone.
But we do consider Darth. So Darth is a point 4 9, and then we also consider Emperor.
And so then we say what is sorry, what is Darth's average rating. So Garth, average reading is a 4, 8, 1215, divided by works out
3.
And what is Emperor's average reading?
9 0 t012-13-1415, divided by 4.
And then we say for Darth, what did darf give to the clash? A 2 and a 3, and then you divide all that by using a weighted average of the deviations.
And that is some number. Okay.
But again, the intuition here is, we're saying, you basically, you.
You fix it on Aquar's average. But I'm saying again, you may be above, ever average or below average.
We kind of ground you there and then we move you around by the deviations of everyone else who's rated the thing
weighted by their similarity to you. It's like a weighted deviation.
So forth. Okay.
so this is in practice. One way you can do this. Okay. Now, again, you could substitute out this cosine for Pearson.
You get a different similarity. Number fine. You still plug it in here and you go.
Okay.
that's all user user. Cf, that's it. You got it. Okay? Question, how do we find users who are like me do the similarities score you figure that out?
Choose a value for K. Is it one neighbor, 3 neighbors, all the neighbors?
Second part. Once I've done that, aggregate them somehow. Find the items to recommend.
You could do this formula. We just tried. You could take the Max, the men, whatever apply it. Now, you get ratings done.
Okay. this is user, user, cf.
core core stuff. In practice, this gives you basically, you know, I was talking about the history of all this stuff. It's basically to like
through thousands.
Okay
long time ago
idea, however, you'll see
later, this week, we talk about matrix factorization.
We still are going to use the fundamental concept of
what do other people think about the things? And how am I related to other people?
But we're gonna change the way we measure similarities away from
Cosine and Pearson and this aggregation to more sophisticated methods. Okay?
But the intuition
still the same still used today in practice every day. Questions.
yeah.
so in practice, like, say, we're in Netflix or whatever we're gonna be calculating this.
each question for each individual user interface.
Yeah. So in practice, it's gonna motivate. Some of the stuff. While we may not use all time is practically a lot of big challenges here, what happens? You have the new users when we? What happens? We have lots of new ratings.
am I? If I have a million users, I'm doing like a million by 1 million
similarity calculations reach back into stuff like crazy. So in practice, no, you're probably not gonna do that building tricks. Okay, you may try to find like local neighborhoods or cluster users, or do something to do some sort of rather doing like me versus all other 1 million. You do some kind of rough analysis like me versus these 100 other people.
Okay?
Or in practice, we may sort of break from these assumptions altogether, and not look at it by row. Look at it by column. So we'll motivate item kind of directly out of that question. So to wrap up user user cf.
things, we do need to think about
new ratings come in for user.
We have a whole new user, a whole new row.
Or we have a whole new item. A new item shows up, has no ratings. so there's not like an answer for any of these. But I want you to think about it so like.
suppose we have a new user. Come in. go back here.
go back here. So imagine here comes Fred.
Here comes Fred. Fred has nothing.
Fred is not near anybody. Fred is invisible.
So what do we do for Fred? And seriously, practice like we have a new user. What do we ignore them?
Would you, in order. Let me get what kind of recommendation
most popular generic stuff, whatever
it's like, you're going like, go you can try this like, go incognito. Go to Amazon. and it's like we don't know who you are. We don't need that browser. You're not logged in.
and you're like, here's some women's clothes. Here's some albums to stream. Here's some workout. But it's like we're gonna be random stuff.
That's sort of popular amongst different groups. And as you click into. We're gonna quickly learn a lot about you.
Okay.
we don't know. We don't know.
What else can we do for Freda?
Gain order.
We can give Fred the generic stuff we can remove from the app.
Yeah, we can ignore. Give kind of generic
Rex. We could try to like to bootstrap them in somehow. And that could be you show them this diverse set of if we're on CAD algorithms. we'll figure out what are some diverse sets of kinds of albums.
Frank sinatra.
you'll get metal current pop. etc., etc., and to show different areas and try to Bootstrap in them.
Yeah, maybe explicitly ask them, hey, when you join Kaz algorithms, you have like a little pop up. Which is which of these 5 things. Do you like click, click, click, click, click, click, and we kind of bootstrap them in. And now we can start giving you some recommendations. Okay, it's a big challenge, how about new items? So we have a new item. It's a new column.
So we're on. Yeah, what's a g album?
An album that starts with the letter. G.
No. What about gangsters? Paradise?
That's Coolio.
our rip coulio? But here comes games, Paradise. It's a new album, and we don't have any ratings for it? How can W. We ever recommend it to anybody? What do we do?
Thoughts, maybe do some inference based on? Does this artist have other outlooks so sure we could. That's called we could exploit.
some content. based features.
Bg, who is the artist?
So this is an artist you already like we could do if we go back here.
Sorry. Where's yep. Imagine we have gangsters, Paradise!
And here we could do some kind of oh, this is like some other album based on the content, and I could try to do some initial inference of what I think the scores might be.
Oh, Akbar really liked
another
like 90 S. Rapper album. This doesn't show up here. I could maybe do some basic inference and assume he may like this one as well. Okay. But the point is, it doesn't fall out of just like using this formula.
because, again.
there are no ratings. No other user had a rating of this particular item. You can't do it.
Okay. so we've got to be careful.
Be careful.
Okay. And the same thing with like, if we get a new user or excuse me, an existing user gets new ratings. We may have to recalculate some stuff. And do we do that every time? Do we do that in batch?
Another question. Okay? Because in practice, hopefully, our users are getting a lot a lot of new ratings all the time, but probably don't want to have like, constant recalculation.
Okay.
that's as user user. Okay.
we put it around. We could also consider it. Item.
what's that all about excuse
and strange
in practice, the item items often has been more popular in practice. So item, item is, basically, you know, we've been doing this.
What if, instead, we did this.
we said, Oh, what items are like other items?
Okay?
And then we tried to figure out, like.
you know, for example, these items are like each other. So you can try to figure out this rating here. That's missing.
like the because you watch, it's kind of like, because you watch this, you may like to watch that. So it's centered on items.
Okay. this is exactly the idea.
And so you say, why do we want to do this. Okay? So some of this is a little bit of a historical artifact. But, like. you know, in the olden days like this point just came up computing similarities between all pairs of users. Expensive
like. We don't want to do that
user profile changing all the time. We don't want to recompute the whole model. Okay.
you know, in practice that like, if we had a few ratings and many items, and you know, empirically, they had kind of bad performance.
And kind of the idea here is that basically like
this is not always true. Now, right? But the idea is, if you have
more users and items. Items kind of converge to some sort of rating distribution faster than like a user. Usually the user is kind of more sparse, but the items are fewer items, many users rating them. You kind of have a good idea of what the items are based on the ratings.
So the ideas are. Item, average ratings isn't gonna change as much to be more stable. So sort of in practice, this is like a very good idea, hey? Let's don't worry about users. Let's worry about items. Okay.
do I have the did I put in here? Want to show you something?
I don't have it on here. I'll show you next time, basically
like in our little history. basically, I think in 2,003, they're very famous. Amazon. Publish their item item vibrate.
And so this is how we do. Item item provider in practice.
very, very influential big companies. How we do it. Everyone got very, very excited. Right?
cool. So here we go. Same old matrix. same old mule. Very challenging for this class.
The vectors defined horizontally. Now we have vectors to find vertically.
Oh, how do we compare vectors?
And and
I don't know. And if you say I don't know. we have a midterm next week you'll be okay. But you gotta study. So how do we do it? We can. Cosign.
Wow! I see some faces like, really, what's this? I've never heard of this concept. What
notice? Similarity? Now, item, I item J, item, I, inspector, item J, vector it's still cosines. It's the same old stuff
I know. But now we're doing it. Column space. So again, we would say, like, How similar are these 2 albums? Similarity Abbey Road born to run? I'm not gonna do it but 5 times 4 plus
4 times 4 plus 0 times 3 plus 2 times 0 plus through time, 0 over square root of
5 squared plus 4 squared plus 0 squared, plus 2 squared plus 3 squared.
Swear it off
4 squared plus 4 squared, plus 3 squared is something.
and we can do that for all the
the columns. Okay? And you see the argument here, which is.
if a new user comes on.
who cares? We always seem to carry items like one user doesn't really change it because we have lots of users. We have 100,000 users.
So they have a boatload of rating. So like any one user or 2 users doesn't change much.
So we could still always find item similarities. Okay.
we still have the problem, which is, what do we have a radio?
Well.
we go. Say, well, let's just exploit the content. What else is like this thing? Is it from the same artist, the same genre, or whatever we have other tricks.
But this is an idea. There's a question in practice.
So let's say you have 100,000 users, and then you have, like a new 10,000 a month that come in. Would you just recalculate this like once a month?
A new 10,000? What users users like at some point. You just.
It's a one person doesn't make a big difference. But as a lot of this stuff, it depends on the dynamics of the particular system like. You know, if you're ingesting users every month, and they're like the previous users. And if you, if you're barely mature, these albums. These album rates may not change much.
Okay, but we do big outreach. So we launch CAD algorithms here at the end. Right? And so we collect all these ratings. And we're doing a great job. We have a good model of these outs. Okay? Then we take it and we launch it. At Uc. San Diego.
okay? And the average student used in San Diego kind of different musical tastes than we do.
So that month we're ingesting all these new users who are fundamentally like have a different rating distribution. Then we're in big trouble.
right? We better better think about that.
What's the correct answer in that
you don't expand to uses.
Build your own.
It's a come to Chili fest.
Has anyone here been to Chilefest for any of these things? I don't know any of these performers. By the way.
I see these billboards, and it's like so and so featuring so and so
also Guest Star so and so. and I don't know any of
sorry.
Well, I'll love it. I know I'll love it. you know all of it.
Here's an egg right?
Okay.
He's also he has a good role. Aggie, a singer songwriter, married to Julie Roberts for a while famous actor, also an actor himself in a few movies.
Check that out. okay. so
we know how to do this. Now, how do we estimate the rating for the item?
Same deal. This is saying, like this user, this item.
Okay. now we say. for all of the items like this item. So we're doing some nearest neighbors.
So for all those day items in this set, how similar are those items in this item? And then what rating did this user give
to those items?
Does it make sense? So, in other words, for
I don't want an example, and you find I don't have a good picture. Yup. So in this case it's like saying. You know. Abbey Road. Okay, so like for that bar. You know, what reading will Akbar give to exile?
So we say, this is the question mark. Okay? So we would say, for exile. What are the other albums like? Exile?
So in this column space.
well, are you alive? Are you alive?
It's hard for me to do this feed, so I would say. a 3 and 3 and 4 and a 4. So these are quite similar.
Right now, there's some other ratings.
But the idea is okay, this item, let's say, it's like this item at bar like this island. We think they'll also like this item, because the items themselves are similar.
Okay.
what's neat about this and what I want you to always remember when we do any of this collaborative filtering
is what's cool about is we don't really care what the stuff is, what the content is.
So we don't have to analyze the content or say, Oh, this is hip! Hop! You also, like the other hip-hop album.
The point is, we're saying, here's a note. this, the people who like this item. like as other things even it is totally unlike the person.
So long as they have similar rating distribution, they're alike. So people who love this one we equal like that. even they have nothing in common. It would make no sense, we'd say, why would you recommend this album with this album? It makes no sense.
For whatever reason people who have this were also like graphic
the minute
cool.
I was gonna try to make a Taylor swift joke. She's at the football game yesterday. right? She's at some football game.
I have no jokes.
and I don't want to cross the swift. Their wrath is powerful.
okay.
items. items.
So I won't get to this into this same deal here. There's a fancier formula.
Okay.
that's the baseline estimate. We know how to do that already.
Okay.
this is, we know how to do all this kind of stuff. I mean, this is on the homework. I wanna make sure we finish this. So suppose we've done all this work. Okay.
how do we figure which actual items to recommend. So what I'm saying right now is, we can use this formula. okay? And we can fill in all these readings.
And if you want to, we can be done, and just say whatever the highest rating is, recommend that right? So
right? So you know we could. You know, option. One is, just find
the highest
rated rated item.
right?
So. but think about this. So like. suppose our user has already created a bunch of albums.
Okay, so 5 stars, 5 stars, 5 stars.
like one idea we could do. We could go do. Item id, cf, on each album to go find the other items that are like it.
So what I'm saying is, we can do kind of
2 approaches, one, we could just do find the highest rated item using our formulas.
using our formulas alternatively, we could say, what items has my user liked? He liked these 3 things. go find things most like them.
Okay? And what you'll notice is, there's some repeats.
and some of these already rated. Now you have a bunch of items that are like those items. and then we have to then turn that into some ordered recommendation list.
Right? I know you gave 5 stars to this, this and this. These are things like those things. We get a bunch of items, and out of that set we'll now make a recommendation.
So how would you turn this into a ordered list?
We're based on the
so we could do by, you know, by the
we have to aggregate the ranks so like. Think about this so like
that sound was one here and a 2 here, right? So that counts for more. So somehow do some rank aggregation
so it could be sketches of Spain as third
here. So we know it's gonna be low going to a third once. whereas let's have with a one and a 2,
so you can do some kind of what's called rank aggregation. We're not going to go over how to do it. But the idea is you could try to figure out like
Okay, highway 61 got a 2. So if you think about it like, this is a one and a 2, this is a 2. This is a 2.
This is a 3. This is a one. This is a 3, and so there are different tricks you could do. But you might say, like
this used to be this one. It's the first that had the most. or it could be. This would be the first that has the highest average ranking.
Okay, but there are lots of tricks that you can do to then turn that into your final recommendation list. Okay, I leave that for something to think about. Now.
notice
now, our Cavs algorithm supports new feature.
When you view album. we can always do. Item item Cf, to find the items most like that album. And let's sit right next to it. independent of the user.
We just say, these are, item like this item, you might want to watch it or listen to it. Done. Okay.
so
kind of race through all, this main thing is you've got user. User. Cf, you've got. Item, item, cf.
you get all you need
intellectually, you have all of the foundation you need to. Now do all of the modern
our recommendation stuff.
In other words, we're gonna start working on now, doing our machine learning.
neat learning, and all of our fancy methods, but all of the ideas and intuitions you have
cool. Yeah, good to see you all. Wednesday. Good, loving your homework.
You haven't started begging you please.

He's like, No.
Hey? Howdy?
I'm hearing from the part of the people, that come to class, so basically you're saying, like 80% of the classes is not gonna where.
So someone needs to help me do this because I don't know how to do it. But I need help finding where to source this stuff and and who who's gonna design it?
Someone. This class is going to design it. So I'll post on slack about this whoops I'll post on slack. We need a design.
We also need a source
like where we're gonna get it. Come. And you know, if it's 1150 or something there about per person. We have around 90 people. It's a big number. But I would. I would pay for it.
Okay.
okay, maybe we'll do it for the final exam or something. Okay? So that's that's the happy stuff. The sad stuff homework 2 is out. You've been waiting for it. Yeah, that's amazing. So
here it is, it's a Pdf. okay? And it's a bunch of problems.
Yeah, and it's a bunch of problems.
Okay? And so basically, what you got to do is
it's the kind of homework that you know, you could do it all with a calculator invite in
if you wanted to write a little script to help you do it. Want to do a little spreadsheet to help you do it? But at the end of the day. You need to show us your thinking and your steps.
and I would prefer it to be as you have to read this. You guys don't read this stuff, I know, but you should read it. I want it ideally. You should type it up and submit your own Pdf. Back to me. If you want to handwrite, you can just make it super clear.
But basically like this first question is like, it's like a ratio. So give you some points. Do ratio do? Pnn.
okay? So it's the kind of questions that you couldn't do this level of detail on a midterm. But if you can knock this out you will be ready for the midterm. Okay.
so please get started. One of the key points I do want to make, because you may not look at this.
I was right, this weekend, the last. These are all calculation questions, the last one we're going to do an SEO contest.
Okay?
So right now, if I go onto Google and I search in quotes has algorithms.
there are no results
validation. So in other words, it doesn't know anything about tabs algorithms. So this last bit of the homework is you need to basically create a web page, a target web page that you aim to be ranked first on Google or Bing for that query, Tav's algorithms? Okay, so for this homework, all you have to do is tell us, your URL, but tell us the page that you're trying to promote.
And you guys tell us what your strategy is to try to get it ranked highly. Okay. The only reason I mentioned this is because by the time you submit your homework it may not even be showing up if I search for it. Okay? But later in the year.
okay, more like towards Thanksgiving Christmas. You should start to see some of your pages
popping to the top, and we'll make it more of a fun contest to see who's first, second, third, and fourth question.
The question is, can I pay? You know, through some mechanism.
So to get credit, you don't have to pay
to win.
brother, but the way it is is like it's not. I'm not designing it. So that, like.
you know, money bags rolls in here and just can buy it. But the idea is.
here you go. We're doing SEO on the some work.
So right in your wheelhouse, we're just talking about this. So
you don't know what's going on about this. Get on slack. We can talk about it.
But I will say historically, people have done some very clever things.
And so, for example, like, if someone wanted to, you know, I don't know by the domain.
Oh, excuse me.
but you don't have to.
If you're like this is some dumb class project. Why would I spend like 10 bucks to buy a domain? I would never do that. Don't you? Don't need to. Okay. But anyway, that's the homework. Check it out. Make sure
you're looking at it. It's fun. Okay.
we got a lot to get through today. This is gonna help you also on this homework. So homework you could probably right at this moment, there's like, with 5 or 6 questions, you can do 2 or 3 of them out of the bag. But there's several. We're not really gonna cover till today. And then Monday. Okay, I'll post the slides for Monday this weekend. So if you wanted to get ahead of things, you'll have all the resources. Okay, we started recommendation. We talked about this last time
our traditional capabilities. Now, we're moving to a model where we don't have a user query.
And we're gonna start adding recommender systems. And so I said, Look, here's this history. And he said, Blah, blah blah!
And then I said, to create our first recommendation method, and you guys gave me a bunch of ideas. Right? We said, well, we could take the freshest, the most popular, the whatever.
And we can begin to do recommendation.
Okay? So these are all non personalized. Right? They're not custom for each person
when you go onto Netflix, your page on Netflix is very different from my page.
Right? If you're on Youtube. Your experience is very different from my experience
in theory. If you're on like a job site being something very different from someone who is like at a different level in their career, okay? Or in a different area altogether. So we know that these methods they're nice to kind of bootstrap the process we're aiming for. So we're gonna be talking about this week next week, probably the next week
is always sort of foundational methods and algorithms personalized recommendations. Okay.
so then
where we left it on Wednesday was we had introduced ratings.
And that's when I had fun coloring stuff in.
Have you guys ever done like a like a grown up coloring book
that that became a thing for a while. It's it's very exciting. But you know, if you go like, you guys don't go to bookstores, but if you go into a bookstore they would sell
like coloring books. but for like not Kitty coloring books, but, like, you know, these like very extravagant, like detailed things. Maybe you've done it. I don't do it. I'll tell you what. That was a great size.
I highly enjoyed that
So where we left it was we lived in this world. Now where, let's say we do have ratings. Okay?
And imagine, after some time our users are waiting things right? And so I'm just filling you. This person said, add, your road is a 5, but that exile is only a one. This person says 5, 3, 3. Also.
you know, this is babies
lot of work to do that. This is the kind of thing you'll see on the midterm. So if you were doing stuff, I would never want you to say something like, I need to find the similarity between user A and Abby Road. You would just say, like SIM user A and a, you know, Abcd, make it simple. In fact, if I wouldn't call it Abcd, I call it like user MNOP. Q. So it'd be really simple. Anyway.
we start getting a bunch of greetings in this thing, and you can imagine over time we fill this thing out right?
And so if you think about Amazon to Amazon, you have all these users and all those products with ratings. So they have a matrix like this, Netflix has a matrix like this, and so forth.
Again, how big is the matrix? How many users do you have
a large amount? More than 6, 5, or 6, more than 80, more than 100 could be millions.
Okay, how many items do you have
again, depends on what we're talking about here. But a lot. If you're Youtube, how many videos are on Youtube
a lot every day, there are millions of videos being uploaded every day. So but in practice
we're gonna always call these users. We're gonna call these items
regardless of what the item is. Is it a song? Is it an album? Is it a job? Lead the way? We'll think about it is we'll call those. The main thing is this, like always, this is sparse.
And this is quite large. Okay? So in practice, that's what's gonna happen. Okay? But imagine we have now ratings.
Okay, now I turn back to you and say, Given the rings.
How can we populate our recommendations?
The one list to remove them all? Nonperson still. But now I'm going to read this.
Want to find them and bring them. Yes, correct
for the high school.
the highest we could do the highest.
So here's a little tricky thing, because already you're thinking, okay.
So we could do like, you know, the highest.
highest, let's say, average rating.
Would you distinguish between Let's say A and B. This has a 5,
you know this has 1 5 stars. This has
a thousand 5 stars. They both have the same average rating. Are they different?
So how would you? Is there another way to populate the top cable. So you're not promoting a bunch of like single rating things.
We could use a wait. We could do a minimum so we could threshold it. We could say.
Highest average rating minimum, 100 ratings. the highest average right? Something like that.
It's not an exact way to do it. There's just a way that you could avoid putting those things there. Okay, give me another way to populate the top K list. That doesn't consider the average rating, or even the Median rating
number of ratings. Yeah, okay, so number of ratings.
The most popular by number of ratings, even if they're all low, right? And sometimes
whatever it's not basically just unique.
so bad. It's good.
Bye.
Sharpen it.
Great example. Has anyone here seen Sharkneto?
Have you seen sharknado? 2.
Have you seen Charlotte? O, 3.
Is there a 3? No, only 2.
yeah. So kind of so bad so good, so bad. It's good. An oddity. Recently there was some online phenomenon of some guy who was doing like eightys
euro pop songs. And if you solve this it was very bad.
but it was kind of yeah, but it was the idea. Was it so bad? Just kind of a novelty? You might find it interesting. So just keep that in mind.
Another thing you think about is ratings in just a second. Hang on. So
yeah, we can do all this. We're still non personalized. Okay, now, here's one quick question. How do we elicit ratings? I asked you last time how many people have bought something from Amazon? Almost everybody. How many had rated it?
3. Right? How many people have? You know, whatever rated something on that voice? Very
so. Here's a question.
how have you experienced this, and how might we elicit? It's hard to do ratings from people right?
We can pay them.
Yes.
Brandon Nguyen
this may not be scalable, it may not work.
but certainly especially for our startup. Kaz algorithm. I could pay you, not in cash. But I could say, I'm gonna give you bonus points. Get in there. Start rating stuff. Give you something.
Okay? Okay. Any other ways to get ratings?
Oh.
yes, spam annoy. And you see this a lot where I get every time I buy something on Amazon, I get email afterwards.
Oh, no.
Brandon Nguyen
a long time. I've never read anything.
no else.
Yeah. they know there's a real first book, and if they must motivate like a small business.
so a small business, so you might have more of like a good goodbye. No person.
So it could be more of like, yeah, kind of like a personal relationship.
So and there's also a kind of alcohol like kind of personal appeals, too. So you go to the local coffee shop and they ask you to rate it. And you're like, Hey, these are my local people. I like them. I'll give you a rating. Sure.
it's not in personal any other ways.
Yeah, yeah, make it convenient.
you've just finished watching something. Now pop it up.
I've seen another version. Where? When you sometimes when you join a new service. sometimes they'll ask at the very beginning.
Hey, here are some things. Do you like these or not? And so try to kind of Bootstrap their model of you, and say, like, Hey, do you like this kind of thing or that kind of thing. Click, click, click, click, click.
like on Youtube, TV. Right now, it'll say, like, Nfl, season is starting. Which of these teams. Do you like?
I'm like, I don't like any of them.
but it wants me to click some things. And in that sense it sort of learns more about me. Okay.
I just mentioned this because again, we're gonna talk a lot for basically the next week in this world of ratings. Because that's where all of this original work started. Okay.
we're gonna then pivot away from ratings to in the real world. We typically don't have ratings. But what kind of user data do we have for user activity data?
You may not read something. But what do you mean when you come to caz algorithms.
you view things. You click things
occasionally you buy things.
You never tell me 5 stars, but you still do stuff clicky clicky. Bye, bye, bye, bye.
because you put something to it, and you like it.
Something has interest. You might like it. If you buy something that means you like it
hopefully.
video buying it on behalf of someone else. It's tricky. These signals are not clean signals. even ratings are not clean signals. Let me ask you this.
I think it's a good move like Sharknado, for example, to go back to that. What? What ratings for those of you who seem sharp, NATO, would you get it?
I'm sorry. Now.
this is kind of bad example, because you're also telling us
right. But if they're really great. practically great one Star. But no one did you enjoy watching totally solely fun. Right?
If you take you know the film class. Right, Citizen King.
You know some of these great, you know. Great movies. 5 stars. But are you gonna really enjoy? You really want to watch it with your friends? And so one of the issues about ratings
is that if you like, if you're on letterbox, for example, that's like, you guys know, letterbox.
Okay, these are the kinds of ways that like, it's kind of like a social network built around.
And the idea is, there's like a self presentation, which is, I have a there's like, I want to present a version of myself. I like art films. I don't watch this scrap, you know. Blah blah blah, and I presented one way.
But when it's me at home on a Friday night.
I'm watching chart me, though. Bro, so just keep that in mind that ratings themselves often don't really reflect your own interest, even if you rated things. 5 stars. It doesn't mean you really want to watch the thing.
Okay, let's keep that in mind. So this is
even Brady themselves are kind of noisy about revealing your interest reference. Okay.
cool. Anyway, we're gonna do all this stuff in the next week or so learning foundational methods. Then we're gonna see how we can take those same methods and apply them in the more like real world practical case, we have where we have no ratings. We only have clicks, views and whatnot. Okay.
so now, new world, is this, okay, we now are going to do personalized recommendations. So our old hippie comes to our app algorithms and they see a certain top K list.
whereas our history comes and he sees a different list. Okay? So they're personalized.
They're different.
But these are real albums.
I mean, it is like, hip, hop, right? And this is like the hippie lights like sixties like, rock. Okay? So each user is seeing something different. We can still do our one general like top game. As to the side.
you know, this is the most popular today overall that can be interesting.
But now we're gonna get each one of them a personalized list. Okay?
So
we're gonna go back to here. Now we have our ratings right? Imagine we pulled it in over time. Okay, I give you a bunch of numbers now
before we do that. how would we for user a make a recommendation based on this list.
So the idea here is they rated 5, 4, and 4. But they have 3 things. Okay.
So the idea is, we might be filling these numbers. Okay? And so maybe if we could fill in high numbers
here, we can say for user, A, you wake up the class exile at funeral, we should recommend
same thing for user E. They have not seen exile. Are they not heard exile, or one to run.
Brandon Nguyen
put in a rating there and make a recommendation. Okay.
so that's our goal. That's what we're gonna do. First.
sound reasonable.
So let me ask you this, how could I put in a number for this out of the clash.
maybe.
Look at what other users put in for that number that has similar rating. Look at what other users have put in that have similar ratings to me
and do that. That sounds very sophisticated.
That sounds like, what we're gonna talk about. Maybe at the end of today. Certainly. Monday, a user user, collaborative filtering.
I like things that other people like me whites.
I like things
that other people like me like.
yes. Oh, look into the content of the things themselves, and say, the class coming out of England! It's it's a it is a punk! It is a certain genre go find other albums that have the same genre or come from the same country. These kinds of things. This is very nice. We've already heard.
So yeah, we have what's called like.
that's like, content based, we actually look into the thing itself.
Okay. we heard user user
collaborative filtering. I'll fill all these out later. That's where I like things that other people like me like. What are the even? What's the even dumber way to do this the simplest way.
Well, we start simple, with a simple way to put it over here, Random. Thank you.
We could do random.
No, give me another. What's another way.
So we could take the item item
average. So that would be 2 2. The item average is 2.
What's a different way? We can put a number here
average your existing ratings average, the user's rating.
That would just be the user average. So in other words, we're saying is the user gonna like it. This user is very positive.
So yeah, I think they're probably gonna like
this user is a little more negative.
Lower. Okay.
you can say it's not going to be either average. You can take the either. Max 3, you can item in one.
Okay.
user average, which is user Max, user men. There are lots of ways to fill this thing in. Okay. So if you were, gonna figure out, what's the best way to do it.
what would you decide?
How are you
really great?
Close the door we're gonna have like this, like experience. And your parents who are gonna walk in
so no move yet.
you know.
No.
that's that's it. Yeah.
Oh, we mean, we have a train dealer.
We have testing data. All of the.
we put the data
and we could learn. We could use item average.
And then when you try it on the held out data and measure something.
Now, we could figure out what we're gonna are we figuring out like imagine
after the fact, I learned that this person really gave a ford. I could say, well, I guessed
4.6, and it was really forward. And there was like an error. Okay, but just to keep that in mind, like, there's lots of ways to do this. But we're gonna need to put this all into some evaluation framework to figure out which one does the best.
Okay?
And what I'm going to show you again, probably Monday, maybe Wednesday.
Is that even dumb stuff like this
10 years ago was as competitive as any fancy method
in production at Netflix.
You you almost do as well at Netflix, just guessing the item average and the user average as their actual engineered solution.
It's true, it's true shocking. Is that true anymore?
Just let you know this. This is the truth. Now.
let's do the key kind of first foundational method. So we're gonna call this our baseline estimate. It turns out this is very powerful and right out of the box works, really? Well.
okay, so what are we? Gonna guess for a user, you for an item, I, what's our guess of their rating?
Well, we'll just guess whatever the overall average rating is.
And then we'll move it around. We'll basically say, like, is this user, like optimist or pessimist, move it up or down. And this is item kind of
like, well liked or not. Well, like, and we'll move it around based on sort of this item kind of bias terms you kind of think about. This is like an item bias.
This is like a user bias.
And you know, that's a user.
That's the item
Brandon Nguyen
that's our guests
to this this user. Okay.
okay.
it's cool. So let's do it.
Now, we have a matrix.
And now, basically, we could build a personalized recommender every user. And I didn't get a different number based on this.
So the first thing we need to know is like what is mute.
What is the overall average rating? Well.
3, 6, 1013, 16. There's 20.
Does the map work here in the 70
he'd be dying full time. I knew he was. Gonna ask that I knew it. So the overall rating is a 3.5.
Right? So then you have to say, Well, what is like the bias for I'll put a
for user. A,
it's basically go back and say, Well, what was that again, it said it was the average rating of user, a minus the mute.
So what is A's average rating?
4.3 3 minus 3.5,
which is what
something like this.
So on average user, A is a little above the average. They're positive. Then you would say, we could also do that like, what is the bias for user? Be
so user? B, they're like 6, 7, 1115, divided by 1, 2, 3, 4, 5 is the 3 on average.
So they go to 3. So they are a negative. They're about a half a point below cool.
And then we could say, Well, now, look at the columns right? You would say, of the different items. So you'd say, what is the bias for?
Let's do the clash
again. We would say, what's that? We just say, what is the average rating of that item minus the mu? So like, how far does it deviate? Well, the class is on average, the 2
3.5. So it's about minus 1.5. It's much lower.
And we can say, you know, what is the bias of, let's say, Abbey Road, and that is what 9, 1011,
14, divided by 4,
which is what
I think.
So there's no bias or like. There's no deviation. That items right on the point.
So now the point is, we can go go back and say, Aha! Like. what should that number be? Right there? You'd say, well. it would be 3.5.
That user's bias, we said, was point 8 3.
And the item bias clash was minus 1.5. So that's 3.5 plus or minus 1.5 is 2. So 2 point
8 3
boom.
And we could do this one and this one and this one and dot dot dot dot we can fill in all of these entries with our guests, based on the overall average, modulate it by the users bias modulated by the item bias, we get a number
cool
question.
This one's like better than
like ideas of pattern, you know. In practice this is a very powerful strong baseline, traditionally.
and just like we talked about like like you're gonna do. We didn't really get into. But like, if you're not gonna do any kind of fancy learning
your first ratings based estimate should basically always be this
empirically, historically, that work. Well, it's simple. It seems to kind of match, like our intuitions deployed
cool question about this
and item biases and user biases, or what?
No, typically, these are totally separate. It's like
we have the things we know about. We make our guesses. and later on let me go check how good our guesses are. But in practice we're not sort of doing some loop where we're updating
the the calculations cause. Then you'll like, go crazy. So don't do that.
Okay. this is baseline estimate.
You should do this on the homework is not hard.
It's like averages is easy. Okay? Now.
there are different ways to to determine. Like your vus and your Vi's okay. You can play around with.
Yeah.
yeah. So typically, we'll usually fit the original formulation on the homework. We ask you to play around with like these different versions. Okay.
I'm not gonna talk about them right now. I want you to do that kind of on your own, because we asked you some questions about this, but the idea is like, this is like the simplest version. You can try some different kind of version. Okay, try
homework to see what happens. Okay. you can do even more stuff. Right? So there's this idea that you can quote regularize the baseline.
So the idea here is, you can add in like some beta
for each user. Right? And so basically, you're trying to regularize like, what is the impact of the deviations?
Okay? And so the idea here is like, let's say, Beta was like, really big 10,000,
right? So you're saying, like, this, user bias is one over 10,000 times something.
So whatever this
differences doesn't amount to too much. Because we divided by 10,000,
we're basically saying. I don't. I don't care
right? I'm not gonna incorporate the user bias into this very much. So this deviation is quite low. However, if Beta is closer to 0,
then we're back to the original formulation.
Okay? So the idea is again.
there's not like the right way to do this, but there is a way to say, like, we need to see at least a few ratings from the user so that we feel confident in incorporating their
their deviations right? Because if you go back here again like you could imagine.
you know, imagine there's like some new user I can't write on there.
I'll do it here. But imagine there's like user, Z, you know. And they came in and they just gave a 5. They only rated one thing we're saying, they're kind of like one and a half over
the average. I know it would change if we added them. But the idea is we're saying it would be weird for users. Z, to think we think we know a lot about user Z, because they only gave us one rating instead. We say, we kind of need to see at least a few ratings before we're really to really believe that deviation.
Okay.
that's the whole intuition. And again, I'm going over this a little quickly, because again.
we ask you to do this on homework, too. So you need to really play with it
and see what happens. Okay, so there's a formula. Maybe it's kind of washing over. You. Don't worry. You have plenty of time. Go offline.
try it, try different versions. See what happens.
Okay?
No, yeah. I'm not gonna do that.
I changed my mind.
So take away.
No.
And II lost the little like the little patch
would have thought.
I know it's like this microscopic, ridiculous thing.
the takeaway, if you have ratings, use baseline
super easy does pretty well.
Eventually, we're gonna use this even when we get to our email, what email coming back?
Yes. And when we do the next. Ml.
we're gonna do gradient descent people.
Oh, yes, yes. What are you doing from like
cause we gotta know this stuff. When we get to Lms.
we're gonna learn about transformers.
The
you know, like, yeah, when I was a kid, I own.
And I also read how many books
it was strange to me, it's like.
okay.
Now, that's all. Baseline. We can do personalized recommendations. Easy? Right? Got? Yeah. Averages, subtractions. Easy. Now.
user. User. Cf. I like things that people like me like
right? This is what this is all about. There's a really collaborative.
The idea is, I look at my people who I collaborate people who are nearby me, who are like me
people. I like things people like me like, Okay, here's some movies I've seen recently that I like, if you like these movies. I want you to raise your hand and keep your hand up. Okay?
you can see any movies recently. okay, okay, okay, okay, great. I like Barbie.
Okay.
last few years.
Ii don't like, I don't like the the recent Spiderman. I like the animated ones. I don't like the live action
now. Okay, and who else is like that?
We just need to. Let's say you also be removed, that you like
also coming back to my point earlier about presentation. Right? So you can. You don't have to leave this. You don't want to. But like
part of this is like you're like me. Make the recommendation
say that our despicable need in toy story 4.
Well, the good news is, I haven't seen either of those. So of course, that's not true one. I've not seen despicable me. I have seen all the toy story movies. But imagine now, this week I'm gonna go watch them.
and if I like them, it's kind of like a signal that this is a good idea. These are good matches for me. Okay.
now, writ large. Think about like all the users we have.
and the idea is out there. There are people who, like all the because all the movies we named were very popular, right? But imagine you found all the niche stuff, all the weird stuff. There's someone out there who likes the weird stuff that you like.
but they seem more weird stuff than you unbelievable. I know right?
And so then find the weird stuff that they like that you haven't seen recommended to you the idea that may be very personalized. Okay.
Brandon Nguyen
this is the intuition of user user. Cf.
big idea are hippies.
All of these people.
It's just a mass somewhere in there are other.
Let's find those other weird old hippies.
and they figure out what our albums they like that are. Our old guy hasn't seen. Recommend those.
Okay, that's big idea
to the challenges. One. We have to find these other users. 2, we have to somehow
figure out which items to recommend just because we found other people doesn't mean we know the items. The 2 steps find these people. and then somehow, based on the things they like. Figure out. Which of those things to then recommend to our old guy?
Okay. So I asked this to you, how might we find similar users similar.
Absolutely. we could ask them.
suppose we suppose we have our big user item ratings matrix already
so integrated the motion, the same would be the same.
we have this thing.
How do we measure like the the similarity across roads? And I say, like, here's a user who's like this user, I'd like to be able to do some similarity across rows.
You guys know how to do any similarity calculations that compare. That's not said
it all is coming together.
Oh.
Wade.
it's almost like this is all planned.
Also. No joke. Be thinking about our T-shirt design. I'm gonna start a thread on slack. and it's got to be funny. It's got to be meaningful. and you gotta wanna wear it.
I'm not gonna buy 90 t-shirts, and they go straight into your garbage.
In fact, I can just picture I'm walking across campus right after class. We hand them out, and the garbage cans are stuffed with my T-shirts. I would rage out like you can't even imagine. That's what I'm talking about.
It's fine. He'll do it.
I would pull all those T-shirts out, and I would sew them into a gigantic T-shirt, and I would wear the gigantic T-shirts to like as a symbol of the shame.
Wow!
470
when you graduates this September.
That's a tough one for me. I've been to a lot, but I've been to a lot of graduations here like a lot. First one more.
I tell you what, they're important for you. Graduation, no joke. So boring. so forth
it is.
But if you were really gonna wear that shirt I might come. They won't let you. If you have it on ahead of time they'll make you take it off.
Be careful, cause I'm also worried. Give me like gun, you down, or something. This is Texas, right? He's reaching for his pockets. What we got in there I don't know. What are they doing
as you're having your last minute.
No, don't do that. Don't do that.
Okay. they'll go back and watch all these videos. They got all the evidence. Okay.
we have to find. Yeah, other users. Let's define a similarity score. We'll use it, and then we'll find
we'll find that
became yours. Neighbors. It is all coming together.
So
how do we find the users? The similar preference? So we're going to define a similarity score based on ratings where we take in rows of vectors.
and we know how to do this. I'll give you one, which is Jack Harder. You remember Jacquard
Card is simple set intersection
overset union. In other words, it ignores the ratings. It just says. Did you rate lots of things of the same things in common?
Okay, so Jack, hard of, let's say
A and
back here
the Jacquard of A and B is.
how many things do they raise in common? 1, 2,
3. They rated 3 things in common.
How many do they rate? Overall? 1, 2, 3, 4, 5 things. So we don't count funeral, because neither of them rated it.
So we would say they are.
you know, point 6 alike.
We can say, what is the Jacquard between A and C.
Fancy nothing, one, nothing.
They have one thing in common. but how many, how many of the Anc rates
for in Union 1, 2,
3,
4, 5.
So we now have a jet card between B and C. So if we were saying.
I'm not gonna do it for everybody. But if we said, find the one nearest neighbor find the one person closest to A who would say, it's B
because they, their jet card is higher than it is for the others.
and I'm not doing it for DNA. But you could do that. Okay?
So that's one way we could find someone else. Now, in this case, let's say, we found user. B,
we now have to figure out like, how do we aggregate, or how do we take their ratings and give them to this user? A.
And so you can say, well, they rated one, I'll guess one. They rated 3. I'll guess 3. That's a very simple way to do it.
Okay.
But if we had more than one user.
yeah, take the average. Yeah, question
finds the people that like to write a lot.
So we stand it up. And then we, you know. So you can use this in practice.
Get that nonsense out of here
that out of here let's go use cosign. Cosine does consider the rating right?
Same exact setup. Right? Now, the rating vector, for the user.
We're waiting for the other user, not queries and documents. But they're still just vectors, formulas. All the same.
intuition is all the same. Okay?
So we do, Cosine.
I'm not gonna do one of those cause you know how sign, I think.
But we can do cosine
probably preferable to Jacquard. You would think
right again. Do you want me to do an example of cosine?
Yes.
why not? But I will tell you. This, though one question is.
is. do you consider the entire
I'm not going to cosign. Yes, question
the magnitude, the vector? Magnitude. It's the norm.
It's just the the sum of the squares.
Other instances.
Yeah.
yeah, it's just telling us the L 2 norm is the square. It's the standard squared magnitude that we yeah, that we know. Good point. Thank you.
we can do cosine.
There's also another funky one called Pearson.
Okay, it's very similar. But notice what we do here. We subtract out the averages.
And let me do an example of this next time. I think we asked me to do this on the homework, too.
yeah. So there is a difference here. The summation is only over the things they have in common.
Notice this this is for all the items in the Intersection
post sign. We consider all of the items. And would you like our vector. Magnitudes? We consider all of the items.
So just come back here like, if we're comparing.
You know, these 2 guys in the Pearson world, we just say we both rated it. Both rated. They both rated it. So we're going to consider those when we calculate our similar area
right where we do cosign, we do the vector magnitude in theory, same thing, it's going to be different in practice. No. But if there's 0.
But in the denominator we're gonna do the denominator that we're not considered in the other way. Okay, some of these points may seem kind of subtle. And you're like, so what you have to play with it and see what happens. Okay.
this is Pearson correlation. So in practice, this is a very powerful way people often use to figure out who's most alike.
Now, can you imagine a problem with this
like
always come back and think about how big is this set right here?
But it's basically saying we are exactly the same. Imagine we only have created one thing in common.
and we both rated the same.
We're done perfectly. So
that seems a little weird. One thing in common.
You might want to consider like you want to sort of consider
like I would prefer to have a confidence if I have like, we have 100 things in common.
Right?
So it seems we have more common. We read a lot of things in common versus be ready. One thing.
So this is one of those things where, like the formulas kind of tell like a story. And like, you're like, Oh, yeah, I kinda get it. I know what's going on. But when you really get into details like, Wait a minute. Wait a minute. This is weird like, I can just go use Pearson. But I'm gonna have all these kind of that kind of spurious kind of connections. I found all these people who are just like me.
but sort of in a soft sense, like there we had one thing in common.
and we happened to rate it the same. Does that mean we're exactly the same.
I would. I would pause it.
Okay.
some Holland's, but they're fine if I'm it's fine.
Okay.
If I were to rank rank the toy story movies.
which one is for again
3 is the Teddy bear? 4 is like, a
yeah, yeah, yeah. I would rate the rate. Then this will be you're not gonna like this. 2, 1, 3, 4. Okay.
thank you. I love Al.
so gang. Here's here's my suggestion.
You start on the homework.
Okay, look on slack post your comments and questions there, post your swag ideas there as well.
Do the readings. Check out the slides. I'll be posting these slides. Obviously. Now, on Monday we'll finish up user user and item, item, that's all you need to do. The homework
cool. See you all next week.

Yeah. Love. Wednesday. Hello, so, yeah.
okay, cool
coming on. But
folks, we got a lot of work to do today.
homework. One. Thank you for submitting. We are grading. Now
we will try to post some sample solutions. Maybe guys we can see at least see, we're gonna give you kind of like our solution. But then we're gonna try to pick out some submitted solution. So you can kind of get different perspectives on how to do the problems. There's not one way but that way you can kind of see different approaches. So we're working on that right now. Homework 2.
And that was fun
homework 2 is gonna go out soon, probably tomorrow it could slip to Friday. I don't know you're gonna do in Bcg, you're gonna do classification. And you're gonna do recommenders. We haven't even started that yet. But we will soon.
All by hand.
Okay, so this is not a programming assignment. It's gonna be. But you can use a calculator, and you're gonna be calculating stuff. And it's in the style of a midterm. But much like.
you know, like the calculations are something you couldn't do on a mixture.
If that makes sense
like on this year, we don't have time to do key and end of like
8 or points.
Kind of preparation for the midterm. There'll also be. That's a secret.
A secret question who knows what that'll be. But we will have some secret stuff on it, too. That will be. I'm excited.
Any other questions concerns comments about anything.
We good.
Are you guys really good. Yeah.
My goal. Also, by the end of the semester, I want everyone in here to truly be in that Microsoft. So we may have started having, like social and mixers. What happens is you come here and it's like you got your crew. You got your crew.
You're right now.
Maybe most of you will stop coming to class because you're like, I don't want to do that. I don't want to know anybody else. I have my friends. I need no more friends. Okay.
so what I wanted today is
hopefully finish all of this recommendation. So I need is everyone to be with me. So we can get there. Okay, also be on your toes. Gonna be asked a lot of questions today. And then the Friday, okay.
yeah.
we talked about, you can use those for meeting classification test beyond is relevant. It's not relevant for each rank business right and can do it. For like is this text to span or not any binary decision you have to make.
You use these classifiers for. Okay, we've been using and ranking context, which is, you give you a period of document error, and I output relevant or not. That was kind of the goal. That was the label. We were trying to put on it. Okay? And so I left it with you. Was this was state of the art in 2,004, and I know in your life, in your mind. That's a long time ago. But in my lifetime it's not that long ago. Okay, we're just figuring this stuff out. You're like machine learning. They must have been using this all the time. And really it's not that old.
That's what this guy was doing discriminative models for Ir. So discriminative classification model is what he was doing and where I left you last time was kind of giving you this puzzle. So I wanna back up a second
and just give you the setup of what this guy is doing. He has 4 Trevates, basically 4, like benchmark collections, queries, documents, relevance judgments. Okay.
he has lieber, which is just like a traditional non-m, L
approach. So this is non, ml.
okay, he's using a support vector, machine, which is Ml, which is a classifier. Okay?
And he used features he used. We talked about how Yends uses like 1,900 features. He used 6 features. and what's interesting is all the features used were variants of traditional concepts that you already know.
Tf, idf, tf, ivf, okay.
so you already know this stuff. So those features all went into his classifier. This is the classifier.
Okay.
now, what features did he use? And I'm just showing you these
to show you that, like you already know all the stuff you used.
So I talk about features. He had 6 dimensions. So what were his dimensions?
Well, it was sum over all of the query terms that are in both query and document, and take a log of accounts of the query, and then, oh, it's just like a log of some counts
or one plus sometimes divided by the size of the document or log of an Idf
or different versions of like these. Ivf combined. Okay.
I don't care even the specifics of these. But if you eyeball these, they all kind of look like Ts and Ivs and various versions. Okay, there's all like simple stuff. They take those features, shoves them into his classifier.
Okay? So instead of having 2 dimensions, he has
3, 4, 5, he has 6 dimensions.
We're the 6 dimensions.
those 6 features. Okay, so he's gonna plot all those points
learn his Svm. He doesn't really block for me.
but he learns his Svm. He learns his separating hyperplane. And now he has relevant and non-relevant.
And the big takeaway was again like, What's up. he said. Look, I'm gonna take.
Let's just grab one of this one. This is lm.
that's traditional right?
That's like, that's old school.
And then I need to use what's a good color I should use. It's not pink that would
does green work. How about this?
I'm just gonna circle it.
This Svm. that's the Ml version.
And like we talked about last time is basically like, it's way works here.
talk about higher, bigger, bigger, maybe like, call, okay.
And so the big takeaway from this work was basically, hey, I tried using machine learning.
And actually, again, it kind of sucks compared to just some basic like, old school tf, idea stuff.
Okay, this is a 2,004. You would think we had figured out machine learning by then. He said, No, it actually doesn't work so great.
Okay?
So I left that I think with you last time. Which is.
why are we still talking about machine learning? Because that's what we're going to be doing. That's what we do use.
And yet this result was basically, you know, doesn't really do a good job.
What's up with that?
They were trust. They broke the trust here.
what's up with that. But so let me ask you this.
do you think we need better today?
Yes, why do we do better today?
We stand on the shoulders of giants.
So yeah, there's a number of reasons why we might do better.
So okay.
Summary, all the best results, you know. At best, the results are about equal to Lemur old school. Sometimes a bit below the argument was twofold. We could
add more features. They only use 6. Okay. And
you know, today we have fancier.
Ml. Methods.
so we don't have to just use an Svm. We could do something, fancier, so we could change out the method.
But also they said we could always add more features. We only use 6 features.
Okay, that's the whole point, is the argument I've been trying to make in this learning to rank business is, if you added 10 more features, and I think more features, I mean, like the popularity, the number of clicks, the whatever
you now have to go update your function. That scoring function
has all those parameters, and you're doing it. The idea now is, we just learn that function, and we can easily add more features, we could add a thousand more features.
I don't care. Let the Svm. Figure it out.
and so again they give an example, where they did some other kind of task, and they added in some new features, and they did start seeing improvements. 58
or even up to 78. They started like nuking the traditional methods by adding additional features. So that was sort of the big idea. Okay?
So I want you to take away from this.
In fact, I think I have a question on the little quick quiz this week, I said, you really care, and not that it's stressful. It's who cares right? This clicky clicky, what
this is kind of a funny result, which is, does it really work? But it's saying, we're gonna it's gonna work soon. Okay, that's where we go
and practice. Okay.
finally, we get to this point, which is all the thing we've been doing so far is basically saying, like, we have relevant, relevant, non-relevant, non-relevant. And then we try to learn
some kind of line that separates them. Okay.
But of course, we know we do ranking. This is not really what we do. This is more like non ranking like traditional, like Boolean retrieval.
Boolean retrieval would say, I have a query. I found all the relevant documents I give you all of them back.
That's what we've been doing so far. All of the stuff we've been talking about is actually not really ranking. But I care about is like, is this one the best one? And then this one? And then this one. I want an order on them.
But we don't know how to do that. All we know how to do is say, relevant, not
so very briefly.
I'm going to get you to like basically the current state of the art. which is, we can do 2 approaches. Okay, one is we can do what's called pointwise learning what we try to do here is set it up, and then, if we're not gonna talk about it.
no one releases this evidence. But I want to understand what it is, and why it makes sense.
Assume we have training data.
theory documents and knocked before it was what was this year
relevant? Was it relevant or not.
Now, let's look at the scores. And the score is like, in goodness.
Oh, yeah, we did this before. We said, what if we had training data where we did this relevance. Where do we use this before
which metric can be used for reading? That
was a precision it wasn't recall. It was our new friend.
Nbc. Gene.
this is the time of the class
where I throw you all out, and I say, it's like you ever had a coach like this.
You didn't call your brackets. You didn't. You're serious. Get out of here.
you all leave. It's crazy. You come back the next day, super dedicated.
Why not really right.
So my brother was on the High School basketball team and had a coach, is not very good.
and my dad at the time gaining this book about. There's legendary Coach Bobby Knight.
He's Indiana. There's a book out. And then he famously would share his very volcanic. But in the book describing the season, he threw out his team to get them to focus.
So my dad gave the book to the coach, the High School coach who read it, and the one thing he took from it was he threw my brother's basketball team out of practice.
not performing.
and my dad felt very bad. It was totally stupid. I'm not gonna throw you out. But anyway.
when we're doing Ndcg, we wanted graded relevance, we want different levels, not just relevant or not okay.
Because again, in our world, so far.
we would basically say, relevant.
not
right? These are all the same.
But we can tell sometimes they're really awesome.
So we would like to know that. Okay.
so there is an idea which is instead of outputting for query documents, guessing the relevance or not, let's guess a number.
And for those of you who took machine learning
and who came to class a few weeks last week
we talked about a different kind of learning task where we're not predicting a label. But we're protect predicting a number.
And what was that called
numer something.
It's not classification. It's regression
brothers and sisters. Regression. Okay.
our goal is to output a score. This is called regression. I think we can have with any value any real number.
if you can like so numbers. If I say you can output any number you could output minus 10.8 or plus
16.3 4 or minus 2 or plus 4 dot dot dot.
Okay?
Alternatively, there's like a special kind of learning called ordinal regression, where you can only output kind of like sort of sortable levels like 0 1, 2, 3, 4.
Okay. this maps exactly to what we were just talking about.
So the idea here is you would somehow take in as your training data
all of these queries and documents and their score 4, 3, 2, one. We would learn something like an Svm. Like a ratio like a cane and something like that.
but that then, would output 0 1, 2, or 3, or 4.
That makes sense. I think.
Now we have, Ricky, because we can say all the dots that score 4 should be the top.
The doctors scored zeroes to be the bottom and everything in between.
Okay. Now, I said this, I don't want any of the details. And I'm gonna do this.
We're not going to do this ever again. I just want you to know it exists. But I do want you to know about is this, okay? I just meant to contrast it with your brain thinking like, why did we do that? We could do that.
But in practice, what we're gonna do is this pairwise learning what's nice about this is. I can basically take all the classification stuff you already know. And what do you already know about classification?
Nothing.
The
you guys ever see the Blair wish project? You really can't
download footage.
So zoom room is a very last image in the home
spoiler, alert
vaccine is, and there's some sort of monster ghost thing.
and the last scene is. No, they come into this room screaming, Where where are you, cat? Where are you?
Right? Yeah. He is the killer, the spirit before she kills them like you would make them go in the corner. And so she comes down to flash 5 different characters to stand like this
is how I feel like
moving some of these, then, emerge.
I don't want to be murdered, but
so we talk about classification.
We don't have any wish here. and Jim
easy
that, trained again in Roshi we learned a centroid.
Feed him, you know there's nothing right here. We learned central it.
and then the test time we take it to do, pointing the distances. And then we can be assigned relevant or not.
Okay.
what's cool about this is we're gonna basically do the same idea.
What we're now going to do is called pairwise learning. You give me pairs of documents.
and I'm gonna tell you which one is better than the other one.
That's exactly telling me a rank order on the 2 documents. Right?
This is the idea pairwise. So 2 instance pairs. So for when we talk about instance, pairs
that would be like, Compare Doc, I to Doc J.
It allows them to train on what
I basically, when I do my training, I figure out. Oh, I prefer this document to this document. Why do I prefer it? Well, it has a higher cosign. It had more clicks. It had certain features that correspond while on this one. More than that.
Okay. But what's interesting about that is for a different query.
Right? It might be a different set of features
right? The idea that I'm learning some function which is not different, outputting a number and say, I prefer this one to that. Why? Because of the certain features. So how do we get there?
It turns out the trick here is is what you're going to like is you can do your traditional classification. So what I need is this. if we took 2 documents?
Thanks.
Right now.
we know how to take a document and a query and get relevant and a different document, a different query and guess relevant. That was our binary classification setup. Instead, we're gonna do 3 combine these to a new world. We have document documents.
Weird.
you with me.
So we're gonna go from like a 2 part
space query document to like a triple document. Document? Query.
okay. so what we're gonna say is, basically.
this is original idea. I know how to say, no, they're not relevant or not. Now, I have a new representation, 2 dots on query.
And if I output basically like we're saying, like, you know, positive
or negative, let's say, is this thing relevant or not, we're saying in this case, because the eye is first, we're saying I is preferred.
A negative would mean document in this case, like K is preferred.
Okay.
but does this. Make sense a little bit. Let me show you example.
Okay, example.
Right now, I have a document one.
And Mike, what is my query? Let's make it real. Let's make it real.
We're gonna we're gonna do it live? What is my query? Meet me at midnight?
Meet me at midnight is my query.
and here we have. We have a bunch of docs. Okay? And so we do is normally, we would say, Aha!
But, Doc, one for that query. we have some vector representation can remind me, 10
3, 2, 4, zeroone, 3, whatever. Okay?
And so it could be, we don't know. But it could be, you know that was the cosign.
That was the number of clicks. That was the some measure of the freshness.
That was something else. That was something else. So these are my features.
Everyone is cool with. This is what we've already been doing
right. If I have this and I have it for a bunch of docs. So here's Doc 2
and a query. Same query, let's say.
and it might be 6, 1 1 4 0 point 4 minus one.
Okay? And if I give you a bunch of docs like this, you know how to do central ways and do nearest neighbors and all that business. Okay.
this is the cool thing about this. We're gonna transform this
into a like a unified
space. So instead, we're going to do is we're going to say, I have, Doc. One. I have Doc 2, and I have a query.
And what's the easiest way to combine these 2 representations?
I don't know. I'll just subtract this one from that one.
Let's try that. So it'll be. for to
1.0 minus point 3
2.
Peter's brief
Spain is negative.
We just look at the difference
direction.
You got some traction.
Yeah.
you're like, Gav. brother, I got addition and subtraction. I got it unlocked. I got it. Don't worry. Here. I got it.
Division. Slow down. I can do subtraction.
Hey? Yeah. So check out what's happening here.
This stock. We can plot.
What I've done is now, I'm actually 2 dots in the same query, and I don't have this new representation. So this lives in like a different space. This is like the
it's like a unified space. I can't really compare this to that right?
Because it's like, it's in this sort of subtraction world. Okay? Well, that's just for 2 docs. Well, let's go to the next slide. And let's say, suppose I had Doc? One
query. and it's gonna be the same thing, which was 13
2.4
0 point 1 negative 3. And now we have. Let's say, Doc. 3 in the query.
And this one is, did you just say sick. Someone say, sick. Is that sick to have Doc? 3.
Okay. same deal. We can say, Aha. duck one. Doc, 3. Whoop it it. Yo.
doc! 3 query
minus 2 0 minus point 2.
What is this minus 0 point 1 0 something like this?
So this is now a representation of dot one dot 3 queue.
Here is Doc, one and Doc 2, a different representation.
Okay?
And so what we know is we don't just know in our training data. So check this out.
Copy. Yes. based slide over. Second.
you thought I was gonna do something really stupid.
Yeah, that's what we come to expect from you.
Right?
So we have now 2 pairs.
Remember when we looked at just
what training needed. Do we know about this document in this period? We would know if it's relevant or not. Right?
So then, what I'm saying is, what do I know about dot one versus dot 2
in our training data.
What I'm saying is, we know if dot one is preferred to Doc. 2.
So the label we have is not relevant or not. The label is like.
let's call. It is preferred.
And so if this was plus one, and this was minus one.
Okay, again, what does plus one mean?
That means. Doc, one
referred to Doc.
2 minus one in this case means
Doc. 3
referred to Doc. One.
So just knowing those labels, what rank information do I know?
33
Dock 3 is preferred to dock one which is preferred to dock
2.
So in my training data, if I know
which is preferred, to which for this query. I can now induce rank of Dr. E. Dot, 1, 2.
Does everyone believe that? Ish? Okay.
I haven't told you how we do machine learning. This is just sort of showing you. This is the label. This is the label that would mean.
okay.
but what's cool about this is.
you've already seen this world, which is.
Imagine we were plotting everything in our just query. Document space. not pairs. We had a bunch of features. Yeah, he is, Mike discovered, like, Oh, kind of different docs or kind of oriented, or whatever.
What we're gonna do is we're gonna take all of that and put it in the pairwise space.
Okay? So the pairwise spaces.
You know this dot minus that dot. I forgot what we just did. We took that
query query, whatever, and we subtracted the doc one minus the Doc 2.
That's a point. This is a point.
Here's a bunch of things.
And so the idea is, the the line separating them is separating when one is preferred to the other one. So it's kind of like saying, you know, these are the plus ones these are the minus ones.
So in our old world, it was like saying, relevant versus non-relevant. So we would say, relevant
over here would be not relevant.
But now we don't say relevant. We say, you know, preferred
not preferred.
Okay?
And so you know how to build. You know how to find those lines. How do you find the line.
Well, you can take. We can do Russia.
So let's do Rochio real quick.
We look at all these points.
You find a centroid around here.
my marker to write one digit
I want to.
Who am I gonna get in trouble from? I'll go with the palace. somebody we don't know who's over here.
Hello! There's someone always watching.
I'm always you guys ever there's a whole magical world back here.
Okay.
we could do. They're just centroid.
There's a centroid. And if we find the line that separates them, it's something like this.
So we know how to find that line. We use ratio right? So, for example. use ratio
to find the find the line. Find the separator.
Okay?
So the reason I'm talking about sort of belaboring all of this is.
you know how to do classification.
Okay. Now, what I'm saying is, we can now do real learning to rank.
So what is my input? I need another slide, another page, don't I
hang on. Let's see if I can do this copy
doesn't say I don't understand, didn't give me a paste. I think it's an ad page. but now, it looks like that. Let's look stupid.
I think it hopefully copied it just right.
Okay.
now.
So what I'm saying is, number one.
a. take a.
we need training data
with some like graded relevance.
At least.
that's like 4, 3, 2, 1, 0. Okay.
2 form pairs
of docs.
So we do like Dock one dock, 2 for this query.
Doc. One Doc. 3. For this query.
Doc. One Doc. 4. For this query, and then each one we would know, let's say.
preferred not
or preferred.
3. Learn classifier
4, 5.
Profit for use. Classifier. Okay. on new
Doc pairs.
This is the whole setup in this world. Okay, I give you training data. Now, it's not as relevant or not, maybe levels of relevance. As a result, I can form pairs of documents such that I know that one is referred to Doc. 2. Why, dot one was ready for dot 2 and rated 2.
For example. I had thought one thought, 3 dot one is a 4, maybe dot 3 was a 0.
No, not referred. But you get the idea.
Okay. so I have all these pairs.
I can go learn my classifier, which is just binary classification. Okay, once I've learned it.
Now you give me any any dock pairs. I do that little trick where I take the representation of one, subtract the other one. apply the function to it, and it'll tell me which one is preferred to the other.
I can rank profit. We win. we won. It's over. We did it
full clack, golf class, slow class. And then we slowly everyone starts violating.
Not very well.
That was one of the most.
What is that like?
We're moving back together. There's the slow, which is, I'm the villain.
That's the item. Very good as well.
We're all climbing together.
You know what?
I've been actually talking to the football team. You have a new walk out, yeah, video or a song all that, you know. They were saying, maybe our class to be able to get the whole.
I'm at.
Okay. That's all of learning to rank. Okay? So what I want you to get out of this is one we had just some basic machine learning. We're gonna see a little bit more when we talk about recommenders. Okay.
right now.
you gotta get this. You gotta have it homework. You're gonna practice with some of these concepts. Okay? And we get into recommender. We're going to be doing more of this pairwise stuff. And you're gonna do it on a like a programming homework. It's gonna be fun. Question.
doesn't that need to be another intermediary step to create a rank out of it, to go through all the different, do all this calculation, and then sort them for something. And in fact, we're gonna end up doing is you're gonna be outputting this a number
and we'll get that recommendation. So you're just gonna output numbers is quickly exact, exactly sort of an app. We're gonna have to do that, probably on a future homework. Okay.
all of this I wanted to show you.
because these are just examples of the actual concrete methods. If I talk about point-wise, we're not going to do it.
These are all different methods. Pairwise. there's all these, you know, that you can do Lvm. Or rank net or frame
or Gb. Rain or Qb. Rain or UV rain or Irsbm or lambda rank or dot dot dot dot
when I'm trying to tell you there was like explosion
of methods. Okay.
these are maybe more sophisticated than just our ratio. Okay?
But the idea is basically, if you know how to do Rosio, you know how to do all of this stuff.
which is all like this, basic learning to rank methods. Okay, the technical details are a little different. But conceptually, you know what's up.
Okay?
So again, if you have interview questions
when you're talking to your boss and internship
missing. What do you about learning to rent? You're saying.
is it? You know landmarks.
You're like no
simpler classifier. But I know exactly how the setup works.
You're golden. Okay?
Right
now, I want to put that to bed.
go to sleep.
go to sleep
every class I'm gonna end with like a
you know, it's like the tennis player. He's like putting the phone down like whenever he beats you. And I guess Curry puts you to sleep. We're gonna do that, and then I'll do some football celebrations. You guys need to bring them to me
like once we've dominated the topic. put it to sleep, really. go to sleep.
This is the big kind of first big inflection point in the class
you came the very first day. It was all like motivation. Why did you take the class?
For whatever reason you decided to come back
for whatever reason?
Okay.
But we've been on this march where we can talk about searching. How do we tokenize? How do we parse? Do we do limitation? What do we do?
Are we ranking and not rating.
Okay, we have more sophisticated query operators like, phrase periods. Whatnot.
Okay? Oh, maybe machine learning can help us to break all of that.
You can't go to some other universities and whole semester just learning there.
You did it before weeks
activity.
Now
do a new topic recommender. Okay, we're gonna do that for the next, like 3 or 4 weeks.
Then we're gonna
then we're gonna go back and revisit all of our basic assumptions and take it to the next level, sophisticated like modern day stuff.
That's the plan.
So we're now get your brain search engine gone recommenders here.
So we talk about search engine. You think about searching Google, this kind of stuff. We talk about recommenders. We think of what
we motivated this way a long time ago. Yeah, Netflix, I don't go to Netflix and type in
action movie. I'm just like, find it. And just show me the stuff I want. Youtube. Show me stuff. I want. what's the fundamental difference between the search engine and the recommender.
There's no query. There's no query. It's just give it to me.
Okay. so
yeah. remember, we have a record store. We're adding to it.
What can we do so far?
And again, when I say, what can we do? So far, I mean, you know, like, really, we're building this thing. What we do. Well, we got Julian Andrews knots.
We had do phrase query with a positional index.
That's a good midterm question about positional index. We don't have an opportunity. Query and wild card query with our per new term index
awesome. They're good answer questions.
We could do rent retrieval. Yeah, vector, space model.
Maybe the M 25. I mean, it was built on the same ideas. We didn't really do it. We know how to do handwriting functions.
The score is this, plus that, plus that. Or we could try to learn a function.
Okay.
all of these require query. Users got to type something, and then we respond.
now we're moving to the world. We don't have query.
We just learn from the users engagement, the clicks, the ratings, the whatever they do.
And we build a model of user. And then we try to guess what they would want. Okay.
so today and next few weeks. no query, but we can still generate recommendation.
Search engine, go to sleep. Bye-bye. Now.
recommenders.
Okay, cool. Now. 2 slides. Just to put this in context. When I talked about history of retrieval.
I showed you maybe picture some old guys. Yeah, old ladies and old guys like people were thinking about this in the 40 s and 50 s.
they're making big innovations in the 60 s and 70 s.
again, vector space model, it's all sixties and seventies.
Okay. recommendation as like a field, a thing we think about. Thanks. I'm slits.
Okay.
very relatively recent.
Later on I may tell you that some of these precursors, okay. 95 Amazon launches. I'll show you a picture of the Wayback machine. What Amazon looked like back in, I can buy it. Okay?
Great.
But all of a sudden, you have real use case we need to recommend to people. We want to make money.
This is not though, okay. we're gonna talk about item, item, collaborative filtering. We're gonna talk about that.
Friday.
So by Friday we'll be up to the 2,000 again when you tell your parents what you learn. Say, we're living in the 2,000 s.
In 2,006. There's something called the Netflix Prize. We're going to talk about that revolutionize
recommendation. They paid out a million bucks to somebody.
But that's someone much like you.
Okay. 2,009.
We're gonna talk about this, probably end of next week
in the next week you're into 2,009. We're getting modern. Right?
Also, I put industry and university because it's back and forth stuff coming out of university going into research or going into industry and back and forth.
The email wasn't gonna take off the tins.
So we're gonna get you in our first and next 3 or 4 weeks we're gonna get around here. We're gonna stop it.
Then we're gonna come back and revisit this. And we're gonna talk about
the deep learning revolution
we're talking about like adding deep learning to recommenders.
Only 2014. Now, we're getting very recent. Okay?
Then, people start thinking about fairness and accountability.
Then there's a push back on deep learning. People say deep learning doesn't work.
and 2019,
there it is. We're gonna get their L oms plus rexis, the stuff that's happening right now.
Every company right now is losing their minds over how to do this. Okay.
we won't get there. It's gonna be awesome. Bye.
it's gonna be awesome. Now.
are you excited? Yes.
you should be.
This is cool. This is like the best.
Okay.
like, I'm working on a team in my like other hat is doing this.
And you have like, I don't wanna say hundreds. But you have lots of engineers. Lots of scientists working on this right now.
I'm gonna try to take some of those. I'm not gonna steal your trade secrets.
We're gonna take some of those ideas and bring them into this class. So you're gonna be doing stuff. It's like right there at the edge. Now
let's go to the basics
recommender right now you come in there and search for it, then
forgive.
We have a first new teacher. When you come to our home age. be mobile or on the web.
We show you one top K list
right here. The query for it is just there.
what are we gonna do? Map this? That's a recommendation. Now we're looking
when they'd be sophisticated. But we have a list for our albums.
We can put a list of albums right there on the front page.
so I need at least 5 ways. We can do that list from silly to fancies or whatever
we have 100,000 homes.
I'm gonna show you 5.
How do I pick those 5 to show?
Maybe like, show the top 5 from the
go. Simpler than that
most popular? Yeah.
5. Most popular. And again, how do you define popular purchases? Clicks. Listen. Something like this. Everyone sees 5 was popular. Stupidest thing you could do is just select the 5, the verb, 5 alpha medicine
by alphabetic order
newest. And we can update, we can update newest.
you know, every month, better week, every day, every
what? What's even sillier than meals.
random?
We have a hundred 1,000 we have. Let's say we have a hundred 1,000 albums.
I pick 5 by random, and I populate my list. Maybe one. Something is interesting. And then, lastly, yeah, it can be
paid placement.
3.
Google around the same time as Google, there was a certain that operated by placement pay, replacement deal. So if you search for university, whoever paid the most is first right, that's how it works. And then what do Google do?
They said, Wait a minute. This is Google was making no money, and I said, Wait a minute.
We'll still give you our results, but we'll do it with an ad. And the ads will be a potentially paying placement.
Nothing.
One of the, you know, greatest business models of all time is made of
zillions of dollars. So
we have 5 ways. We've now generated 5 different recommendation methods. You now know, recommendation algorithms.
Okay.
these are all non personalized. However.
yes.
they're not for each person.
We could also. Yeah, this is and tune. We could also do this. One.
Paths picks
about 5.
These are all non personalized. Okay.
top, most recent and most popular. Okay. for now we're going to introduce ratings. We're gonna assume we have album ratings.
This is very satisfying.
It's a 3 star album. It's a five-star album.
the different balance
in a moment, and we'll talk about this next time.
How many of you have ever raided 8 months.
Good map! How many has? How many of you have gone or raised something on that? What's in the last month.
Nobody. maybe one, maybe one.
How about on spotify? Are you thumbing things?
Who's done something on spotify in the last month?
More more kind of customizing your feed.
Okay.
How about on who here never bought anything off of Amazon?
How many of you ever made anything on Amazon?
Few of you. Okay. Did you write a review? One answer.
So one of the challenges. And we're going to talk about this, which is, it's hard to get ratings. They're very sparse.
But for now assume we have ratings. They we could pay people if I have. Yeah, if we have ratings.
our users come.
they read albums
dot dot dot dot. We have a big matrix. We had ratings
in ratings world. How can I generate recommendations?
Give me a recommendation method using ratings? It's not just the freshest, the newest, the most popular items
highest rated.
See you all on Friday.
We're done. We'll continue this soon. Bye, bye

You have today.
This is the last possible day. Correct.
Okay. So if you haven't finished. let's get her done. So tonight, absolute. Last
last moment we have a midterm on October sixth. That's a Friday.
That's 3 Fridays from now. That right, let's double check. So what we're gonna do is homework 2 is gonna go out this week. It's gonna be not a big killer programming assignment.
It will. We can make them all killers in my mind
in my mind. It's kind of practice for the midterm, the midterm. Of course you don't have a calculator. You don't have lots and lots of time. Okay, so the homework will ask questions that we couldn't ask on a midterm. But it will force you to really get hopefully work through the material. It'll go out this weekend this week, so you'll have
you'll have 2 weekends. I realize weekends are very important.
Yeah.
you'll have 2 weekends to work on it. The issue is.
it'll probably be due like like the Monday of this week.
and so my hope is
doing. The homework is
preparing for the midterm.
It's not all you need to do to prepare for the midterm, but it will help you for the midterm, so I'll make it do on that Monday. So then you have the rest of the week to not worry about it. But again, some of you may want to use your late days on it, in which case you're gonna come up right to the midterm.
I can help you.
Okay. The main thing is, I'm gonna give you those 2 weekends. It'll be good practice. Any questions, concerns, comments, freak out
praises.
This is angry. Diatribe's
on any topic. It's open design, remember, I understand.
You can stop it.
Yes, the idea would be, I'm gonna ask another thing where I limit the number of late days. So I can give you guys a solution. No, I can't grade it, but I'm trying to do this.
so might end up being due like Sunday night.
Something. We'll limit the we'll limit the limit the late days.
So we can return. We can return a solution
again. It's it's like multiple things. I'm trying to juggle. That's very good point. Thank you anything else? Life?
You might go to the game.
was it? Was it fun?
Is it hot?
It's rainy. Did you see these gigantic planes?
It's alright. rivers, obviously.
In the State of Texas. There, there's good news and bad news. The good news is there's some rankings, and we were the highest ranked
university in Texas. Is that right? Oh, yeah. But then in the football rankings, ut is ranked. Third.
yeah.
And we're we're not ranked because we lost a game.
Thank you.
What would you rather be higher ranked academically or higher ranked in football?
Football? Right? Don't answer that question, Diane.
Also all those college races. They are a joke.
How do you compare a school with like? You know, like a public state serving institution, to like a tiny private school. totally different circumstances.
How do you compare a big research university to a small teaching focus? How do you compare, like a very strong engineering school to a very strong liberal arts, I mean.
hey, let's go. We're gonna finish up learning to rank this is a big week for us. We're gonna be finishing this week learning to rank. We're gonna start recommenders. Recommenders is our big like, second theme. We're gonna bang out recommenders for a few weeks, and then we're gonna go next level supernova
to finish us up on learning to rank. I want to walk through a couple of concrete learning algorithms today, the context that we're doing binary classification.
You give me an X, you give me a thing. I have a magic function. F.
It looks at the thing, and it says yes or no, true or false, plus one or minus one.
And there are lots of examples. So in the context, we were talking about like of our albums. It could be, you know. Is this album appropriate for children? Yes or no.
it could be. you know. Is this album? Melodic or not. is this album? Do the themes of the of the content of the lyrics? Are they introspective or not?
Okay. So you can imagine any of these labels you can cook up, and we can now put a label on our albums.
We're going to use this in the context of
trying to do some sort of relevance assessment. So we're not just going to look at the album. We're going to look at a query and an album. So the X
here. you know, the X is a query and an album. And then the Y, the thing we're outputting is going to be
relevant or
not. Okay?
So we said, we knew from our training from our from our benchmark collection, we have documents, we have queries, we have binary assessments, and so the hope is we can learn a good classifier, and then, if we learn a good classifier.
you give me any new query document pairs that I've never seen before. We can label them relevant or not relevant or not.
Okay, that's the dream. Now.
this is that picture I drew last time, and this is from the from the readings. But the idea was, there were these 2 features. This is like cosine score, and this like window lead score. Those are the only features we cared about to represent
the query document pairs. And we plotted those. And we said, Okay, these are the non relevant ones. These are the relevant ones. And so now the issue is, we want to somehow learn a classifier. Okay? And we didn't talk about how we would really do that. But the idea would be
somehow we would look at. This is all of our existing data. This is not the new stuff. This is our training data. We would look at it. And maybe we would learn like, this is a good line to separate them.
Okay, so we talk about learning this function. F, one example, would be learning like this line.
Okay? So now we talk about, give me a new document. Query here a new document? Query here, and we can make a judgment. Okay, we may not be right, but that's the idea.
Right?
This is all super cool. Now.
how do we learn the function? F, this is the big challenge.
How did we learn the function? F. Here was I drew it. I use my eyeballs.
and I drew that line. Maybe that line where I had a line like Here, do you guys all yell at me. and then I have on my fear, and you all yelled at me, and admission kind of looks like right here. And then he didn't yell at me.
So how did we learn the function? F. Well, we just kind of eyeballed it right?
We can't design all it. We gotta be more rigorous. Okay.
But the mechanism that we're going to do is typically we're going to say we have training data with known labels relevant or non.
we applied some learning algorithm to learn the function F,
once we have F, we freeze it right? And now the idea is you give you new examples that I don't know. I then give a label to it.
I don't know if it's right or not.
Thanks. How do we learn the function? F.
So for those of you who've taken machine learning course, which is who
a few of you, Heidi, on the function. F, give me some examples.
some, some classification algorithms. You learned
Galcia, naive bays.
Nice bays. That sounds good. Sounds good
support
vector machines. Yeah. Yeah. Yeah. Yeah, yeah, yeah, yeah, cool. Okay, yeah. So it turns out.
there's a gazillion ways to learn these functions. Perceptron. Svm's team, nearest neighbors, decision trees. Maybe you did Random Forest. Maybe you did fancier versions of all. Oh, yeah, let's put that into random forest
dot dot dot if you go online and you look at something like their psych hit. Learn
which is a nice package.
It lists a tonnes of these methods. Okay. there's a million of these, okay, and not just these. Then people take them and they ensemble them together. We have multiple different ones. We combine them.
Okay. So if you go take an Ml course, you can learn about the details of all of these. What's neat about this perceptron?
You wouldn't take AI.
You learn about perceptron.
What about perceptron? Is Nml, okay, okay, perceptron is like.
OG neural network. right?
So this is like the basic neural network. Well, guess what
with the recent advances, like in deep learning. Now, we can take this thing. And now we can do kind of like deep learning based
neural networks to do the same thing to learn this function. F, okay. So if you take machine learning, you can go down this path and learn all these different methods. I want to teach you about 2 very simple ones that connect very closely to what we've already talked about. So then you've got it, even though you never took an Ml. Course.
So we're going to learn Roshio classification.
So you're good. We're doing machine learning. Now, it's very sophisticating.
Okay. training. This is sophisticated like big time stuff here. Right?
Oh, for our label training data find the centroid of each class.
IE. The relevant or the non-relevant.
And then at test time for the unlabeled data just assign the points to the class of the nearest
centroid. So a couple of things one, we got to remember what the centroid is right. I can't remember that. Let's let's look at a quick example, shall we? And I'll do this in 2D. This is easy.
So imagine.
imagine these are my relevant okay. and I'll do a really silly version right now.
And imagine, this is
non-relevant. Okay. what do we gotta do? So this is our our training data. Right? So all of these points here from our training data.
In other words, I know what they are in practice. We're not gonna know. but for my training gave that painful. So what is the ratio training method? It says. find the centroid of each class.
What's the centroid? And just like in in English or intuition.
Hello
again.
or even those formal 11 min or the Internet
Center
centroid center. That sounds about right? You can think about centroid as just like the average right? The center of mass.
Okay, so if you were looking at all the greens you want to find, like the center of mass of all the greens.
Okay? And so I'm gonna hypothesize some places where the centroid can be. I think it could be here.
You like this.
Okay? So you're now on the you know the Atari, or whatever you have the voice that move your own.
But oh, down
well down. But
I got it. I got it right down here. Okay? So you said something like, here. he said something like, Here-ish.
okay, fair enough.
Okay, let's do the let's do the X's real quick. The Red X's the
no, probably right here you're done. Okay.
that's fine for
something like this.
So we found the centroids. We'll talk about what the definition is in just a second, but we found the centroids.
Now get some more colors here.
Now, given the centroid. What's interesting about the centroids is, if you, if I gave you a new.
So we've learned it. We're done right. That was the learning. Now, at testing time. Here comes
a new point, a new let's say, query document pair.
and we want to assign it to the class of the closest centroid. So, for example.
if we had a new thing, I'll put a little box here.
Is this going to be a red or green red, because we're gonna measure this distance.
And then we're going to measure
that distance. And then we're going to say, Aha.
red.
okay. And if I give you a point here.
we're gonna do the same thing. We're gonna measure this distance.
maybe green.
and if we have a point here we'll measure that
whoops I measured. I did the wrong. Sorry. I don't want to confuse you.
I lost an axis. But I was going to the wrong thing.
And for this guy, right here we'd measure this distance and that distance, we probably would say, green. Okay, that's pretty simple. Pretty easy.
let's so that's sort of visual examples. Everyone get that. We trained it. And then we applied it.
Let me show you a different example.
But if I actually gave you the points, and so what if I told you like
for relevant?
And then I said, Okay, then I have non-relevant. And we have to figure out like right now, we're saying. you know, imagine, like it's I'll just call them X's
x one x 2 and x 3.
But we represent the points as some kind of feature vectors, right? And so, what are the feature vectors we use to represent our points like here was 2 dimensions
X and Y. What was the X axis? What was the y axis?
It could be anything. But when we talked about in our lyric what the ranking factors we like for our rare search engine. What were the factors that involved us?
Oh, yeah, remember all this stuff
or
all these factors.
So when we talk about like, what are the dimensions? They're just these features.
Okay. so let me go back here.
So we talk about like you know.
What are the dimensions?
What are the dimensions? IE. What are the axes?
They're just our features. What are our features? Well. it could be the
cosine of the query in the document. It could be the number of clicks
this week. It could be Dot, Dot, whatever those features are that we decided we liked.
and we don't just have to have 2. Would you have a lot we have like a hundred we have like a thousand.
So right. Now assume we just had 4.
So the idea would be like.
this very document here might be represented by 1 3 0 0.
Because maybe
I'm just using simple numbers to make my life easier. But you can imagine one of those is the cosine one is the popularity. One is the whatever
do you guys get this. it's not 2D. It's now fourd.
And so let's say this, this one right here is 2, 4, 1 0, and this one is 3
to
2 0, okay? And on the red side.
let's say we have x 4 x 5
like 6.
So imagine here instead, we have different.
Doc, different query, document pairs. And the features are 0 1, 2, 2, 1, 1, 2, 3, and whatever. Okay?
And so the idea now is if we want to find, like, what is the centroid of these guys? And so I can represent the centroid of the of the relevant. And let's say, the
the centroid of the non. What am I doing here?
And the I got too many colors.
Why did I use red.
anyway? I'll just do. I'll do black them that's cool
of the non relevant. Well.
the centroid is just the like component-wise F,
so what is the average of the first dimension?
Excuse me.
And to be clear
centroids.
component. wise average.
Yeah, so what is okay, what is the second dimension? 3. What is the third dimension?
Hey? It worked out.
Sec. Let's do this 10, man.
one, third. two-thirds.
2 and a third.
2 and 2 thirds. Why'd I do that?
Because you don't know
I would.
because I'm I'm a madman.
I like to live on the edge.
I like to use fractions.
those are our centroids. So when we talk about training our ratio model. We just did. That, was it.
It's not sophisticated. If you took machine learning, you did all these sophisticated ways to learn like model parameters. And all this stuff.
all we did here was, say, take all of my autism examples, finding one white average while it's centroid done
for the non-relevant
find the component? Why the average color of the centroid done so. This green and this red are in essence.
this green and this red
right? Because this green here is the component-wise average of the x's and the y's of the greens. Same thing for the red. But now we're doing it in 4 dimensions. So it's hard to visualize
cool. So
we just trained our Roshio model. Yeah.
And now here's the big kicker.
when hear what I was saying, Well, what is this thing?
Is it a red or a green? We had to calculate some distance. So same deal. In fact.
I'm gonna need an extra slide here, so let's copy.
Gotta add a page.
I got a whoops.
I got a copy and I got a
hmm good. Good.
I got a copy.
and I got a paste. Yes.
so
we have now trained our method. And so what are 2 things? 2, 3, 1, 0,
2, 3, 1 0. And what was our other one?
One third, too? Oh, man.
one, third, 2 thirds. 2, and a third 2 and 2 thirds.
So this is the mu of the non.
And this is the mu of the of the relevant. You know what I'm doing.
This is the view of the relevant. And so now the issue is, here comes a new.
a new point. And the new point is, let's say.
I'll make my life easier. Okay, 2, 3, 1, one.
So now, the question is.
is this thing relevant or not? So this is the testing testing phase
in your mind. You can think of the application phase. We've learned the ratio algorithm. Now let's go apply it. So what do we need to do? We need to measure
the distance between the new
and the Mu row. and then we need to do the distance between the new
and the mu not rel.
And so we got to measure the distance. So why don't we use Euclidean distance.
and so the distance between the new and the irrelevant is going to be the square root of
2 minus 2 squared, plus
3 minus 3 squared plus one minus one squared plus 0 minus one squared. It's warm
worked. It's very simple. Let's do the other one. It is
2 minus a third
one and 2 thirds squared, plus
3, yeah, 3, minus 2 thirds, 2 and a 3 squared, plus
1 one and one minus wait one and 2 thirds squared, plus
morning
to something. Whatever it is, it's much bigger than one.
Okay?
And so then we would say, What is this new point? Is it gonna be a green or a red, and it's like green?
So we'll say. oops. we will say, question, mark.
question, mark, what are you?
We will color it in green because it was closer to the Mu rel green versus the mu non-rel red
aka. Visually. What do we do? We had a bunch of points, we found their centroids. We had a new point. We measure the distance. It was small, the distance here was big, so we labeled it one versus the other.
Cool
questions concerns freak out over ratio classification.
is it? Yeah, please, yeah.
there's a contact there. So something called a roast relevance feedback, which is a related idea
which we talk about later, this semester, yes.
general classification.
And so imagine a world. Imagine, if you will, a world
where you could have. You can imagine this world right
now, we have 3 classes. and so maybe not for our contacts we're talking about. But you could imagine, like.
am I gonna find this album like? you know, could be, is this album? What topic does this relate to? Is it
you can imagine this is like a Pop! This is metal. and and this is, let's say, hip, hop! Right.
And so the idea is, you could say, Oh, I have a new album. Which bucket should I put it in? Put it in for promoting on my website? Okay. so that would be, now, this is a generalization of ratio to multi
class
classification. But the same idea
where we would find the centroid.
the centroid.
the centroid. And now, when we have a new document, let's say here or a new whatever, we would measure
all like 3 distances. Yeah, right?
Yeah.
Me, too. If we ever have time.
When we get a tie, the distance exactly the same.
Any ideas
universal, simple.
Let me say.
let me show you something else. If you go back to our original picture.
Remember this this picture, we said, Oh, okay. we're gonna draw a line.
right or not a line, but a plane, not a plane, but a hyperplane.
the separates the linear separator. It turns out ratio also finds a linear separator, because
if you looked at the green versus the red.
you found every point that was equal distance to both of them.
You would form the line that separates them.
and so it turns out. ratio is going to find a line that separates both of them.
Okay, let me show you a quick drawback of grocio
problem
everyone gets when I wrote all these greens and reds. These are just points from our training data.
So help me find the centroid of the Reds. Oh, it's right here.
Where is the centroid of the green?
Oh, no? Oh, no. So now you know.
Question Mark, here's a new thing. Where are you?
So you need you give me like what's what's happening here? What what does this mean on both sides?
It's it's like Bimodal. It's like, there's. there's some structure about the way we've represented this, that somehow it's not like one compact
class.
But for whatever reason, it's kind of spread out. Can you imagine how this could ever happen
like a real example where something like this would happen.
Yeah, I was older. That came back now. Yeah.
So you can have some kind of like natural like, there's some natural factor in how the thing was generated. It's older now it's newer. And so, for whatever reason, there was a change in the structure of the songs or whatever. And now they're they're both the same style, but they're now somehow different from each other.
Sure.
If you think about this in the context of like
let's say, spam. This is always easy example. Like spam emails you get at your tammu account.
So say, let's say these are the messages. These are. But the method is you want to read. Let me rephrase that. These are the spam messages. Okay. these are messages you want to read. Right? These are messages from your instructors.
Yes, ma'am.
and these are messages from your friends.
and
not a great example. Your friends don't email you? I know right. I know these are messages from your maybe your parents. I don't know. Do you give what I'm saying? Let me change this up.
Text messages on your phone right?
These are text from your your parents. These are text from your friends.
and these are your text for everybody else.
So the hope is, in fact, from your friends, and your parents
probably don't share a lot in common. I think about some of the auto corrects we talked about before. I hope I hope you're not texting your parents duck this or duck that
they don't like that. Okay.
also, let me ask you this, this is something I face
I know so sort of. I didn't grow up texting right? So it's like, you know, a.
you know.
I merely adopted it. You you were born into it totally. It's true it's true.
And so like, I know, there's like texting anxiety and stuff. Is that right? Like, if someone does or doesn't respond fast enough.
all I know is that with my kids they don't. They? Oftentimes they don't respond.
I'm like, Hey, man, I wasn't so much nothing
like I saw it. Whatever. What am I supposed to do? If you ask me a direct question.
hey? Do you need this? Do you want that? They'll respond?
Is that something? Is it just your parents? They're no idea.
It's like, I don't know what is the trick to get responses.
It's impossible.
Add a question, mark. It's like something for them to build off. You gotta give them something to work with. It's kind of like, improv. Yes, and always give them something to respond to.
Okay? Fair enough.
Okay, that was ratio. Let's do one other quick one. Okay.
we're gonna learn. Learn 2 classifiers today. The second one is k nearest neighbors. Kenya's care nearest neighbor's training.
Not really anything. Huh? What? Who
but testing for unlabeled testing data, assign these point to the majority class of the K closest points.
Okay. that makes sense. Let me draw a quick picture.
Hello!
No, no, no, not again, not again. And and and
a that's
turbulent
right now. It's about my pencil.
Yes, okay.
same deal. I have
something like this.
And then I have a new point. But the new point is this one.
And so the idea. I don't learn a centroid. I don't learn a lie. I don't do anything like that. We do need to know. K. Let's say K. Is equal to 3.
So we say, find this guy's 3 nearest neighbors. Who are they.
those 3? They're all red
red done. Let's say I had this point.
There we go. So what do we have here? So we have
2 green.
one red.
So what are you? Green?
That's k nearest neighbors.
In other words, I'm not looking from some overall structure. I'm only looking for some local structure.
So what's nice about that? Come back here.
You can imagine some nonlinear case
so you can imagine some nonlinear, because now imagine, here we were. Gonna do our ratio. Where would Roche find in green and central aid
was here, and then the red, maybe here.
But it's kind of confused, because here's all kind of jumbled. So in this case we might prefer, because our features are not cleanly separating the 2 classes. If we did, there might be more like local structure. So he did, King, nearest neighbors. And he said, like, if you're this point.
You're probably red. If you're at this point, you're probably green. If you're right in here making green.
The idea is, it's not like a clean line separating them.
But it's like these little local patchwork of decision boundaries.
Does this make sense? So does everyone understand why there's no training here?
Because I don't do anything ahead of time. You just give me the new thing.
For example. I don't learn anything. You just say, Okay, what are you now? At test time I have to go find my nearest neighbors done. Okay. this is key. And now. so
as a practical matter.
how do we like? I did this example with Rosio right where I showed you
this business. I found the centroid. I did the distance calculation.
I'm not going to do that for you right now. But if you were to do that.
what would you do? You have this point? What do you need to do now but the distance to every other point.
And I just take this.
Yeah.
I have all the distances. Second point, which is what was my cave.
Why? Because we just chose it.
Notice. I often I'll pick an odd number. Why odd? So I don't have ties. but should it be take, you can do one nearest neighbors. Why would I do 3 nearest neighbors versus 11 nearest neighbors. Does it matter?
Let me give you another example.
I'm not sure I can do this just right, but
if I said.
I have a new point. I said, what are you? One near Sabers Green? I said, what are you 3 years named Red Red? I said, what are you? 5 nearest neighbors?
Maybe still red. But I said, 7 nearest neighbors green. The choice of K affects the decision right? So K.
Hey, is a hyperparameter.
Okay. that
it's a parameter that guides our algorithm
that we're kind of humanly selecting. Okay? But we don't know the right choice. So
freeze it there for a second
is Rosio. A linear classifier
is Knn, a linear classifier.
This is linear.
This is Nonlinear. When I say linear. I mean. you know, we can find a line or a hyperplane.
hey? Nearest neighbors? We don't. It's just like funky thing.
So they're linear versus nonlinear classifiers. Okay, that's the first thing.
Secondly, for Key. And how do we choose? K,
right now? We're just guessing.
So anytime you have hyperparameter.
This is why we do not just training versus testing, we do training plus what we call validation.
So in practice, and you may get to do this a little bit on the next homework is this is all label data set, all label. We know the answers.
the test. We hide away. We put it in the lock box
where our model, our algorithm can never see it. It's hidden away because we do not want any leakage
like, if you were here last week when I was looking here, some people were raising their hands over there.
and I learned their labels. Then I compete.
So the test set is in a log box, totally separate.
Okay, but we can do is we can pretend we have our own test set. We call it validation.
And so we take all the training and we chunk it up into our training plus a little leftover validation.
And so if we're trying to figure out like what is the right choice of K,
we can play games where we say, well, let me try this. Let me train here and test here with K equals
3.
Let me train here and test, and K equals 7.
And so the idea is you can try to find your best choice of K.
By testing on the validation set once you're happy, you're like, oh, I got a good K.
Only then, do you? Then say, now, let me go. Try it on my actual test set. See what we get, and we're good.
but I can't cheat and then go back and change it again.
Okay.
so the idea of this is like the the validation set is allowing us to, for example.
eg. choose K, 4 KNN, okay. Now.
you know 2 classifiers.
Rosio K. And M.
They're in your back pocket.
They're under utility bills.
To continue the better metaphor.
If you want to.
you can go take machine learning, and you can learn about decision tree. You can learn about perceptron. You can learn about support vector machine.
all different methods, but still doing the same idea, which is, you give me a bunch of data, learn some way to separate them.
Done. Now, I apply to new stuff. Okay? Same idea. There's different kind of approaches to doing. But we have 2 classifiers now
back to our scenario.
we said, given the training set of query, Doc relevance triples.
So remember the whole idea there is. You give me a query. You give me a document.
I know if it's positive or negative, right? We learn a model F, how
we can use ratio or
whatever you like.
Now, classification time given unseen query documents a
apply the function to it.
and then output whatever Roshio tells us. relevant or not. we did
okay. Now.
So we did this example, before begin.
One more. One more thing. Learn. This model. F, of course.
depends on
features
we choose.
So all of this is not just the algorithm ratio.
But is the representation that we choose? What is the representation? Well, do we use cosine popularity number of clicks, all those features?
Okay?
So what I'm hiding from you for those of you who took machine learning. How do you deal with that?
I give you a thousand features
which could be all of them.
Maybe you have some trick some way you can figure out where the right feature set.
Yeah. Pca principal component analysis.
There's lots of tricks. Okay? So just go. II don't want you needing this class.
and we're doing your technical energy. I say, oh, you know, machine learning, you're like, Hi, let's do this.
And they're saying, Yeah, how do you choose your feature? And email, yeah, we can talk.
Oh, by the way, this phone down, no one here has ever put a phone down? All right? You have
yeah, reading the real crummy home thing that you plucked in.
I'm giving it away for now
wasn't also correct
exactly what
he beats everybody, and then does this.
We saw he played Jokovic. Jokovic destroys him, and Jokovic is like in his face.
Okay, I take it back. You know what a phone is.
Has anyone here ever
at a museum?
Your grandparents had one.
Where did your grandparents live
it
in Florida, where they keep all the rotary phones.
Tell you what one of the most satisfying sounds of your life
also, if you're don't say one
like where the numbers more so like my cell phone would be like versus like 1, 2, 3,
yes.
the board
devastating head office.
But like about the save Icon, you never! You never use an but you have them as like as like. Your parents, my dad added, like I
what a 90 stab on that right? I love it. Okay.
has anyone here ever seen a punch card
in a museum? Right? They're pretty cool. That's how you used to program. You feed in the Punch card.
Think about that!
Alright enough history. This is ridiculous. Talk about phones in this class, but the point is. if you're trying to figure out the features. There is all like sub area, but it's been weeks talking about which is pure feature engineering, finding new features, constructing them, feature, selection.
and that is to identify the best features, all this kind of stuff beyond that. Think about this.
What are we doing here? We're depending on the features we choose. We're trying to pick a representation for that.
Have anyone heard of this area called representation learning?
So it turns out, there's this like incredibly influential venue called Iclear, the International Conference Conference on Learning representations.
And it's all about. No, no. Why are we using machine learning to apply our features? Let's use machine learning to learn how to represent the things.
It's like a meta step.
Okay? And so if you continue down this machine learning pathway and you get into deep learning. One of the great benefits is not just. You have a cool algorithm like ratio or sophisticated.
but you actually learn good representations
from your raw inputs. And so this is one of the exciting things right now, which is, if you came back. looks like this is the last one. Right? This, if you. If you were looking, I wanna reach you
if you're looking at
all of this stuff, and you're like, what are the right features and so forth.
A current style would be.
I don't care. Shove it all in and let the machine learn the right representations from this, either using these surface level representations or learning the intermediate representations
and using those to drive our algorithms. Okay? So that's the cool thing. Just a little preview for you. Now, back to where we were
blahdy, blahdy blah blah blah. Okay? So last bit, I want to show you you do not have to read this paper. Okay, I just want to show you as of 2,004. This was state of the art.
What this guy did was, he basically said.
he said, let me try machine learning verse traditional.
Okay, that's it, he said. I'm gonna do machine learning versus traditional. And the big takeaway. if you go read these details.
So for example.
that's machine learning.
that is, let's say, traditional
machine learning loses.
It's close machine learning loses machine learning loses.
Huh?
Let's look at this one.
He says, well, how about this one machine learning loses?
Oh, we're better. We're slightly better.
So I'm gonna leave that with.
I spent all this time to tell you this story about machine learning is so powerful.
Here's someone using something more powerful than ratio. and he's still a big loser.
So I want you to maul why that happened. What does it mean? We come back on Wednesday. We're going to solve that puzzle.
See you, then.
of that other week.

Yeah.
I may have done something goofy here.
The
so you guys let me know that. Yeah, we're gonna talk about the homework. A bunch for this to to begin with here, because I can feel the the freaking out.
Campbell II, Nicholus D
I can't even the zoom. I do not see the slides
recording. It's progress.
Say, number
no audio.
Yeah, don't do that, guys
they want.
Now, I think it's in the past. Oops.
I do. Alright. Okay, cool. So thanks everybody. Let's talk the homework real quick. I got kind of feeling
getting to some people. So what I would say is this a couple of things one there's lots of stuff on slack. Please take a look there. Lots of examples, etc., etc. You can usually late days, I know.
I would even be willing to extend the
Monday. People need more time.
you don't know.
So in terms of kind of rating and expectations and personal credit and stuff like that. So, for example, this parsing, this right, the main thing there is.
we want you to sort of feel some of the audiences in the pain of dealing with real data. And so I heard some folks saying, Well, you know, I was trying to remove the brackets within the brackets. Sometimes their brackets, or sometimes there's parentheses. And so if you don't do a perfect job on that, I don't really care.
Okay. I don't really hear him.
If you're looking at the top 5 tokens, you get overall, and you're totally different from someone else. I do care about that. Okay, but if it's off by one, or you parse it slightly differently, not a big deal. Okay, it's not like we have the kind of correct answer. And we're just gonna check. Did you hit the exact numbers? Otherwise get 0? No, that's not gonna happen. We are looking to see that you, you know, approach the problem the right way.
But when you get to things like
the building your index right? It should build your index in a reasonable amount of time. Okay.
we have some queries there where we're gonna ask. You know, you would have the right to use other. Okay, yeah. But 10 years. We're not gonna try to break your system right and ask these real edge cases and break it.
But you know, we have like, never know, or something like this. We might try, you know, love or high there, or some other way, or some other. Query, just to see what happens. And so make sure that, like
your thing actually works. Now, in terms of like those tfids, cosines and all that
same deal, you're gonna want to hit those numbers
ballpark. If they're off a little bit I wouldn't freak out. But if you're way way off sometimes you'll realize, hey, I wasn't dividing by the right thing.
So you wanna check? Hey, guys, what was your numerator? What was your denominator?
And you're fine. If you're on slack, I want you to drill down that level of detail, or say, Hey, forever! In this document, this is the score I'm getting. What are you guys getting?
That stuff is all good but if it's all by a little bit, I wouldn't freak out too much
any other questions concerns clarifications about homework.
Okay.
yes. Yeah. What base are we using for our logs? Were you guys using 2? That's good. Yeah, it's just easier if everyone kind of agrees because we didn't mentioned in the examples. We've done base 2, but it, you know, oftentimes base 10. But if you guys are all using base 2, that's fine.
it it we don't really care. What I will say. This is sort of for the future homeworks. So we're kind of thinking like in total of the plan was to kind of have around 4 homeworks.
We may do our next homework baby more of a less of a programming, intensive homework, and more of a
kind of by hand work, some stuff out concept application cause we have again, we have a midterm coming up 3 weeks. And so, yeah, I did want a homework where we talk about indigg, you know, this last couple of days. And you guys all sat back there. You're like, Yeah, I got it right. Whatever you know, I saw it. We may give you a homework where you really have the Indc
like.
really do it. And you really understand? Okay.
how does that sound?
Okay, we will still have more programming assignments. but we may kind of structure them a little differently, so that we want everyone to kind of like. Come along with us and have a good experience, so
keep your eyes painted any other they wanted his monologue, or
this is our hypersphere, if you like a therapy session. Anything you want to talk about.
Okay.
okay.
Remember, last time
some of you were here last time I was here.
We talked a little bit about a B testing.
I just wanted to show you an example, cause I was Googling around for that the other night. and I found this really nice Netflix blog about ab testing.
And I just want to kind of give you an example. Just it's also a nice blog just to read about kind of cool, a little light.
There's this whole idea of how do we know if the changes that we've implemented work or not?
And so a lot of the stuff we talked about so far in Dcg precision recall, that's all. Typically in some offline scenario
AV testing is online. So if you go to Netflix. Everyone, 99% of people see the standard box art, right?
But you may say, what if we flipped all the box art upside down hard to see. But all of it is upside down, right? So you can imagine, like 99% of people see this
1% that people see this you can think about. This is like the control. And this is like the experiment
or the test.
So the idea is, we're gonna go run this for one, we're gonna divert 1% of our traffic to see this. And then we're gonna measure something.
And so here comes all the Netflix members again, most people go up here to the control one. See the test? You don't do it. 50, 50. Why do you not do it? 50 50
people freak out. And these 50 never come back to Netflix again. Right? So you just destroyed your business. So when I said 99%, I mean, really, it's like 99.9 9. This is really 0 point 0 0 1.
They go down here. So the idea is just a very small fraction of seamless
and like I mentioned last time. If you ever use spotify. If you use
Google, if you use Netflix Youtube, they're doing these like point OO, one test all the time, and many times it's kind of not even perceptible to you. Maybe the font got a little bit bigger. Maybe they change the color. Maybe they reorganize something on the right. Okay? And they're just
that's what they do. And they're constantly doing this testing.
And so, for example, if we were to run this A B test. Suppose this is some engagement metric.
We have to agree on what we mean by engagement, but it might be some click through, so on and so forth. And so if we run it. And so this is the original.
and this is the flipped.
What do we conclude? We conclude that moving to upside down box art gives us a big premium in terms of engagement, so we should deploy the flipped Boxart correct?
Yes.
Oh, what's our what's our X axis
time?
Right?
Well, could anything else explain why engagement is higher here than it is here? It's summer break.
It's
a new movie came out. Stranger things season whatever it came out.
something squid, game, part 2 came out.
Okay. So the point here is. if you do it sequentially, you're trying to control for only the flipped
change.
And instead, we're not controlling for only flips. Instead, we're saying it could be flipped or
the whole ecosystem changed. A new movie came out, a new interface came out. Something changed. So this would be, you know, don't do this. This is bad. Okay? Instead, right?
The idea is you would want to do it together.
Right? So now you can say, Oh, okay. So yeah, engagement for the original was always higher.
And then something happened.
All of engagement went up. We're still blue is always higher. Okay.
that's the main thing is you want to control for everything but
the one change you make. Okay?
So when you go back and you think about this.
but we're trying to control for everything. What's the fundamental difference between this one. And this one
that we we kind of we, how do we control that? So yeah, we change the interface.
But necessarily the people are just
right. Cause. There's no fraction of people seeing this one and some fraction people seen this one. So how would you if you were implementing this? How would you drive people to one versus the other.
You do it randomly.
Okay, even if you do it randomly, sometimes imagine more like North Americans down here and less Americans go here. So you could have some slight differences. Yeah, you might want to find some representative population. So yeah, you might want to say
North America and others I will sample.
I'll just really stratify sampling, or from different states. To make sure you sort of maintain some ratios. Another point over here. Yeah, I guess, like more like an updated
well, if you opt in, what's the difference between opt-in and opt out people. Rachel Poppin, or we're weird. Beta version.
Yeah. So I think the idea there is like, if you're the kind of person who opts in on a Netflix interface, you're probably not representative at all at all of like the average Netflix person. That's another concern is that the people who opt in are somehow fundamentally different.
Yeah.
so just keep this in mind, you want to be very, very careful, because we don't want to do is decide like it turns out this one was way better. but because somehow the distribution of people that we picked are fundamentally different.
That's what happened. So anyway, I just kind of those out there. This happens all the time. It's super cool.
we're gonna have to do much more about it. That's really neat topic. So check that out. Okay.
now, we're gonna move to learning to rank. Ak, a ML. Machine learning for ranking. Today, we're going to go through high level conceptual distance.
If I had the energy this weekend next week we're gonna come back and we're gonna go places I've never been before, which is, we're gonna go into the meetings
or enough a ranking in such a way that even if you've never taken an off course, even if you never even heard those words.
So that's my dream.
you believe. Do you believe in my dream. I'm not sure I believe in my dream.
but I believe you know what I believe in. I believe in myself, and not only that I believe in.
I believe in you.
Let's give it up. Let's give it out.
Do I do? I believe in the football team we play this weekend against the easy team. If if if it's a close game.
if it's a close game, does Jimbo survive to Monday?
can you imagine having a guaranteed contract
to eat for anything.
Can you imagine like you're gonna get paid like a million bucks a year? Oh, I mean, he's getting paid way more than that. But like
you get paid 7 4.5 million a year. And it doesn't matter what you do
for the 10 business.
It's amazing. Okay.
so let's go back in time. When we started this whole ranking discussion we had a scoring function. We took a query and a document. That was it. And on your homework. You're doing like a cosign right? We also have one where you're doing some summation of some Tsf scores or Tf-idf scores.
or we didn't really talk about it. There's there's lots of ways we could construct that scoring function. Okay?
Easy.
Easy. Ish, okay.
But remember, when we were talking about ranking factors for different kinds of platforms like our algorithms. And we did a big brainstorming session. And people are throwing up their like. Well, you know, we could consider, you know how many openings are on Linkedin like, how close is it to me? And what's the salary? Okay? Or for the cab's algorithms? You know, we might care about like the keywords match. What language is it in? Where is it? You know how popular is it? Or for Youtube, all these factors? So we know all these factors really go in.
But our function right now is only cosine. like. So in our word is is something. All we have is that
some sort of key word overlap. That's really all we measure. We have all these other factors we'd like to consider. So
when you think about those features. We typically put them into 330, man.
we put them into 3 buckets. Query, independent.
static features. They only depend on the document when I say static. I mean, you can pre analyze the doc
and pre-calculate that
whatever that figure is.
examples of query, independent factors for cavs algorithms. But they only depend on the document. The lyrics
give me an example.
Number of sales. and last month.
now that changes, you understand, like the number of sales changes of over time, of course, but the idea is for this document. It doesn't matter what the query is. This document has had a number of sales in the last month or the last year some measure of popularity.
What the heck
in the last month? How about another example of a static feature of a document
that does not depend on the query.
The development is created.
Yeah, we call it maybe freshness or date. It could even be like the number of times.
The word tailor is in the dock.
It doesn't depend on the query. By the way, this Taylor Swift, concert movie. it's gonna yeah. There's a movie. It's already done 65 million dollars free sales.
It's gonna be a big hit like a Barbie level that
I'll be there. Maybe I'll see you, too. Okay? Query dependence. This is both the document and the query. This is like
Cosign. It's a factor. We need to know the query and the document, and then we can calculate it
or our Bm, 25 between the query and the document. Okay, dot dot dot. Then you have query level. Those are only factors that depend on the query. This is not as exciting, but it could be like, you know, the number of words
in the query.
and the issue is, it may not really affect our ranking. But it's another factor we might consider, okay. So when we think about moving from our single factor like cosine. We want to go to many factors.
And so what you'll notice here now our scores is big on formula. We had the cosine. This is a query dependent. So Qnd.
we have
query, independent or static feature, like number of views in the last day, because it only depends on documents. and then maybe have, like the number of words, the queries, query, only we have all of these factors.
So just to keep in mind what I'm saying is like these are
query independent.
We can precalculate these
right? These are query dependent.
We have to calculate at at query time.
And then this is like, Yeah. that's just only on the query itself.
So just keep that in mind like these, these are the guys we pre compute at period time. We have to calculate these guys.
Now, of course. what we're gonna do is not just sort of add these up. But really, we're gonna do is we're gonna have some kind of like
parameters that guide how we put these things together. So maybe you know a one all these A's, you know, they could all, you know, a one may be 0 point 4, maybe a 2 is 0 point 1 and a 3 is something.
And the idea is we have some scoring function where we're gonna weigh all these different factors kind of by their significance. Right?
And so we're gonna put all these factors together. Now, we have a scoring function. Okay? So let's do, let's do an example.
So suppose our scoring function is, this is a times the cosine plus this, a 2 times the popularity. And so we had to agree on how we measured popularity. And we can decide that. But the idea is this, like, we have Doc one.
and we have Doc 2. And here comes the query. And so you can imagine the cosine here could be 0 point 8 and 0 point 4.
But then we also have the popularity. And so we can just maybe some sort of normalized popularity.
And so we'll say, the popularity of this one is 0 point 2.
And this one is 0 point 7. Right? So now the whole thing is is like, well, how do we put these things together? Right? So it depends on what those weights are. And so you may decide that we're going to overweight the cosign so it could be a one is, you know.
0 point 9, and let's say a 2 is 0 point 1. So then, the score of the query in document one is whatever that is point 9 times point 8. What is that point 7 2
plus point one times point 2 point O, 2. So 0 point 7 4. I probably did that wrong. This one is what point 9 times point 4.3 6
plus 0 point 0 7 0 point 4 3. And now we have a ranked order. D. One, the D 2
right? But you could always flip these around, and it would change the rank order right? Because you may decide. Popularity is more important than cosine. Okay.
But the point here is, you can basically cook up these functions. And in fact, that's what people did through the 2,000
into the 2,000. These. This is all hand tuned. not always. But
in fact, there was an edict at Google. There was a high-flying executive who said, We do not do machine learning
at Google in the 2,000 in your lifetime.
you said, you know what they called it. We do algorithmic ranking.
Okay? Scared of using that phrase machine learning. We'll talk about maybe next week. Why, someone was scared of using the word machine learning because it's true, didn't want to use it.
Okay. And so the idea is.
if we're trying to cook up this formula
for a long time, this would all be hand-tuned.
like we literally pick. Hey? I tried a one as like a point 8
seems pretty good. What do you think? That's pretty good, and you would go to evaluation. Maybe we could try it, we would go pick some different numbers.
We can do our offline evaluation, see? Which one gave, gave gave us our best in Dcg. Feel confident we go deploy it. see what happens looks pretty good.
Okay? Then, what happens is we're constantly creating new features. This is called feature engineering.
And so what would happen again if you were working at Google in the mid 2 thousands is you would sit there and you go. Hey? You know what?
There's a thing called like social media
people are are like, they're like posting like links and stuff on social media.
So what if we counted like all the links on social media to different web pages. Now be a new factor. So people are talking about your web page. We would give it more score. So you'd say, Okay, I'm gonna go measure that. And you would cook up like a new.
a new feature, a new factor. You would go cook it up. And you would say, well, this is our current banker. Now we're gonna add a 8 for this, like social media. popularity.
Okay, well, now we gotta go hand tune. How do we change with the A 8 factor? And then we go deploy it. And hopefully, it works. Okay.
this is cool. We can do this.
Okay, people really did this. So feature engineering, always cooking up new features and then putting it all together typically by hand. which again. Seems like madness. Now imagine we saw, like the index
when we talk about started this discussion, the index, the Russian search engine. They had, like 2,000 features.
So imagine your job is to manage the 2,000 little parameters that combine them.
That'd be a fun life right? And then something breaks, and you're like, Oh, crap! I gotta go change all this 2,000 numbers. Right?
This is what people really did.
Okay. But you're smart.
So we don't do this. We just do. Ml, we learn the rank Earth
ml, machine learning. We're gonna do some sort of machine learning method. That's gonna figure out how to combine those features. I'm not gonna do it.
I let the computer do it for me. This is the dream.
Okay? So
now we do our high level conceptual introduction to machine learning. So I'm assuming you don't know anything about.
Okay.
So I'm gonna do set you up regarding Ml, of course, this will kind of refreshing, and then we'll go in a different direction. We'll talk about topics you probably haven't talked about.
If you've never heard of this stuff, at least you can walk out of here and say, Yeah, I know it's yeah. I'm a little bit.
There are lots of tasks.
regression classification, multi-class classification ranking ranking sounds. That's something we do
ironically, we're not going to do that. But yeah, I know, right? Regression.
this is a machine learning task. Okay.
we try to predict a real value like a number. Okay. the input is some X,
we have some function FO, some magic function. That's we try to learn that.
Okay, so we take in some x, we apply some function to X and then output some. Y, that's just a number
like a real number. Okay.
so examples of things that we would take, and we would want to put a number on ideas.
Let me do it like this. I would like to be able to take as X
your homework one.
I don't. I don't want to read your homework one.
but I apply my function, and it outputs a number
98. Yay. So, yeah, how about homework grading?
The X's are homeworks. The function puts a grade on it. 98. Okay, over time. So you learned a very simple function, which is a great, it's a real function, right?
F of X equals 98 doesn't matter what X is. I always give out? 98.
Are we back to that Socialism thing? Okay, you're like
another example where you would want to put a number on something.
So
I mean this is not as relevant to you. But if you're gonna buy a house.
how much did I pay for the house, or how much I list the house for so you can imagine, like a house is the X
and the Y would be the price. How much I list the house for.
Okay. this is regression. Okay. this is regression
classification. This is what we're going to focus on. So this is what you want to pay attention to.
We just try to predict a simple yes, no. Again, this is magic function.
You give me some X apply a magic function, and it outputs a negative one or a plus one. So you can think about this as like a no or a yes.
or you know. false and true.
So something where you take it in. I want to know if this is true or not. Right
example in your homework
picking up homework, I could say, didn't the students cheat on this homework. Yes or no.
it analyzes it and says, yes or no. Okay.
I could do separately, not give you a grade, not give you a number. I could say, does this even pass on this one? Yes or no?
Okay.
we showed up on the first date. What I'd like to do is, do a take a picture of you.
the input would be the X and the output would be, will the student pass the class or not? Yes or no.
So anything you can frame is a yes or no question. Think about our mirrors, right?
So we haven't helped.
We looked at an album. What is some label? We might want to put on our album. That's important to us
relatives. It could be relevance. We're gonna get to there in just a second.
you know. You could say, Will this be popular or not? Yes or no.
you can say, is this appropriate for kids or not? Yes or no?
Is this album?
you know, should I feature this album or not? Anything you can put a label on is the classification task.
Okay?
So we gave a bunch of examples there, and I've already lost that. But like, you could think about this like an email
spam or not. you can think about an album.
appropriate
or kids or not.
you could think about homework, and it could be
will pass
or not
anything that's like a binary decision. Okay.
fundamental task. It turns out, there's also well, oh, yeah, this is my one joke
from the onion. Long time ago
she declares, 70% of everything's witnesses to be a shame.
So Gertrude.
So she basically is like X. Here's this function that's Gertrude
and X for her ex
is anything. And 79% of the time. Why is a shame?
79, 79%, and not a shame. 21%.
So Gertrude is of is a function. Okay? Yeah. Gertrude, is this function. She outputs shame or not. Okay. You also have multi class classification.
That would be, I'm not putting a binary label on something. But I put it to a class. So I see a picture of this.
I say it's a catch.
So I'm not saying is cat. If it was binary classification, I would say, Is this a cat? Yes or no? Yes.
but it said, What is the set of classes? I'm considering, Mary.
like maybe all animals, or it could be even more general, like all
living or dead things.
which is not just yes or no. But it's like you have to think about it, is it cat, horse.
dog that that that that picnic I have to pick the one.
Okay, so we're not gonna talk about this. Don't worry about this.
Sorry we're not gonna worry about this regression just showing you some new note. Okay.
there's also something called ranking, which ironically, we're also not gonna deal with, even though we're doing ranking right. But ranking is basically like, I give you something in an order 1, 2, 3, 4, and I try to put it permuted into the right order.
And so you might be thinking, wait a minute. This is exactly what we're doing, right? So you could imagine.
you know, you could input all of our all of our albums and a query.
and then F, and then the output would be like, You know, Doc, 100
Doc, 50, doc, 200
da da da, and we could order all of the docs. That seems like we want to do right. And in practice this is exactly what we're trying to do. Okay, I'm sorry. In theory. This is exactly what we're trying to. In practice. It doesn't really work as well. And it's very, very expensive to do so in practice, we're not going to do this specific task, even though it seems like it makes sense. Okay.
though, I mean, there are methods that do this for our sake, don't worry. Okay.
So
we're trying to do focus on classification. How do we put labels on things. Okay?
And so going back to the point earlier, how can we determine if, like a document is relevant or not? That seems like a task that we want to do. So we got to learn that function. F,
the magic function.
we're gonna do some setup where we have some training data. We're gonna learn it. Guess what
you skip this. Guess what we're gonna assume. We've trained our function. F,
okay, just like Gertrude. She was born raised.
She learned about the world, and this is determined. Shame or not shame. She she had to be developed to learn that similarly, we can try to learn some function. We'll do that next week.
Okay. But imagine we've got this magic function. F,
okay. So now, I'm trying to connect all these pieces. And hopefully, you're with me. So to go back to our Ml. Plus ranking the whole like point of what we're trying to do here.
Well, remember, in our offline scenario what we have.
Remember, we have documents, we have queries. Remember, this is for our offline.
This is like our whole offline, evaluation setup
and notice what we have. We have binary.
We had assessments of whether a document and a query
like like the document is relevant to the query, we have this binary assessment.
what is this. It smells near classification something. I don't see the connection yet.
but we have binary assessments, yes or no, relevant or not relevant.
Somehow this smells like
this smells like
classification.
but not exactly something goofy, we gotta we gotta structure this the right way. Okay?
And so the way we're gonna do this is, we're basically gonna do something like this. Suppose I knew ahead of time
I had a bunch of queries, a bunch of documents and their relevance judgments. I know that.
So the idea is, I'm gonna learn from all the labels I've already got. Okay.
So let me do an example. Here.
let's do an example together.
Suppose I'm trying to determine
a good example here.
I want to be appropriate cause we're in hyper spear
across.
It's a good example. So Google.
try to learn something about you.
So I'm gonna learn a function this side of the room.
This is my training data.
right?
So everyone's here to think about
whether you had breakfast this morning.
Okay.
everyone have that in your head. If you had breakfast meetings.
Did this happen? Okay, he did not have reference rate.
So we're doing here. Remember.
apparently our motion. If I can theme.
then I've there's been a leakage. And I'm
this is my offline data. I want to learn from you guys and then apply to my new data.
Okay. So now for those of you who if you had breakfast with your name.
let's do this.
I mean, what's a what's a characteristic thing? Are are you? Who's who's wearing sneakers as well
doing where you're not sneakers?
How about of the of the non breakfast people. Are you guys all wearing sneakers?
So what distinguishes the practice of the non-grants?
I need some features.
So if you okay, so
and
so now I've learned Michael. Now you'll tell me
I'm asking this. Did you say it was last night?
Yes, and he's running sneakers. That means he did not have breakfast. Do you have breakfast? He did not have breakfast
going back there was last night. Yeah.
are you? Very speakers?
I believe it's
I'm in the States.
Sure it doesn't
speaker. So yeah.
you're not okay, very.
I'm gonna say so. Another mistake. Right?
So this is exactly the idea I learned from some methods here or the American people.
Hmm.
and then I applied.
okay.
this is the dream which is hopefully. I can learn which features. Maybe it's like you brush your hair. the depression teeth.
Are you wearing green or not green? Some factors that we think are somehow related? Having breakfast, I learned my function. Then I go over here and I can apply that function to my unknowns and hopefully do a good job. In this case. I did a poor job.
So I may want to go back here and try to learn a new function. New features.
Okay, so we do that over and over and over again. So come back to our context of
of this relevance judgments. Imagine I have a bunch of my training data, queries and documents. And I know if they're relevant or not.
we want to learn a model of F that's really good at saying
the document is relevant for the query. And so you can imagine the function might take into account.
You know the the cosign of the query in the document.
It might take into account the popularity
of the of the document am I taking in all these factors? And it's going to either output relevant were not relevant.
So we we learned some model. We don't know how we did that, but we did it somehow. Okay. now, in practice, we want to go to unseen this half of the room.
not people. But in this case queries and document papers. and we'll say I have a 3.5,000. Is this document relevant or not?
And I go apply my function. And it says, relevant or not
makes sense. Yeah, in practice. Why, you use this classification of the ranking. Yeah, yeah, we're gonna figure that out. So there's a question, which is, why are we doing this classification?
Well, turns out we're not going to do this for very much longer. This is like the simplest way in, okay.
you guys got this questions.
This is machine learning. If you always thought it was so. Fancy, this is all we're doing?
We learn some function. We apply the function.
That's it.
Notice the problem here, which is, suppose I learn my function here.
What's the problem? I know the users.
Okay, what if I learn my option here? I think over there
and then I have right here. And G, this is leakage
some information leakage in practice. This happens
all the time. and not always. But I mean you don't mean it to happen
every day this happens. You're trying to do some kind of fancy model that does whatever. And you get some unbelievable high accuracy.
So anytime you read some paper or see some Google or big company say, we have 99.9 accuracy on some tax.
you say, wait a minute. What happens is as I work my function.
I really was speaking over here.
and so it was achieved.
This happens all the time. I'm not saying it happens on purpose sometimes, just like as you're doing your data split right?
You accidentally, the data gets split happens all the time. Okay, so
I have some. So here's an example.
We have
7 example documents. Okay, I'm sorry. 7 query document pairs. So for this square query, Linux operating system. And then document 37,
we know
it is relevant. Okay?
But this query, penguin logo in this document. 37. It's non-relevant. It doesn't have the logo.
Okay? So again, those are our queries and documents.
There's our relevance judgment. This is our binary thing.
We have 2 features.
just like when we were building our function cosine plus popularity plus whatever. In this case we have 2 features.
Now, what's weird? Here is the cosine is now a feature
right? And in this W. This is like some window.
Don't worry about it specifically a window link. It's like, does the query term occur within a window within the document.
The point is, it's just some other feature.
Okay? So the idea here is, we have 2 features.
We have our judgments.
Okay. Now imagine we were to go plot these.
This one has all of them, I think.
term proximity. So like, imagine we had that. and we had that cosine, and we went and plotted them.
So it's like, okay.
this is hard to plot.
I mean, grab them
very important
copy.
Yeah, those are my numbers. So we have like
oops.
2, 3, 4,
5. What is this?
0 1?
So it's like 3 0.
That's the first 1, 1, 2, 3, 4, 5, 6, 7, 2. Is it? 4. Zoom
3 is at 2,
or is it?
I've exerted?
6 is at
and 7. Is that
okay? So I just plotted all those numbers. That's like a miracle. I just did that. I can't believe that
now of those. Oh, I didn't grab the relevance judgments. Let me grab that to whoops
one.
Yeah. So let's go and back in and say which ones were relevant or not. So yeah, help me understand that. So this first one was 3.0 3 2 that was relevant. The third one is relevant.
5 and 6 are relevant. and then the ones that were non-relevant.
We're 2, 7. I didn't do 4, I guess.
Here's just the point.
Oh, and 4. Thank you.
Okay. okay, cool.
Okay. So we've now plotted all of our points according to these 2 features.
Now, my question is is like, we're trying to learn a function
right? So it turns out, a function here can just be like a line.
Everything on one half is gonna be relevant. Everything on the other half is non relevant. Because what do we kind of learn here which is the bigger this this factor is.
and kind of the lower it is kind of you get a low cosine. but kind of big window.
You're not relevant.
If you have kind of like, I post sign and small window you are. Make sense. So if someone help me draw a line that separates
the the good from the bad, they're relevant to the non-relevance.
Probably.
No.
okay. So it turns out, what's the right line to draw.
So look. there's a line. There's a line. You have a lot of lines that draw that separate them. Do you guys know we call this line right here.
relevant slide boundaries.
This is where you go. Tell your parents we learned about.
It's a separating hyperplane.
It's not a line.
Yeah, that's true. It's in 2 dimensions to line in 3 dimensions. It would be a plane in higher dimensions. It would be hyperplane.
right? So we're showing it in 2 dimensions. But you could imagine. If it generalizes to a thousand features
it would be separating hyperplane. Why do we call it hyperplane? Because again, it's a separate, it's a, it's a plane in high dimension. Why do we call it separating? Because it divides
these guys from those guys. Okay.
so what we've learned here is like a linear separator, a way to separate them.
Okay, so we talk about learning a function. F, there's lots of ways to do it. Taking all these points, and then we cook up and mandate the output of this line. Let's say. now, given that line.
Here's my question to you. Now let's go test this thing. Let's go use it.
So suppose here comes document 8
question mark. Here comes document 9 question mark.
Here's document 10 question mark.
So I ask you now
is a should a be relevant or non-relevant
relevant because he said, It's up here. So okay, how about not not relevant is down here
about 10. It depends depends on which line we use and where we are.
Okay. But that's the idea is we've learned this function that we can now take any query document there and say, relevant or not. Okay. So you see the power of this, which is.
we go train over these
query document pairs, just like I trained over these people. I then can apply to new theory, document bears
or new people and get a label. Sometimes we do right, sometimes we may do wrong. We don't know. Okay, so this example.
they show it with many more.
In this case you'll notice the separating member plane. Notice.
Here's an in a not relevant made it up. Here's an RA. Relevant made below. It turns out sometimes the 2 classes are not what we call
linearly separable. just for whatever reason, the way we represented them.
we can't divide them all perfectly. There will be mistakes.
But guess what? That's that's kind of life. We're gonna make mistakes.
This makes sense.
No.
How do we learn those functions? There's lots of ways.
Okay. at the end of day. My hope is, you believe I give you any features.
Apply this line. Are you above it or below it? I can put a label. I can do a binary classification, I can tell you if you're relevant or not.
So I get a new query. I've never seen it before.
I could go to every document that I know about. And I can apply this function and label. Everyone is relevant or not.
So kind. It's not really ranking. But it kind of helps me
check it out. Okay, folks, good luck with your homework
check slack. We will see you all next week.
Good luck at the game.

Welcome back. It's Wednesday. Your homework is not due.
hey? It's new on Friday Monday. So, anyway.
in terms of your collaboration declaration, remember that was one of the things that goes into the homework you have to tell us, hey? I use sac overflow for this. I use that
so, presumably by default. Everyone can use slack, so you can just mention I use slack. But even if you don't mentioned, it's not that big of a deal like this.
It's sort of an expectation. Of course we can be on there, but anything else. Just make sure you give us a note, and it could just be, hey? I talked to Joe. I talked to Alice, and we all work on this problem, and that's how I figured out as part of it.
Any questions, concerns, comments about homework, collaboration, any other stuff that's going on.
And so just you need to bump the bottom to add a bunch
you can even use Markdown. Is it really fun to read? No, no! Any other questions concerns, comments.
Everyone is good. Hey? Hey? Do you want me to blow your mind for a second. Sure. So today is Wednesday.
Yeah, I know. Right. The thirteenth Friday is the fifteenth. We have a mid term in 3 weeks, I know. Look at his face
so just. I'll just put that on there. This is a little, you know, and I say that
midterm in 3 weeks.
over 3 weeks, you know, it's like 3 weeks and 2 days. So I just mentioned that because I know you're working on the homework great.
So you're gonna see tf, Ivs and cosign on the midterm. So this is helping you towards that. But it also means like as we're talking about Ndcg and precision and recall that stuff is gonna show up, too. So just make sure you're staying on top of it. So for those of you who are watching the videos.
give them, etc., etc. And for those of you not watching the videos are not in class. I can't reach you right now. So I'm kind of talking to the void. But if you have friends who aren't engaged in the class and got keeping up, if there is a midterm which is when, like the
the payment is due.
I'm trying to freak you out.
It's all good. Okay. Are we ready to keep keep cooking?
Anyone have any good news to share or anything interesting to share. wouldn't learn anything in the world of technology.
Iphones finally, have Usbc, which one is Usbc, II have so many the ipad.
So not the little thing, but like little, slightly bigger thing.
But is the phone any different?
It's probably yeah. It's probably like slightly faster.
Hey? Here, here's a fun topic. I got a family of 5, which means right now we're into like 5 iphones, which means I'm constantly like.
I, basically, we've owned every iphone. I'm sick of it. I just want one iphone where stop it. Stop it.
Okay?
So if you, if you're lucky enough. Maybe your parents are paying for your phone plan. Maybe not. They are maybe text them or call them hey? Thanks.
cause, guess what? It's no fun
perfect.
Get off my dime. Good job. Graduate. Okay.
what are we talking about in Dcg, right? This is tricky. All this is, gonna blow your minds talk. There's some Asians, there's logarithms. There's division. I know. What was your mind, right? So we motivated last time today we're gonna do real example and try to hammer home. This is the thing.
If you are really working on any kind of search or recommendation task, if you're really in a company, and you say, like, Hey, how do you evaluate? And they're like, Oh, we use precision and recall you. Laugh at them. You say you don't use Ndcg. They say we don't even know if that is, you laugh in their face. Okay, you gotta know this stuff super super critical.
So first things first, typically, when we think about agency. We think about multiple grades of relevance. Everything we talked about before in precision and recall is just binary. Is it relevant, or is it not?
Now we're in a world typically where we may have multiple grades.
So again, the query, you know, is Frank Sinatra albums.
you know. I give you back.
Taylor Swift. No.
0, not relevant. I'd give you back a frank sinatra album. Absolutely. Maybe I give you a compilation that has someone singing it bright sinatra song. But it's not really frank, sinatra, you might say, what's somewhat relevant, or maybe really relevant depends on how you create it. Yeah, it is like degrees of relevance.
Right?
So one thing you immediately should be thinking to yourself, which is, where do those come from?
Where do they come from?
Or are Schmucks like you and me sitting around
greeting
query, document and putting a number. So you thought it was hard to just say, relevant or not. Now you got to distinguish 0 1, 2, 3.
Okay, so it turns out.
And this may show up on a later homework. There's a good example of this.
Microsoft put out something called Lead tour will mention that probably on Friday, or coming back next week.
which is a big data set where they actually spent the time to build a big data set.
So just giving you the data is a huge benefit for us to try to figure out how to build our rankers and how to do our Ndcg, so the the idea is like.
you know.
this is muto expensive major pain. Okay, so if you can develop a really good data set that has these levels of relevance.
Now, we're gonna we're gonna squeeze that thing for a long time.
No.
So Nbc, oh, wait a minute. Division. We've got. Okay. So first of all, P, that's just like
the rank. you know. Eg, you know, if you did it like, if P. Equals 5, that would be in Dcg. At the top.
5.
If P. Is a 10, you're calculating Ndcg over the top 10.
This is the regular Dcg, and this is some ideal. Dcg. we'll talk about them in just a moment. So the idea is we're gonna calculate some. Dcg.
we're going to divide it by some ideal Dcg, and that gives us then our normalized.
So we're normalizing the Dcg over the ideal. Dcg, okay, it's division. It doesn't really make a whole lot of sense. Let's look at the what is the formula for? Dcg, okay.
Hi, Karamba, okay, so dcg, there's that. P again. So what are we doing here? Right? Okay, we're going to loop
from rank
loop, iterate, rank position. One
shouldn't say, loop. We're gonna
we're going to iterate
to rank
position. P,
eg, 5 or 10, or whatever you decide. Right? So we're gonna do this thing.
We're on the top. This is basically kind of in in the intuition is like, these are kind of like the you know, the points you you earn. This is the game.
This is the discount.
Okay, we're gonna work all this together. So we talk about Ndcg normalized, discounted, cumulative.
This is the cumulative. This is the adding.
So we normalize on the previous slide. We discount the cumulative gain.
So what are we doing on the top? We're basically saying that rel, I,
that's like the relevance level.
So if something is super relevant. This is like a big number like we just decided on the previous slide. We said, relevance, these are these are our.
This is REL, 0, REL, one REL. These are our relevance degrees. And so basically, we're saying, but if something that's not relevant, it gets 0.
So it's 2 to the 0
one minus 1 0.
If we said it was like 3 is like super relevant. We're saying, you get 2 to the 3, 8, minus maybe 7 points.
Okay? So the higher the relevance, the more points you are on, the bigger the game.
Okay?
And then what are we doing here? We're dividing by the log of now what is I again?
That's the rank position.
So let's just do this for a second over here. So like log base 2. If I was in the first position
versus the second position
versus.
you know, let's do the
120 seventh position. So this is like how much the discount factor is. So the idea here is basically saying
you got in points because you were super relevant.
But now, based on where you are on the ranked list. We're gonna maybe like discount. How many points you get.
because the higher you are, the better the lower you are, the worse. So if you give me an awesome document low, we're going to penalize you a lot more than if it's an awesome document high.
So the penalty for being. In first place.
log base 2 of one plus one. What is this? 2 log base? 2 of 2 is
one. So we're basically saying is, if you're in first place, you get 8 points. We divided by one. You're still 8 points. You're still awesome.
What if you're log base? 2.
Here would be 2. Right.
I can't do this one. This is. you know, and what is log base, 2 of 128
9.
It's not.
So we're saying is.
if you were in first position, you get 8 points divided by 1 8 points.
I can't do this one, because I can't, you know, do the log base 2 or 3, but, like, if you were in the third position. you only get half as many points.
If you're way down at the 120 seventh position. You get a seventh of the points.
So you're getting demoted. You're getting discounted because the lower you are. Now.
I have a real example to make it real for you. Okay.
so we're comparing 2 search engines, A and B
search engine A, we have one query, remember, in practice. It's never one query. Really, it's hundreds of queries. And we're gonna average.
But right now I want to show you for one query is, meet me at midnight
we get back 4 results.
And I'm showing you here the relevance level of those 4 results. So what do we said?
How do we interpret those somewhat not perfect. So basically, we're saying, this album is perfect.
This is not at all. This is not. And this is like somewhat.
Again, this is all offline. The search engine does not know these labels. Okay.
so the idea is, we have a query. We run it to the search engine. We get back the results. Then we look at the labels
and we decide how good we're doing. Remember, this is all offline trying to decide if this is a good search engine or not, and if it is, then when we deploy it, fingers crossed.
The hope is it'll still work well on queries we've never seen before. Okay. so here's the point.
Our person didn't did a good job. Found a relevant document. There's only some somewhat relevant. They found crap crap. It's something awesome. Okay? So we gotta figure out like.
how many points do we get for this thing? So what I'm assuming right now is we're gonna do. Dcg
at P. And so we'll say, T is 4, we'll do it over the all 4 of okay. So we're gonna do the summation. So we need to do 2 to the rip. Well, let me just erase that.
We gotta figure out like how many points is the first you know the first album.
How many points does it give us? Well, it gives us 2 to the one minus one. Does everyone believe that
due to the relevance degree, this is the relevance degree
2 to the one minus one cool.
and then we divide by
log base 2 of one plus one. So how many points does the first album. Give us
one.
the second album.
2 to the 0,
1 one minus 1 0. You were just jumping ahead.
So 0 divided by something 0,
the third one same deal.
3 sorry, 2 to the 0, one minus 1 0, 0, divided by something 0.
The fourth album, though. gives us what? 2 to the what.
2 to the 3 minus one over log base, 2 of
5.
So that's what 8 minus one. That's 7. What is log base? 2 a 5.
Well, I know that. But log. And when someone what is it? Log base to 5,
I'm Googling this. I'm not using a calculator. I'm so lazy. I'm just googling it
instructions.
It's really slow.
and it didn't answer it for me. Nice one
blog, based to the 5 is 2.3.
So whatever that is 7, divided by 2.30, great. It's some number. Okay. So
now we summit this right? So we go one plus 0. So what think about it like this? This is like the individual gain.
Then we want to do the the cumulative gain.
1 1 1.
I'm just going to call that 7 divided by 2.
I see you guys make me so mad.
Thank you.
Oh.
3 point something. We'll say approximately 3.
So 4. Does everyone buy this. The queue with the game at the first position is one. The queue into being in the second position is one plus 0 1 one plus 0 1 one plus 3. So we get 4.
So it's kind of 4 points worth of. Dcg, we got for this search engine. Okay, that's the Dcg, we did it.
Cool
problems.
No, we're gonna do. We're comparing to a different search engine B
search engine B gave us different relevance results. These are not in the slides. I updated this this one. However, it returned as not relevant.
Pretty good. Okay, not relevant.
So it's a different rank order. And so we got to figure out. Now, how much is this sucker worth? It's a different search engine.
So once again.
the first album 0
2 to the 0. 0. Okay, we know the fourth album is also going to give us 0. But what about the second and the third?
So the second album is worth 2 points or so worth 2. So it's 2 squared, minus one over log base, 2 of 2,
which is some number.
and then the third album is 2 to the one minus one all over.
Log base, 2
whoops. Yeah, okay, yeah. Yeah. Yeah.
So this is 2 to my 2 to one. My phone's one over 2. So that's one half and then the other. One is one over log base, 2 of 3.
I'm sorry. It's 4 months. Once there's 3, divided by
3 over log base, 2, 1, 3, 1 1.5 8
3, divided by 3, divided by, I think, 1.5 8. I think this is like 1.8 9 something like this. Someone can double check that for me. So same deal, that's the gain.
And then the cumulative gain. 0 1.8 9 whatever. That is. What is this? 2.3 9? Something like this?
So it's works.
So what did we learn just from a Dcg perspective. The way we've defined this. which result is better. It turns out, this one.
what like somewhat bum bum awesome, is better in this case than nothing. Pretty good. Okay.
Okay.
Now, mind you. If we had done the Dcg at 3, this one would be better because we would get 2.3 9 points versus
1 point. It was getting this perfect one in the fourth place really helped.
Okay.
now.
I don't want to confuse you. But I'm going to try my best. Okay.
that's what instructors like to do right? We like to muddle things. Yeah, why do we do that? Why do we like to make things confusing
because we're evil because we're horrible people
because we're lazy. We didn't know could be could be.
I think it's because, like in like
this is, this is a little fallback right? In reality, life is totally muddled. Right? So you're used to
like in your data structures class, your intro program class input outputs clearly defined. Go on this function, do it, do it, do it.
Guess what rest of life is? You don't even know what function is.
You don't know what the inputs are or the outputs. We gotta figure out
little white button there.
Okay. So also comes back to Friday and into next week, we're gonna talk about optimization.
So how do you optimize these rating functions? You have to figure out, what are you optimized?
What is your life doing it again? Doing it again? What is your life's optimization function?
Yeah.
I don't know. Okay.
so to confuse things for a moment, I'm gonna go back here before we continue.
You may decide for this is, you say, but you have.
Why did we define this 2 to the realm
like. I don't like that. Or why did you do this long discount? I don't like that.
So that log discounting is doing something like this right
and the lower on the ranked list you are.
The more you get penalized, you can change this.
you can make it. You can make it linear.
You can make it super steeped. So this is just a choice of how you do discounting. You could change how you discount.
This is a great, by the way, is a great midterm question
on the top side, the number of points you get we may decide it shouldn't really be 2 to the rel minus one it should be
10 to the Rel minus one. We want to give you lots of points.
Okay, so you can change both of these. I'm not cleaning. This is the exact right way to do it. But it's in practice. This is like a normal way to do it helps us understand what's going on. Okay?
So everyone buys it. Yeah, 2 search engines. We've captured the Dcg for each of them.
Now, what's up with ideal? Dcg, what's that about? Okay?
So again, this is an offline evaluation in practice. When you deploy your search engine, you don't know ideal usage
ideal. Dcg is basically like oral. It says for this query, I can look into my collection of documents and pick out the very best ones and rank them first.
Okay, so like, what's the best possible result we can get for this query.
That's what the ideal piece and G's doing. It's like this oracle. You can like see things that like, you know, it appears as the veil of like reality, and it can pluck the best possible documents and put them first.
Remember, in practice, we don't know this.
So in practice, we build the best search engine offline that does the best here. And then we deploy it and fingers crossed. We hope it works well.
okay. So you don't know the ideal. Gcg, what I what I did. We'll come back to this in a second.
I went into the collection, and I found
that there was a perfect. There were 2 twos.
There's a one, and there may be some other ones below it, but I've written these from the best to the worst.
So what you notice is for this query, there's only one perfect match. This one.
you know. This could be like that could be like
in a document zeal. So when we did this ranking before. like, that's literally document Z, right there.
Okay, so this search, this search, a, it found that one document, but it put it forth, but it found it.
Okay.
So the best we could ever do is have a 3, 2, one people get this. It's not all threes, because there are no other threes. There's only 1, 3 that exists in our collection.
There's only 2 twos that exist in our collection search engine B
found one of those twos.
Okay? And then, you know, there's at least one that scored one. But there may be many more. At the fifth, sixth, seventh position, but below those ones, then they'll be all zeros all the way down.
Okay.
cool. This is the ideal. This is the best possible case for a different query. The query might be, you know, what's the weather? It might be 3, 3, 3, 3, 3.
Okay. For another query.
It could be 0 0, 0 0, or even 1 0 0. 0.
Okay.
so let's finish this example. And then we're going to come back to like, why this is so important. Okay.
so it turns out to do ideal. Vcg is just like Dcg, the calculation only we do it for the very best possible ordering of documents.
So in this case.
same as before, we say.
what is the first document worth? Well, it's 2 to the 3 minus one all over one.
the second, the best case we could do. 2 squared, minus one over log base, 2
of 3. This would be 2 squared, minus one over
log base, 2 of 4, and this would be 2 to the one minus one
log base, 2, a 5. And so
that is what 7
4, 3 3. That's 3 halves. I think
that's I gotta do this again. 3 overlong base, 2 of 3.
See?
But this is like 1.8 8 9.
And this last one is one divided by log base, 205,
2 by 3.
This is like 0 point 4. Okay. so
so when we do our cumulative gain. It's 7
8.8 9 there, 8.8 9 plus 1.5
10.3 9 plus something like
that. Okay.
so this is the dream scenario. We nailed it. We get 10 points.
Okay. so just to do the final calculation, which one is better.
you know A or B,
so remember the Ndcg
we need the Dcg over the ideal Dcg.
so we gotta go back. And so for both of them, the ideal Dcg is 10 point sevenfour.
We need to go back and say what was A's Dcg
or
and 2.3 9.
So this is
A's in DC, so we got 4 points out of the best possible case of 10 search engine B got 2.3 points out of the best possible case of 10
4, divided by 10.7 4. So this is like
3 7 and 2.3 9 2.3 9, divided by 2. This is like 0 point 2 2, something like this. So for this query.
which is the but better search engine.
hey? Yay.
this is our big winner. It's better. Okay.
questions concerns freak out. Does this make sense? Does it seem weird? It seems? Goofy? Yeah.
what's the point? If the doctors are what you can say? Oh.
great question.
and you ask great questions. The question is this.
why don't we just use DC,
why are we normalizing? Because the same number? It doesn't really change anything.
It doesn't for this particular query. We're gonna look at lots of different queries. And so it's interesting about this, this comes back. This is perfect segue. So what's so cool about
Ndcg? Is, it normalizes across easy cases versus hard cases. And so what I mean is this.
so think about it like in the precision at K world. Okay? So imagine I have a query that's super super difficult. There's only one good document. Okay.
if my search engine finds it.
the let's say, in the top 4, just like this case. So here I'll do an example here like, suppose there's a query
such that you know, search engine a.
and then the ideal. So something like this, 2 0 0 0
and
1, 2.
Okay, so here's a query, this is. This is a difficult query.
Okay. then here's an easy query.
Let's say the ideal is 3, 3, 3, 3, and we're like 3, 3, 3, 2, or something. Okay?
So we have a very difficult query that comes in
like, What's the Nbc gonna use this case
not to calculate it. But it's getting pretty good.
Right? Excuse me proof because you you found we found only the documents in the third spot. Okay? In fact, if you want to make it like super straightforward, I mean, imagine oops.
you know. Imagine this case. So we actually nailed it. We got it.
What's the Nbc gonna be in this case, though
one is as good as we can do, we nail it. What is the precision at pay in the B with this query.
how many of the top 4 documents are relevant.
one one-fourth.
So the precision at K is 0 point 2 5, saying, you kind of sucked.
But we're saying, you know we did as great a job as we could possibly do. We deserve credit for that. There are no other documents to rank
that are any good.
So like traditional measures, like precision at K kind of penalizes you for difficult queries. Because you did. You know, they're just hard. Whereas in DC. Is saying, we're going to normalize it versus the best possible scenario. You you nailed it, you get full credit.
Okay? Whereas in the ideal or sorry, this easy case there's lots of queries that are very easy.
And so we're saying, here is basically our Ucgs and close to one. Okay? And if we did, an old school metric like precision at K. It would also be like one.
So that's fine.
But if we aggregate over lots and lots of queries, you can see something like precision at K. Is kind of favoring the easy query world, whereas Nbc is trying to balance
hard and easy. That's why we do the normalization. So, yes, it doesn't matter for a single query. But in practice we average over many queries. Okay, that's the big idea.
That's very cool. So so for that kind of mental kind of mental challenge.
you know.
compared to precision at K. Just to think about it offline like what's like, how how would precisionate. K. Treat these cases versus, how does Ndcg treat these cases?
Okay?
Questions in Dcg.
that was it. Cindy siege? Go back here.
Normalize. It's kind of cumulative game. If you were on the midterm.
and you forgot your cheat sheet.
which guess what it happens. You get a cheat sheet, you know that front and back.
Do you want to hear some of the bad excuses I've heard?
Yes, yes, Kev. I was trying to print out my cheat sheet right before class, and the lab printers weren't working.
You know what I say to that? I say, just walk home.
Yeah. If you're trying to print your cheat sheet right before class. Danger, man, danger.
But if you were that person you had forgotten to print your cheat sheet, or you couldn't print your cheat sheet.
You just go back to the definition in Dcg.
I will tell you. I'll write on the board. Normalized, discounted human game. Okay? Normalize? Normalize. Okay, that's the Dcg over the ideal. That's normalization.
the discounting that's I'm dividing by something log something the cumulative I'm summing. And then the gain is something on top the points I'm getting. I don't really that formula that's kind of this.
So if you can kind of suss that out and show it to me, I'll yeah, I'll probably give you the formula.
Okay. but I'll shame you in front of everybody.
Goodness
forgot his cheat sheet. Doesn't know in DC. Gee.
what a freaking loser.
Okay.
that was probably.
hey, ho, hey, class! Let's all stand up and point at that student and call that student a loser. We would never do that because we are a, what circle of trust?
Okay. Bcg, Dcg, lot of stuff. You're putting it all together. Yeah. You divide compared.
Also, remember. remember
average over many
queries. Okay, now.
let's read it. So far, we've been doing offline evaluation.
We gotta do this. This is very important for our machine learning that's coming on Friday and next week
we mainly discussed offline.
We want to compare A to BX prime to X, new algorithm to old algorithm.
Should I parse this way versus that way? Should I ring using? Cf, I again cosign, or should I use? Vm. 25? All of these request, and we have one version of 2 versions.
We assume we have test selection.
we have documents, we have queries. We had expensive
relevance judgments in practice. Again. if you're working in a domain like
sometimes there are kind of some standard test collections. So remember, this is all quite expensive to get together.
It turns out there's a whole community.
Talk about this later on. There's a whole community called Trek.
TREC.
The National Institutes of Standards and Technology runs a thing called Trek
text retrieval, evaluation
hundreds.
And they basically put together Doc's queries and relevance judgments
for different important tasks. For example.
you can go find a collection that deals only with legal retrieval. you can find it deals with web search. You can find it. Deals with
a, podcast
you can find that deals with.
you know, chemical
like a domain, a retrieval, all sorts of different tasks. People put these together, and then we can all use them.
But now I want to turn around and say. suppose you work at a company called pads algorithms?
Have we talked about the equity states that you have in the class working in in Cas algorithms.
So make sure this, if you ever sign like with Startup, this happens a lot. You say we're going to do some kind of stock grant or some Ely Grant, you always want to know what is the denominator
like? We're gonna give you a hundred 1,000 shares if you're like
a thousand times.
you know. A reasonable number is a big number.
How many shares are there? And they're like
trillion
that I.
But for our class we're in the. So we're the Hyperscore trucks. We're gonna we're gonna do equal
share ownership of this project. Okay, here's my question for you, which is
if awesome.
I have a big share of 0. Still, 0.
okay.
so we're doing this like album lyric search stuff.
And
so number one.
you know.
there is no test collection
like, there's no one out there that has a website that has docs, queries and relevance judgments for our algorithms
website. So we're internally developing these methods to help our our customers. and we don't have a test collection.
So now I turn it back to you. What do we do. We're trying to launch this thing. We don't know if our methods work or not.
What do we do really? What we thought of you quit.
You can ask people.
We don't get a little bit about this before. Right? So we're bootstrapping this from nothing.
So you know, one idea is, you know.
you know. Ask, friends.
okay, another idea is just deploy
just ship whatever we have.
and then maybe collect from our our users.
So just deploy it and start it and start collecting. What do we get from our users? We don't get relevant judgments. What do we get
like if you go to Kaz Algorithm to research. What kinds of user like log data are we collecting
Frank Sinatra albums? Search
the more they do.
they click some things. What else can they do.
They may buy some things so
we could get some we call typically, we call this like implicit data.
So we know the query.
we know the clicks.
Maybe we know the purchase
so we could use this to try to build up our own test collection. Now, it's not necessarily perfect.
But we could say, you know, this person searched for Frank Sinatra albums. They bought.
you know, album 1, 2, 3, so we could treat that as
a query
documents. And this is like a relevance judgment. They bought it. It must have been relevant
and steal all their stuff.
So this is a good point.
Some sort of like transfer learning.
time, work sharing is carrier. So through that in practice this does happen a lot right? And so sometimes it may be a domain that you know about already.
So for example, there's lots of like movie rating data out there. You could use that kind of bootstrap your own like movie shop. Okay, you go to Imdb, go to Netflix, kind of scrape their own their data and use it to help you get started. That happens. Okay.
you can also try to. Maybe, are there patterns you can learn from other domains and apply them to your domain could work. Okay.
the point being is like it's quite difficult. All of this is difficult in practice.
I would say a lot of times. You're gonna ask your friends indoor. Bill, you know. Sort of D, you know, maybe quickly build.
You know, a small data set
just something because you need to have some insight if you're doing like, if it makes sense or not. Okay.
So
all of what we're doing here, these are offline experiments that we've been doing. Okay.
It turns out. even if we had Cav's algorithms was live and really working well.
even big companies, they still go use offline data, not everything is like, live A B test.
Okay?
Why we why do we do this like, if I'm Google, why does Google do offline experiment? Probably just deploy it and see what happens? Repetition. reputation, reputational risk. which is, you put something bad out there
problem. There's also
only so many experiments you can run. And so in practice, what what's gonna happen is cavs algorithms we've deployed, we've deployed our current solution.
You are software engineer, slash co-owner of CAD's algorithms, you're developing your new method.
And so before we allow you to push it into production. You're gonna come to your boss. How do we do? We have bosses?
No, no, social. Yeah. We just we all both. Yeah. So you're gonna come before the board, the entire class. And you're gonna present what you did. You're gonna say, I did an offline experiment. I show 10% improvement to these queries. What do we do? We say? Let's go test that
alternative minus 25 pricing much worse in DC, what do we do? We say
they're still minus one year.
But in any event, even though big companies do it, our small company is gonna do it.
The big question is, always does it generalize to real scenario?
Okay, this is another issue that comes up you're gonna face this in your career many, many times, which is you go before the board, all of us, you say cap
and glass
co-owners, brothers and sisters.
I have, plus 25% of Dcg in my offline servers.
This is awesome. We say great and employ it.
You deploy it
bye, bye. 9.
No change in user behavior, no change in click, nothing. Why am I bad
online, perfect online. You see nothing.
It could be some mismatch, the queries, the documents are not reflective of what the users are asking for, the relevance judgments, maybe coming from a different user populations. But maybe in mismatch queries
it could be a randomness. Let's say I couldn't ask you for some specific queries and the offline data set. But again.
for whatever randomness are not, we're seeing well lines that have mismatched the problem.
Any other thoughts.
We didn't think about your practice happens all the time all the time.
Great idea! Got to deploy it. Nothing. I think you know how demoralizing that is. You're like promotion case
right? You're trying to get promoted. It's all about, hey? I gotta ship baby. I gotta ship and show improvement. You do all this work. Great results. You ship it.
you sit back as the like. The online experiment is running.
You know, the first hour, the first week, and they're watching the numbers, because if it's really bad they'll kill it.
So the idea is we're gonna go run this thing for a week and see what happens
after the first day. If the numbers are all down, we're gonna kill it.
And so like much of your career, if you get into this is gonna be I launch it. It's Monday, and you're just watching those numbers. Look good, hey? There, there's a flat. But that's okay. But go up day one a day, 2. It's good. Good day. 3 day, 4. It kills it. Oh.
and you gotta go figure out what happens. Okay? But in practice this happens a lot. Now.
just to concretely wrap up our discussion of evaluation. So offline, we know how to do this. That's that's our primary focus. Okay, this is what we do offline.
You can also do user study. This could be you empanel real people. They come, you record them.
You ask them questions. You watch what they do. You observe them. Okay, alternatively. You mean something like Amazon mechanical Turk.
where you go and panel real people all over the world, and you pay them a nickel or a quarter
and they click through your search engine. Okay.
user setting, very powerful, very useful.
And then, of course, there's online online for your class project. Typically, you're not going to be able to pull off.
are you? We have a project in this class. Yeah, need to build something.
we need a production system with actual users.
So for class project typically don't have
for Cav's algorithms getting it off the ground don't have it.
We need a way to get it going?
Then we're going to measure something like, put through it or whatever test it.
Okay. no. we'll skip this baby testing. You guys know about AV testing we have. This is a classic example. It's like a clock is how we do our online test.
All Google traffic sees regular Google traffic. And then a fraction of a percent sees our new method.
Here's like, yeah, some sign up. It looks like this.
We had a different final form.
They use the maybe a green button and a little arrow
maybe had higher click through rates.
Okay, let's say, testing you have like an A version and a B version.
So this I mentioned this to you before. This happens to you all the time you're you're everyone in here has been a part of an A B test like every day. And so the one example I gave was.
they give this one like, so like Netflix. the thumbnails. those thumbnails are changing.
Okay, cause they're doing is they're saying it's like.
What's a movie out right now? So like.
here's the Barbie default default.
That's good. That's the Barbie Defart thumbnail. And you can imagine you're gonna test an alternative, right?
And so then the alternative. And here's another one.
And the issue here is is very personalized. So I don't know if you've noticed this like on Netflix or Youtube, it's like, sometimes it starts featuring an actor in the show
who's now like the primary person. So recently. I noticed this for Ryan Gosling.
He's in like all these movies, like, I think, like the Big short all of a sudden, like the big short, is showing Brian Dawson's face.
because presumably he's a big star. He's in Bar V, people like him. And it's getting more engagement.
Okay? So just so, you know, these kinds of things happen all the time. Yeah, everyone's like. yeah, we're done. See you on Friday.
Good luck with your homework.

Yeah. this program is not okay.
Okay. Howdy? I think I've had 8 football coaches since I've been A and M.
And I said, every one of them. They give you different flavor, the eyes and the lows. But I said, in in all these years I've been here at the end of the day. It's always the same, I said. I'm sorry for you, Simon. I'm sorry to do class.
This is your desktop. and then all of those recruiting class
bubble people agree. That's it's amazing.
And then you go to Miami and use it totally. Truck. Don't even like football.
Okay? So a couple of points here, so I'll open it up. Are there any questions, concerns, comments here?
So I'm seeing some activity on the slack which is good about the homework.
It also makes me a little nervous. And then I went, and I was reading the responses to the quick quiz where? I asked, how's the homework going?
And some of the common things were
haven't started yet, or
it's like a little open. I don't really understand. so it makes me very nervous.
So I haven't heal the Big 3 right now.
So my offer is this.
you have 5 late days correct.
I'm nervous that you guys are gonna burn your late days on the first. So what I'm offering is this. I'm not talking my tapped me down from this.
It's due on Wednesday.
I thought what I could do is extend it to Friday.
But you may only use 3 late days. In other words, I'm holding back. I'm basically giving you 2 late days, so
you only use 3. So still the ladies can turn in until Monday. But it said, if you were interested in Monday before you burn 5 late days. Now you only burn in 3. How does this sound?
I do not change it. I'm saying this one. You may only use 3. Okay, so
okay, so homework one due Friday. Okay, Max. 3 late days.
That means Monday.
Monday is now the latest. You can turn it in. Okay?
So the lesson is, we don't use the slack. This is coming out to one more time now, this is only for this homework. But basically what happened is I had more faith in in my my students here. So you had 2 weekends to work on it, and seems like most of you sort of pushed it off to the second weekend.
And now I'm noticing all the comments about like the first part of the homework, and the second part is going to be tricky and haven't even done yet. I think so. This is more. I want everyone to be able to get off the ground and feel good of homework one. So when over 2 hits, you'll kind of know the expectations. Another start earlier. And this is like the one
one time only. Okay, so
don't! Don't get cocky.
Okay, this is a one time deal.
Thank you. Alright.
hey? This week, folks, it's exciting. We're talking about evaluation. But we're also gonna get to basically building out our machine learning ranker. Okay? So right now, evaluation, very important. How do we know we're doing a good job.
Once we know how to do a good job, we can use that actually to help us design how we combine our scoring functions right now we know how to score things using like tf-ivf like with Cosign, maybe being 25. If you read that, you wanted to try it. That's kind of all we know how to do. But in fact, we know scoring functions have, like hundreds, thousands of inputs, thousands of features, thousands of factors. So we're gonna figure out how to use machine learning to combine them.
It's gonna be awesome. Now. evaluation
critical is an empirical experimental science. So we need evaluation to help us understand if whatever we've designed is good.
does it do anything positive? Okay? Oftentimes we'll design stuff. It makes it worse.
Okay, we don't want to do that. Okay, so we're going to have to evaluation on the specific scenario that we care about.
So if you're building it like a recommender later on, we want to build evaluation specifically for recommender. If you want to build it for ranking tasks, you want to make sure your evaluation is customized for that, even within that you may have a different evaluation, for, like a general purpose web search engine versus algorithms album in the mail system versus netflix. These are all different. They may have different factors that we care about when we do evaluation.
Okay, so today, we're gonna focus on this, how do we evaluate a search engine
again. Why do we care so much? So remember, this is experimental science.
So we have to do basically design experiment. Okay, every day. You don't know it. But you're being exposed to experiments in all these online services you use right? Like on Youtube and on Netflix. You know, the thumbnails like their AV testing. They're they're doing experiments all the time to figure out what's the right thumbnail to show to get more engagement?
Okay. both. So how well do our system work is like our newly designed system, made better than our current system meet.
So you've deployed your cosigned Tf. Idf. Ranker, you have a new idea, is it better? We better not just deploy and and and hope you better have some way to sort of test it and have some confidence that it may work. Okay? So this is also gonna drive. What's your research? When I say what to research? I don't mean just like some 10 years in the future research. I mean.
what are the next steps I should take to improve what I've been working on, hey? I had a cool idea. I tried it. It didn't work. Why did it not work? And maybe that'll guide me moving forward to make good decisions, to do things better.
Now.
how do we evaluate a search engine. How do we compare search engine? Remember, this could be for our algorithms.
Okay, it could be web search engine could be on your phone whatever. But we're having having to figure out, how do we know we're doing a good job?
Okay? So there's some ways we can evaluate
that are not really user facing sometimes. So how fast does it index like on this homework? I could say, Oh, you know your homework code is faster than someone else's. So it's better.
Okay.
how fast the return results. Hey? Your system returns it in a snap. Someone else's takes like 10 s.
So those are easy ways I can compare right.
The third way is like expressiveness of query. Language. So this would be like, do you support? Is it just Boolean.
you know? Do you support phrase queries? Do you have wild cards?
All these other kind of design choices? You could do a comparison which one is in theory sort of better than another one. Okay. these are all not really like user facing. They kind of are. But in practice what happens is.
you know, the speed of indexing or the speed of results. For most cases you're going to be able to solve those problems. Okay, the question is really going to be.
do your users care or not. Okay. So in practice.
we often care about user happiness will amorphous.
This is like a vibes based approach happiness. Are our users happy. So for CAD's algorithms.
how could we measure happiness?
And I want you to give me some very concrete ways, and I want to give you some off the wall ways for, yeah. Okay, some sort of like, I'll call that kind of
cut user retention usage. People come to our site. They stay for a while, and they come back to our site. That seems like something you can measure.
and seems like it may be correlated with their happiness. Give me another.
How how to do this? Yeah.
stay, click on.
We can look at what they click on.
So some sort of measure of like, I'm gonna call it like, maybe of like exploration.
So if they click like the top thing.
maybe we're giving them something good, whereas if they had to really kind of go next next. Next, they're really having to go deep into our system to find what they want. Maybe they're not as happy.
Okay, so some sort of it's tricky. But something like this, where and how they click. Yeah, yeah. So some sort of conversion rate.
like, you know, do they do. They spend money. Right? They came. They click, they click, they click. They never bought anything
they came to quickly bought. Yeah, they're happy. Yeah. User sense been on feedback. So we can go look at some sort of reviews or social media, and we can sort of measure some sort of sentiment. Are they happy or sad, but they somehow getting feedback. Hey? I used tabs algorithms and it sucks. Okay.
I took this class 470, and it sucked. Okay.
yeah. Okay, how about kinda off the wall like
these are all like, what if I gave you like an unlimited budget? And you can instrument up people.
And then.
okay, what if I have a camera?
Yeah, other sensors? You could key log them. What about if you put on some kind of like a brain wave. you know, helmet or something.
Yeah. But there are these other ways that they maybe don't scale, and people don't want to do them in practice, but in small scale, like lab experiments. And so, like all these big companies do this, they'll bring people in, have them use their system, and they'll look at these things. How do they play? etc., etc., all this kind of stuff? But they'll also have them hooked up to cameras, and they'll look at their faces. Okay, and they'll hook them up to some machine, some sort of
like a brain wave and trying to understand like, are there signals there that they're really happy or not for real, for real
and practice, maybe user privacy. We don't want to do this for everybody, also very expensive. But yeah, this kind of stuff really does happen. Now, all of these, these are awesome. Thank you.
You have to be careful. There's something called Mcnamara's fallacy. Okay?
And the idea is basically quantifying success
in terms of easily measured factors versus while ignoring other stuff. Okay? So this is, has anyone seen the fog of the fog of war?
Errol Morris's documentary about the Vietnam War and interviews with Robert Mcnamara, anyway? So he was the Secretary of Defense during Vietnam War. But he was also like a systems engineer
from like the fortys and fiftys. And the idea was like, we're gonna systemize, we're gonna measure things. He's like a quantitative guy. So like what ha historically, had not been a quantitative like work.
And so I imagine many of you are sort of in the same boat you're like, hey, man, like, I count things. I measure things. I'm very quant. So he was like one of our Og wants, but when he did, is he made one of the like disastrous mistakes? Which is, you focus on things that are measurable, not like? Are you actually winning a war? Okay? So examples of like Mcnamara's fallacy, or where we could optimize on things that are easy to measure. But that may not actually correlate with success. So example.
we're optimizing our website, for time spent on websites. So we're gonna try to do this is going to measure. We know when you first arrive. And we know when you leave the website. Okay? So that's easily measurable. And our quantitative optimization is to maximize how much time they spend on our website.
So this is easy to measure. So is there like, is this easy to measure feature? Could it lead us to designing a system that is actually worse for our customers. And if so, give me some examples. I guess I'm on a website for a search engine. Could it be
potentially on
a lot of time trying.
Okay, slow, hard to navigate down. Yeah. So more time. Yeah, it could be slow.
It could be hard to navigate.
There was a point up here like hard hard to kind of read.
Understand?
OP-ups. It is sort of like hard to navigate. So like lots of like clicking. Next, you know, adds
all this kind of stuff right? And so
this is one of those cases where again. You can imagine, like, Hey.
we, as a company, are measuring time on websites. You had the smart idea. We'll take that news story and we'll break it across 3 pages instead of a single page. Right? So now, the user had to click multiple times, yeah.
yeah, tick, tock, right? So tik tok is like, crack. Okay, it's just like, it's user engagement, like, it's insane. And it's just like.
it's like going to the Casino. Right? It's like the slots. Yeah, yeah. See this little
not endorphin. What are you getting dopamine? It's just like.
okay, good examples.
Let's do another one. What if it is, we're gonna minimize the time until purchase.
You show up on Cavs algorithms
and people are buying stuff very fast. That seems awesome.
They're not taking a long time they come to our website.
They find the albums they want, they buy them. This seems like a very smart thing to do. However, can you think of ways in which it might lead to some suboptimal outcome for our customers.
Okay? So they're buying. They're buying junk so it could be kind of one time purchases, hey? I bought the thing, but it really was stinky, and I never come back.
Okay, give me another example.
They're not buying anything else. There's like no s like serendipity. Serend, how do you spell that
dippity?
It's like, you know you're not. You're not kind of co purchasing
or like exploring.
you know, when you go to the grocery store, you know. Come that last aisle. They have, like all the candy bars and stuff right? It's like you bought the stuff some more stuff, you know.
You come to our website. We want you to buy the album, but we also want you to find other albums, and like be engaged and like it any other ways a time until purchase could be bad.
And so you don't like that.
Okay, some sort of like expiring coupons. There's some sort of like race, like clock race that like trying to force you to do something. Maybe it gives you a bad feeling about the whole experience and could hurt your long-term user retention. Okay, cool. One other thing to think about is like, when you design these we want to watch.
It doesn't mean you don't care about them right? We care about. How long does it take people to purchase
the key is you may not want to minimize it. You still want to care like same thing with this, the one before, like, how much time do they spend on our website. We'd want to know that. Okay, it doesn't mean we want to optimize for it. The other thing to consider is the time scales.
Right? You also want to consider, not just like you know. How are my users engaging at this moment?
But, like all of you know, you want to design these methods so that you're watching them for like, do they come back? Is there retention?
If they come back like they buy more, do they buy less? So you're trying to calculate lots of lots of characteristics of your users. Here's just a couple. So don't be dumb and like and like flow it right.
In fact.
you know, figuring out what is the right thing to measure is oftentimes like 70% of the of the of the job.
Okay, so again, speaking from some experience of these big companies, and some of you may have had this experience like on internships or whatnot.
It's like, we know what we want to do.
But first you have to get buy in on what is the right thing to measure. and you often do that before you actually deploy the thing. because if everyone had agree, what we're trying to do is move the needle on this.
then you can go deploy it and have buy-in that it does the right thing versus you don't know what you're doing. You just deploy the new thing. You measure a bunch of stuff, and you don't really know how to interpret it right
now
on the search context, this guy, Cyril Cleverring.
had these classic experiments called Cranfield experiments where he had this innovative idea at the time was, we're going to measure user happiness. We're going to simplify it down to just
are the search results relevant or not.
Okay, I'm not claiming this is the right way to do it. But from like the 1960 s. On, this has been like the classic paradigm in which we would look at a search engine and do some basic evaluation. So as a proxy for our users, happy is are the search results we give them? Are they relevant? Are they what they're looking for or not?
Okay. So
to do this is the canonical Ir setup for evaluation. Okay.
we need a benchmark document collection. We need a benchmark set of queries.
Then we need relevance judgments for those queries and those documents. So, for example, the query could be Taylor Swift's tour.
The document could be my homepage.
the binary relevance judgment would be not relevant.
Okay?
The query could be Taylor Swift Tour. The document could be Taylor Swift website tour listing.
the binary assessment would be relevant.
Okay? So you got me, we have a whole bunch of documents.
a whole bunch of queries. And then some humans have gone in there and labeled.
is this query relevant for this document or not? Okay.
this is all typically offline.
So this is not like an online. I've deployed it. It's a I'm in the lab trying to figure out. If I have a good idea, let's go see if it works here.
It works here, then we can worry about going online. Okay.
that's cool. I think I have some examples. Yeah, well, let's yeah, let's talk about some challenges. So for Cavs algorithms. Let's do it for us specifically.
what? What is, what are the documents? And where do they come from?
So right now, we're doing like lyric search. So we need a collection of lyrics. Okay.
where do the queries come from
users?
We don't have any users right now. No one's using this thing.
So what kind? How do we get queries? Then
make them up? So we're gonna cook up a set of queries.
How would we design good queries like. In other words, what would be good queries as a match case is
edge cases. We could
panel some real user app and say, Hey, if you're going to use our website. What are the kinds of queries you can use breaks and offer albums?
Your magazines?
II don't know what kind of queries right? In fact, this is also a big problem, like, when people come to cab algorithms, are they asking for specific albums?
You know, that's where the challenge right? Because if we had a set of queries which again don't correspond to our real users. who cares we? We blew it right. All of our queries, these weird queries.
they're not like anything like a real user would have. So having assessments of whether they're good or not is not very helpful.
So the idea is, we want these to be like reflective of the scenario that we care about.
So imagine, like in practice, the CAD algorithms is up and running. What we would do is if you're designing like our new rinker.
you could go into the query logs and all of our logs, of all of our user interactions and say, what are the actual queries our real users are using? Hey, what do they actually look at? And you could use that to start building an offline document collection setup queries.
Then you can do some tricks to try to get some assessments. But now you can have, like a more like ecologically valid reflection of our what our users care about. And then you go into your offline world and experiment with
purely offline
like, it's really happening for our users. So in other words, imagine this, we have first software.
What about waiting? Yeah. So imagine we have a million sets of lyrics. Okay? And imagine we have a thousand queries
that are somehow reflective of our of our users. Right
to do this. Now, this is some like very expensive human
classically human labeling task.
So we say, Okay, okay, here is a query. The query is a
frank sinatra.
And here's Doc 100. What do you say it is relevant or not? And so little human is sitting there judging you could use mechanical Turk
you could in panel some volunteers, people in your company.
But this is to be very, very expensive, right? And you're really going to do 1,000 queries in a million sets of lyrics.
You're really going to get a billion
labels.
That's right, this is where you're gonna lose your mind right? So in practice.
that's what I mean by challenges. So even building the collection. Even doing this is hard. So in practice.
people have done that already for you. These are all old, but I want to show you like
no joke. There are some like classic data sets that have only 3,000 documents and 64 queries. But people used it because we didn't have good data. And so we would go run algorithms and see how we do here.
Granville had 1,400 documents 200 years. This is tiny.
Okay? So it's 20,000 documents, 600 periods.
Okay, 700,000 dots, 200 queries.
Okay.
in this case, they got over 100,000 query, document relevance assessments. Okay.
so one of the challenges is just building these test collections. Very expensive major pain.
Okay? And so if you build a good one, typically everyone else is going to glom onto it and use it.
Okay, so just getting good data, really, really challenging.
Okay, so just showing you, this is like, historically how we would approach this.
Now.
now, assuming we have done that, assuming we are, we built our own collection for Cavs algorithms. It's totally offline. Okay. So now we do. Is the ir black box. This is
that's you build that
that means, are you using tf-idf and cosign? Are you parsing it this way are you using? Stop words? Are you stemming? Are you limiting all those questions? You decide?
You build this? Okay? And so we're gonna do, is it? We're gonna take a query.
We have our boatload of documents right from our test collection. The black box is, gonna say, okay, here's the rank order, I think
like on your homework that problem and say, Okay, we give you some queries. Here's the ranked list done
now. Given the ranked list. We've already decided on how we're gonna evaluate the goodness of the list. So we need relevance judgments
so already collected, good and bad, good and bad, relevant non-relevance. And we need some way to measure
the effectiveness of of these of what we did at the ranked list. Okay.
so let me show you some examples. Okay, we have 3 systems. We have A,
B and C,
and suppose
each one of them gives us 5 results. Okay? And so imagine the results are such that
we said, relevant.
relevant. relevant. relevant. relevant.
relevant elements. relevant. relevant
now is right, not. It's not
not not not
not okay. Now.
question
which is the best system, A, B or C, which one should we deploy?
Oh, okay. So let's dive on. Okay. So in other words, system A for this query, give me $5 back. The first 3 were relevant. The next 3 were not
okay. So to be the first one's not relevant. Did it be 4 relevant
and system. C gave me relevant, not not and relevant, relevant. Okay. now.
And when I have all this recitment and and your brain decides and needs the best.
Aye.
you can vote.
Thank you.
All who think that A is the best.
Come your hands
a big chunk.
all who think B is the best killing some pants.
viewers, and all who think C is the best. Show me some hands
interesting. Okay? So for those of you who argue for a.
Why.
verse 2 are verse 2 are relevant
in the story. First, 2. Irrelevant. That seems good.
Okay, everyone who said B. Why did you say B more relevant
4
greens, and nobody argued for C. I'll argue for C.
It has. It has more relevant than A,
and it has higher relevant than me.
Seems pretty good. Seems like a balance between A and B. I like seed
so objectively. What is the correct answer?
It's an interesting. Are you arguing for relativism.
That's a slippery slope, sir.
so yeah, it depends. Go back here.
We have to decide on the measure of effectiveness. Right now. We have query documents, the black box. We had 3 of them 3 rank lists. We had relevance judgments, and we haven't decided what the measurement is.
Okay. And so as a result. I don't know which one is the best, because we haven't agreed. What is the best measurement? Which means we haven't thought about. What do our users really care about. So can you give me a scenario?
Really, where a really is better than B from a user perspective that you could actually observe?
Yeah. So we saw that evidence I showed you before of like eye tracking studies, and where people click.
So we know that being first and second is so important. People click on that stuff all the time. And so in this case it could be no one's even scroll to 2, right? They're gonna click this. So
so we better get this right.
If we give them the first one is bad they're out of here. They're unhappy.
Now this is not really great, for, like a lyric search. But imagine you're doing like,
you're filing a patent and you're searching through legal documents for prior art.
Okay? I would argue, B. Is better. So if I'm filing a patent, I need to know about all of the prior art.
In other words, I'm willing to read every document.
And if there are, isn't that 4 documents that are relevant to my patent? I need to know about them, because if I don't reference them, I'm not going to get my patent.
Okay, this is a scenario where and more is better.
And this is a super hybrid. Okay? So it turns out, we can design some measures of ranking effectiveness or metrics that correspond to these different scenarios. And then we wanna make sure we pick the right one to make sure we're
doing the right thing for what our users really want.
Okay, so we're gonna talk about a bunch of these. Okay, there are gazillions more.
Okay, there's lots we're gonna do precision. Recall. F,
then we'll do something. I pay. And Dcg, these are, typically, we call them unranked.
And these are ranked. And and you'll see what I mean by unranked and ranked.
Okay. has anyone done in Dcg before?
No, yeah. Could do good? Has anyone heard of precision and recall. Give me this
total recall.
It's a great movie, not the reboot.
Never watch the reboots. Has there ever been a remake? That's better than the original
opposed
talk on Maverick. Yeah. But
I'm gonna I'm gonna give it up there. Yes, that is, that is correct. That is objectively correct. Every other one terrible
rogue one.
We're not gonna talk about star wars in this classroom. And sorry, yeah.
we're gonna we can't do that. Okay. let's do. These are classic measures. precision and recall
precision.
My system retrieved a bunch of documents. How many of those were actually relevant?
Recall my sister? Retrieved some documents was relevant. How many of all of the relevant documents did it find?
I'll show you examples
to have a blank one. Yeah, I'll show you example.
Here comes the query. imagine we have a bunch of stuff here. and we get back some hits.
Okay? And so imagine the query, imagine there are a hundred 0 docs.
Okay? And imagine for this query, there are, let's say, a thousand
relevant docs
to this specific query.
okay. so this is like, I'm looking for albums about
Texas themes. Okay? So we know about lots of albums. There's 1,000 that are relevant to the query.
okay, so now, imagine my system has now returned back. a
it returned back, let's say. let's say they returned. 200 docs. and then we go look at our relevance judgments. Imagine we find that there are a hundred 50
are relevant.
Does everyone understand the setup? Here I issue my query. I know about 100,000 docs.
my system found $200, of which 150 irrelevance school. So in this case.
if I were to ask you the precision and the recall.
So the precision is of the retrieved docs that are relevant. How many is that, by the way.
I found a hundred 50?
But how many documents did I return to the user? 200.
So what is that?
So we're 75% precise.
In other words, I told you about 200 dots 150 were relevant.
and I was to be clear.
Then, I'll say, like, you know, 50 were not
relevant.
Thanks.
How about
my recall. So how many relevant docs did we retrieve 150? So the numerator is the same. But how many relevant docs are there? Total
a thousand.
So our recall is
15%.
So we only found 15% of all the relevant ones.
The ones we found 75% were on were relevant to our our users.
The reason this is these are called unranked measures is the idea that, like of the 150, I don't care the order of all. In those other 200 150 were relevant.
In other words, there could be a rank that says, like the first 50
in rank order, we're not relevant. And then the 150 were relevant. We ignore all of that. We just say, of the 200, how many were relevant
by 1,875%. Precise.
Does this make sense?
Okay? So again, you want to think about scenarios. So like for our cavs algorithms. Do we care more about precision? Do you think, or more about recall?
We like both right?
If the query is Frank Sinatra albums.
do I want to give you every frank sinatra album recall. or of the ones I give you. Most of them are about Frank Sinatra. I think precision right.
Go back to our example of the legal. You know you're you're you're writing a patent. Would you care more about precision or recall?
Recall, recall, because you need to find every possible one I need off. I will suffer through imprecise results to get all 1,000.
Let me give you another example. I don't know if this is still relevant in today's like with your with your dating apps. Historically. If you were going to go see somebody.
you would Google them. Is this person a psycho. Has this person been arrested? Okay. so maybe you don't do this. But maybe if you tell your parents
the person's name, your parents will do this. Okay? So as a parent checking in on your significant other, do we care about precision or recall?
I'm willing to read everything
I want. Recall
if there are 10 documents about you, and one of them is your arrest record.
I want to find it. I'll read a thousand documents to find a one
I want people I won't recall. Let me just show you I don't have.
We'll we'll come back. Let me just show you quickly the same setup.
How do I do this?
So you know what? No, let's do this. Let's do this. I'm gonna I'm gonna big brain in here for a second. Let's just copy.
Let's paste. I can't paste
copy. There's no paste
to add. Page.
have I lost my mind?
I can't copy the whole thing copy
least
okay, sweet. Let's delete that.
Yeah, let's go back to here
and now let's do.
It's your race. let's erase.
So let me do a different one. Suppose same deal a thousand documents are relevant to the query. And suppose, I have,
10,000 results.
and
we'll say 9, 50 are relevant.
And how many does that mean? 9,050 are not so. Now, in this case this is very different. What is the precision in this case?
So it's 9 50
out of 10,000, which is what you say. 9.5%. And what is the recall of this case?
9 50 out of what 1,000,
95%. I nailed it. Okay, so this is just give an example that, like
here, I have way more recall. But I paid it for it in precision.
In practice they tend to, they tend to be inversely correlated like that. So higher precision, lower recall, higher recall, lower precision tends to be the relationship.
So, given all of this.
could you design a search engine with perfect recall, return the whole thing every time.
have no precision. So the queries, Frank Sinatra's albums. What do we return? Everything. The query is Taylor Swift's latest album. What do we return?
Everything so? Yes.
yes. return everything. It's just an important.
So this is what I mean by like you want to make sure you don't go down these rabbit holes of Hey? You know what like I wrote like, have like a perfect recall search engine.
That's amazing. It's like a hundred percent every time. It's like the 100%.
He's like a hundred percent Hoffman center both. So here
here we're opting, we would be optimizing recall. But our users would have a terrible experience, because basically, no matter what their query is, the results would always be the same. Everything
they would leave us quite soon. Okay.
Now, because people are lazy. be included.
We don't like to have 2 numbers. We like to have one number. So that's what's called the F measure.
and it's just a way to combine precision and recall into a single number.
So we just did an example. So we had
we. So we said, our
what was our
75, and 15? Right? So this is just 2 times
75 times 15 over
75 plus
15 equals. Some number or F is 2 times when we says, like 0 point 0 9 5 times 0 point 9 5
over.
And whatever that works out to is it a way to combine them? To give you a single number. Okay? And sometimes we like
because it gave us some notion of a balance between precision recall. And it's a single number. So if we're comparing 2 different systems, it's a way to compare them. Okay, that's all. It is no worries. Now
notice what we haven't been looking at is the rank order. So precision. Recall F. All of those. Just say, look at all the stuff. Tell me what's up?
What if, instead of be measured precision at a particular peak.
So let's go back to our example. Where was our example?
This one right here.
Copy.
Least
notice.
Okay, so we have the precision at K for a precision at K. For B.
Well, here we have to define what is our cake. So what is the precision at K, so let's say with K is equal to one. When is the position at K or a
one? So the person is relevant out of one.
So yeah, it's one. How about for B, and how about for C.
We could also do. K. Is equal to. Let's do 3, and we'll do. K. Is equal to 5. So the precision at K for A at 3 is
2 of the first 3 are precise, so we would say, 2 thirds. how about for B
two-thirds, and then one third, and then we did it. 5. It'd be 2 out of 5, 4 out of 5, 3 out of 5. And so what I want you just to notice about this is.
even with this precision at K. It seems it's capturing some notion of rank right? That, like higher, is better.
But they didn't know what you choose like this one is the best, the best, the worst.
worst hide from the best, the best. the best, the worst in the middle.
Okay? So you know, the whole point here is like.
you can't just pick the the magic number that tells you which one is best. It depends on again
what your understanding of your users is, the interpretation of it, etc., etc., because even just using precision at KI can give you different versions of it that give you different
relative orders of A, B and C, okay.
let me ask you this question.
how many queries have? I shouldn't be here.
Wonder
are you ready to ship your product
based on one period that way? Live madness? No.
no, no way.
So in practice, right? It's not a single theory. We need lots of theories.
How many birds do you need
alliance? And again.
if those queries don't reflect what our users care about.
they're not very, very unique.
Right? So I can took up a bunch of queries that our search engine is awesome for. But no one in practice ever uses those queries.
So what does it tell me? Really.
this is a single case. And in practice, where we're gonna average over many of them.
And we're not gonna talk about this here. But then you have to do statistical significance testing to make sure you're actually observing a real difference. Okay? Now. last one Ndcg.
I guarantee you you're going to see precision recall F,
maybe in Dcg on the midterm and the final.
I guarantee it. Future homeworks are Gonna have to do this kind of stuff.
Okay.
indcene is basically in practice with most kind of like. user facing like, web app facing
platforms are, gonna use some version of
okay. So for sending it to recall, those are classic measures. They're easy for you to understand. But we're not going to use them very often
we're gonna more often use precision at a and then a lot of times in Dcg
normalized, discounted, cumulative. A,
what? This is cool.
very popular. very cool.
What's the big deal here? Okay?
So well, you can read what it says. But do I have the formula next.
yeah, it's coming. Okay.
So what we're gonna do with Indie. Ccg is basically as precision at K.
You notice, when I did precision at 3,
these 2 are the same two-thirds, 2 thirds.
So it cared about the top 3, but within the top 3 it didn't really care which was one, which was 2, which was 3,
right? So it kind of has some rank, but like at a threshold, it doesn't really need to be fine range like oh, this is not. But this one is that important.
So what Ndc do Ndcg is going to do is basically give you basically points. And it's gonna give you the higher rate you are, the more points you get.
So man like doesn't get like 100 points, and then let's say 50 points, and let's say 25 points. So this would be a hundred 50 good points. This would be 100. You didn't get them 5025075.
The idea is this would be way better than this one, because you got the first one. Right? You got more points. Okay?
So
that's one of the key factors of Ndcg. It's gonna kind of give you different amounts. Okay? It also is going to be normalized, considering the difficulty of the query.
So what I mean by that is.
what do I mean by that? So
imagine this. There are some theories that are easy.
We'll give you an example of an easy query that we might face
right easy. I mean, it's easy to find good documents.
Taylor Swift. It's pretty easy. Query. Because why? Because there's lots of content about Taylor Swift. Lots of high quality content. It's kind of easy to satisfy the users. Information need
give me a query that's much harder to satisfy.
Also a Taylor Smith, for example.
phrases, a query for which there are not many results.
maybe not obvious that they are
so. So
spelling it, and football system
and football chances believe. Imagine, for Taylor said. If you're like.
I even know about how long tailors with front row tickets
that are legitimate and that are less than $50. It doesn't really exist. It's impossible. Okay.
so, or you could say something like this.
Nope, we're out of time, folks. Good luck on your homework. I'll post on slack about the update about the due date, and I'll reflect it on canvas. Good luck! See, you soon.


Your audio, start video recording, start
no audio, stay participant, share, content, screen, start broadcast, do it every time.
Okay, do that. Do that, do that, do that.
hey? Howdy? So you know, last time I can stop working, you know, the problem was, I own my failures. Apologies welcome back everybody. It's Friday. How are we doing? We doing alright?
Pretty good. So
I like to say, this is this is the view and the proud. So what we're gonna do right now,
is, I'm going to mute.
Okay. So let's get started. Sorry about the delay. If you're watching on the the video apologies. Okay, gang. So we're gonna continue on Tf-idf and post sign
and
homework. One is due next week. Don't forget some of you have not even started. Do start. Some of you already been posting on the slack. Continue to do that. I've even seen someone posted like all of the I think, documents that a word occurred in. And then Alan Rta said, you only to tell us 5. Well, that's true. But you I think you were trying to show like
for completeness. Do I have it right? Is that what you're getting? Keep doing that I like that. Okay, if you're parsing the documents. Here are my top 10 tokens. This is what I got.
I did a different kind of tokenization. What did you get? Okay? If you're having trouble with tokenization.
I heard someone talk over here about some parameters you have to pass, or something is confusing, posted on slack. Okay, we will respond, or your peers will respond. And again, if you engage with that kind of stuff, I make little notes. And you're gonna do okay. You're gonna be good. Any other questions, concerns, comments about the class life, liberty, pursuit of happiness.
anything else?
You guys are good.
does? My, yeah. So okay, sorry. I was saying that.
the little tip to come slightly unscrewed.
that was all. It was so dumb. Apologies so dumb. Okay.
last time.
if you missed last time, this is what we talked about vector space model documents, queries or vectors each access term in our dictionary.
How do terms get into our dictionary is how we tokenize
right. So if you tokenize and you throw away some word, it's not an access in the dictionary and or the in our vocabulary, your document doesn't have that term when we do the representation.
So if it's important, we need it. Okay? So that whole tokenization discussion very important because it determines what, how we represent our docs and queries.
Second, we said, we're going to typically use tf-idf waiting. And then, third, we're going to use the cosine measure similarity where, if you're perfectly similar
one, if you're orthogonal, perpendicular, not similar at all. 0. And then everything in between. And then we rank that way, and that tells us the rank order of our docs. Cool.
super cool. So what I want to do now hammer home. Got your hammers, your handy dandy hammer.
Dora didn't have a hammer.
Did the Diego have a hammer handy? Manny had a hammer. I knew that. Who here watch handy, manny. Sometimes
anybody here like Kai. You
yeah.
Like panic attacks.
are you are? Is this generation? I don't know. Is this generation Paul Patrol, or is that too young?
I used to watch all this stuff. I don't even have kids. I used to just watch.
that's a joke. It's awesome.
Okay, I wanted to hammer this stuff home. We're gonna do some examples. If you already know this stuff, you've got it, I'll do a few more examples. I just wanna hammer home. This is on the homework. This is classic stuff that's gonna show up in the midterm. It's also stuff that will be integrated into other stuff. We do. And so if you don't get it now you're gonna be like, doubly hurt when we do the other stuff later in the semester. So anyway, our key app is basically saying in a doc more times it occurs better, more valuable.
And where, I comment before, is that we have to throw the Ross here just count the number of times this term occurs in this document. Discount it.
or sometimes we're gonna do some log scaling. So we don't get kind of like too much bonus for just repeating words again, there are lots of ways to do this. Next week we're gonna talk about evaluation. How do we know this one better, or is this one better or a different one better. So we're gonna talk about that next week. But the idea, this is all kind of like empirical. We try some stuff. We test it out, we see which one does better. We use that one.
Okay.
So, for example, I've changed the example here so that it'll work out better. You'll also see this on midterms and finals and stuff where I try to make the math work out. But the whole idea here is we have a document, the Aggie football game, Aggie. It's a very exciting document we have to figure out what our dimensions are.
What are our dimensions
of? Of. I said it was just the tip.
Is this happening again?
Okay.
it knows about the pencil. You can't see it. It says on mine. It says pencil. It's a hundred percent charged.
Be back. Okay, so what are our dimensions here? They are the
Aggie
football.
So you have to determine the order in which you write these. So again, like on the midterm, I'll usually give them to you. So I don't have to read the numbers in weird orders. but you had to decide. This is my vocabulary. And so if we just do, this one is just, remember, this is just counts.
So in this world.
the Tf representation. If you wrote it as like a, vector it would be like, the is what one Aggie is.
3. Football is
game is one. We did it? Yay, so this would be like a raw keyf representation.
Right? So this would be like if I said.
that's the document one vector, using, just like, raw? Tf, so that's that's a vector representation. And we could do it like this. Okay, then we have document one representation, which is
this log version.
and that case is going to be log of now one plus one. And typically we'll do base 2, or I'll tell you base 10. You'll have to decide on the homework. I think we didn't specify.
So what you need to go do is go into slack and say, I'm using base 10. Everyone else use base 10,
or I'm using base to everyone else. Use base, someone decide that and just do it. They'll make our life easier, because then the numbers will all match.
But if we do base 2, what does this thing become?
Log base? 2 of one plus one log base, 2 of 2, but still one.
and then I fix this one. So now add the log base, 2 of one plus 3 log base. 2 of 4 is.
So thank you. Thank you. Everyone's gonna have to leave.
So the point is, but this is a raw dft representation.
This is a log based representation only in the Tf world we're getting towards. Tf-idf. But right now this is just the Tf representation. Everyone's cool.
The point here is like, if agony had occurred not 3 times, let's say. 127 times.
It wouldn't show up here at 120. It'd be here 127. But here it'd be log base, 2 of 128, which is
names, I think, those 6, 7
a number that's not 127 much smaller. So the idea is, it wouldn't be dominating these other words by 100 times. It'd be only like, you know, like 4 by 6, 7 times. Okay, that's what we do is log base waiting?
Tf, we got it easy done questions.
Yep.
no. So yeah. So the logs. What I would suggest is, you're gonna have a cheat sheet
and we get a cheat sheet. We'll talk about that later on, but it's front and back anything you want on it.
Yes, when October, or something
we have. We haven't met. Wha what is this class again?
you have a cheat sheet. We'll talk about that all later. The cheat sheet can have anything you want on it, front and back.
And so I've always said, like, if you wanted to like the joke is, you're gonna
you know you did the whole book or something, and, you know, have a microscope, and that's fine.
You just want to have like a picture of like your mom smiling at you to make you feel good. That's fine. Someone said you should take all of a Tetg parameters and then just write them down.
Then you can. Then it's a joke.
Okay.
that's as funny as it gets secured. Okay.
let's do idea again. Just a hammer at home. So there's no concerns no questions.
Again, idea is sort of kind of in contrast. Right? Ts, as the more you occur the better.
But here idea is saying, the rarer you are across the documents the better. Okay, the idea is, the rarer you are, the more informative you are, the more valuable you are, the more discriminating you are okay.
So Ivf, we, you know, here's a formula. So the idea of a term and the point here, this is not. That's across all docs.
So the idea is the idea, for a term is the same in each document. whereas the Tf. Changes for every document. the idea is like across the whole collection.
And we just say, this is like the
total number of docs.
And this is just the number of docs
with T,
okay.
that's it.
So if we have a hundred documents
and 10 of them have. T. This is log of a hundred over 10.
Oh.
it's one. It's 100 over one. If it's in every document. 100 over 100. Okay? And so it's a measure of like how rare the terms are and the rare you are, the bigger this number is.
Let's do example
I give you again, we'll do log base 2, to make our life easy. Right?
And so we say, this is just the idea of notice. I'm not doing a vector. For a whole document. This is a score for a term across all the documents. So the idea is like the Idf.
For Aggie is just the idea for Aggie for this. These documents.
Okay, it doesn't change predominantly.
So in this case, the Ivf for Aggie.
we're saying, is log base, 2 1,024 over.
we said. Aggie is 32, right? What is that
else?
What is what is 32 is 2 to the 5,024 is 2 to the
okay. So
10.
So 2 to the 10, over, 2 to the 5.
Oh, man, you guys, are you guys in big trouble? You've only memorized the 64. So then you can. Just.
I don't know. So let's do the idea for
football. Same deal log base, 2, a thousand, 24 over 8.
So notice 8, 1632. That's 2 away. So it better be 7. This one for game log base. 2 set
should be 6,
and this one is the fun one.
So the is in all of our documents. So it's 1,024, over 1,024. So the log base go up one. So it's just 0.
And so what's cool about that. at least to me. is that the is like nuked away.
We didn't have to make the a stock word. We just used a waiting scheme that said because the occurs in every document. It basically has weight of 0. It basically is nuked away.
So imagine we had another word that was in note a thousand documents.
Similarly, it's not very informative. You don't have to precook it and say, I know this is a common word. You just let the waiting scheme say it's so rare. I'm sorry it's so popular. We're going to get a very low score. And it basically won't count for much.
Okay, that's what's cool about this idea, is it kind of does some of that stop wording for you?
It automatically figures out what's informative, what's not informative. Okay. record this
awesome. It all works out now on the homework.
It's not gonna you're not gonna get these beautiful like 5 7 s. And 6 s.
On the homework, though you have to do tfs and idf and multiply them and do all this business. So that's an easy place where people are gonna like kind of goof up off by ones. Where do you log things like that? So that's the kind of thing. If you want to post your intermediate results
like, Hey, over the lyrics we have for the word, Aggie. This is the score I'm getting in this document, and that way you can check with other people. Am I getting the right number?
cool
in practice, too. You gotta keep in mind sometimes they'll we do a lot of these calculations at the end, like we do cosines
like the decimal places. You'll have some slight differences, and that'll be because of the order of operations in which you did stuff right?
Just keep that in mind.
Yeah, question.
My dream is you? You can in private say.
I would like you to be inspector.
Yeah. So if someone has already finished the homework and they wanna be a good citizen. They can just say, Hey, this, these are all the numbers I got. What did you guys get?
Yes.
yeah. Well, the point is like that. So
my hope is
because sometimes you do the stuff and the way we do the homeworks are, I'm not gonna say it's open ended. But like
we don't just give you the function and say, like, you know, fill in this one line. We said, Go build the search. Okay. And so the end of it. You're getting these results. And you're nervous. You're like.
so I want to make sure you feel good. Yeah, that's the thing. I don't want you to say like, Hey, my numbers are not like your numbers. Give me your code to get your numbers, but you can talk about it. Hey? Did you log in here or log in there? Did you do this yet?
Cool.
cool.
super cool. Okay.
Okay.
Okay. idea.
Let's put it together.
So really we want to do is we kind of want a representation for the document using this scoring function.
So notice this says. if it's for a return in this document.
in the context of all of my documents is the Tf. Up in the documents, the Ivf. Across the documents.
but we just multiply them to get a number for that term in that document for this collection of documents.
Here's our example, the Aggie football game, Aggie, Aggie.
And so right now, if we're gonna do like the log based version, we just we just did the Tfs.
Where is that?
Here? We said it was 1, 2, 1, one right, the Aggie football game.
Okay? So we said. the Aggie
football. So now, what I'm gonna do is I'm gonna give you the whoops. Come back like the doc one vector.
with, like the Tf-idf waiting is something. That's what we want to get. Just a representation. That's the whole point of all of this. Well, the way we're gonna get there is, first of all, we need to figure out what was the Tf.
And what was the Idf for each of them, and then we'll just multiply.
So we said that the keyf was for the Aggie football game. Aggie, Aggie, let's go back here.
and again. 1, 2, 1 one. We, we all agree. This is the right. Tf.
we're cool with this 1, 2, 1, one. Okay.
so that was
1, 2, 1, one.
not put a little note here.
See
earlier slide. We just did that. Now. The idf, what was the idf for the
let's go back here. The idf, the idf, and the was 0. It occurs in every document, so it kind of has no value. So we said, the Idf.
0. What was the idea for Aggie. let's go back and look at our thing. Aggie was 5.
What was it for? Football? 7. What was it for games? Okay. So now we have the Ts score, which is, remember, if we for this document, these are exactly the Ts scores.
Okay, if we have a different document getting out of different Ts scores.
the Ivs scores are the same. Always right does. Ivf is always 0 never changes.
So now we just put them together. Tf, times idea. So one time 0
0
10,
7, 6.
So this is the Tia idea of weighted representation of the document. Okay.
compare this to the raw, just count version. We did
earlier, the raw count version would be one.
3, 1, one.
Now, it's 0 17, 6,
right? So it's basically saying, the has no meaning nuke it. it's saying, like football is quite rare. Seems to get a big score. Game is quite rare, gets a big score.
and he is also rare, but it occurs twice. So it kind of gets bumped up to 10. Okay.
that's it. That's our tf-idf weighted representation for this document.
Okay.
cool. Cool.
You're like what you missed last time. Once.
idf is, yeah, that's right. So this is, remember, this is across all docs. That's the whole point. Yeah, yeah, it's just like saying,
but I've heard once we're saying, it's so popular. So boring. Okay?
And again, these are all like cooked up numbers. So it doesn't, you know? Yeah, trying to sort of emphasize here, like, though, went away. And then Aggie, footballing somehow got really organized, based on their rarity. Okay.
but this is now a Tf-ids representation for the documents cool. Now we get a query. The football game.
Okay? So now we have to say, so, we want to do is find what is the cosine
of the query? And, Doc, one that'll be our similarity function we're using between the query and dock one. So the question is, what is that? Right? So we need to represent the football game also in our vector world.
And so let's go. I'm gonna rewrite this. The Aggie football game is what we're doing. So we have the Aggie
football game. Those are our dimensions.
We have a query, and we have Doc one. And so the doc, one would, we say, was 0 7, 7, 6, 0 17, 6.
That's the vector representation.
Now, the question here is always going to be. How do we represent the query?
And so I think on the homework. We just say we'll just do like the raw counts.
So the raw counts would be the football game.
0.
Okay. that was a design choice I made.
I could also say I could take the query, and I could weed it with the Idf scores from the collection if I wanted.
in which case the we know the idea of the 0. So there's actually be a 0 here.
Okay.
this is a design choice in how we want to represent the query, because the query is like, kind of different from the documents.
So what I'm asserting right now is just design choice. I'm going to represent the query. I'm going to say we're going to use the just the raw counts.
That's how we got that.
Okay, this. On the other hand, these are the Tf-idf weights.
Are we cool with that data? Yep.
okay. So now, we need to figure out, what is the cosign between the query and the document? And so did I copy my formula. I didn't
but remember the formula. which is
dot product of the 2 vectors divided by vector magnitude of each. Vector
so let's let's do that here. So yeah, that was. we know we just talked about that it could be raw counts.
It could be
idf weighted.
It could be some other way. for now we're doing this makes our life easier.
But why did I even write that? I mean, no one can read this. This is like a
write once, read never, or something.
You don't know what that says.
I don't know why I have all these extra slides where I need some blink slides. Anyway.
Let me let me watch. Watch my this. Let's see, we're gonna say. here we have that.
and let's drop that in.
Let's paste it here.
Beautiful. Okay, cool.
Okay.
okay. So we have the cosine between the query and the document. And we remember.
you know, the cosine between the query and the document is just the a query vector dotted with the doc vector over the query, magnitude
times the document. Magnitude. Remember all that. So let's do the dot product of the query in the document.
So the query and the document dot product, we just walk along each element one time 0 plus 0 times 10 plus. Plus.
Okay. So if we do all of that, what do we get
13 one times 0, I'll write it out. So
one time, 0, plus what was it? 10 times 10 plus
7 plus
okay.
13 over something that's just the numerator. We can stop product to them. Now, we gotta do the vector magnitudes.
So the query we just said was like one squared, one squared, one squared. So that's going to be square root of 3, and the other one is going to be like
0 squared, plus which is, give me all the numbers 10 squared, 7 squared.
6 squared. And what is that? 100 and 49 plus 36 185,
1, 85
square root.
And if you do that, you get something like that. 55. So
it's 13 h for
okay, they're like, if you actually, if you actually calculate this out, you should get something like 0 point 5 5.
You're cool. So that that's the cosine. That's that similarity score. That's the similarity.
We have a number point 5 5. Okay, we did it.
That's how similar that query is to that document.
jackpot.
Now. again, remember, like, when you do your homework.
I've had people like no joke. You have, like, 200 songs. Okay, it's nothing. I've had people like, literally, they're like Kev.
I need access to the supercomputer on campus to index these documents. I say, what madness is this
they have like, yeah, you scratch your head like, this is madness. It's like they have somehow, like 10 nested loops of like length, you know, a million. And you just what
don't do this. So
keep in mind. Remember when we're doing that I mentioned this last time. But we do the dot product. The only thing that can contribute to the numerator are the non zeros.
So the point is that you walked along here. You don't even need to look at these because they had zeros anymore. The 0 time. Something is always going to be 0. Only these 2 matter. So really, you only need for the numerator terms that are in the this intersection of the query in the document.
Okay.
so only footballing game are in both. because they're the only ones in both. They're non. They have some nonzero element I can
a little tricky, depending on the Ivf when you calculate it, you have the idea.
Okay? And then the other trick is like on the denominator, the vector, magnitudes. They don't change. So if you want to.
you could precalculate all those vector, magnitudes. You only have 200 documents. So you could just do one loop.
calculate all of those. And so then a query, time you just do a lookup
right to store it. Thanks.
Also. Keep in mind, like, when you guys, I know we're doing this all in these python notebooks you're welcome to add cells. You're welcome to add intermediate cells. Intermediate. You know. Sort of
you can store stuff inner. You have to. You have to run it all like this over and over and over again, like in one big block. Okay.
sudden.
Okay?
So we just did a Tfi idf plus cosine. That's like important. That's really great.
I wanna mention this I mentioned this last time. I just want to show you just briefly.
this is the cosine. Right? So we said, the cosine of the query. And the document is just the query vector dotted with the document, vector magnitude of the query
magnitude of a document. Now, I was just showing you last time. If you wanted to.
you could do it this way, right? You could normalize first, and then don't.
I'm just showing you this for completeness. Of course you're like, you know, about, like the associative rules of multiplication, you know, like you could do this. You've done this since you were probably like 4 or 5.
I just want to show you this, that, like you can also think about it if we wanted to, we could take the query.
The query, original is, what.
how do we write it out?
What was Aggie? So it's going to be 1 0 1 1.
So we have 1 0 1 one we could normalize.
And the the normalized query is
one over root 3, one over 0, one over root 3, one over root 3. This is normalized meaning, what is the magnitude of this vector.
one
meaning it lives on a unit hypersphere. Same deal. The document
was our document again. It was 0 17, 6,
0 17, 6, 0 17, 6.
Magazine was 1, 85.
So the normalized is
still 0. But now it's 10 over root, 1, 85,
7 over root, 1, 85, 6 over root, 1, 85.
and
actually, I did that. I'm gonna write it down so like.
let me hang on. Let me.
let's cut that out. Let's go back here. Let's go here.
space. let's do that. So that that document.
which is, we said, 0 over root 1, 85. What do we have? 10,
7, 6, over root, 185,385, route 185.
So if you actually calculate that out, that's 0. I already did this, Tim, this is like 0 point 7 4, the 7 is 0 point 5 one is 0 point 4 4.
So this is the like normalized document. Vector and so if you take the magnitude of this 4 4 squared plus 5 one squared plus 7 4 squared. it's going to be one.
Okay.
So the point of this is like this is a magnitude. The magnitude is one. And laser lives on the unit.
I like to write hypersphere because it just sounds cool.
meaning all of our vectors. Now they were like different links. Now they're all pulled back. And they live on this like sphere, where the magnitude of every vector, is one.
Okay.
you guys know Vana White
wheel of fortune. I will, fortunately. but still on TV.
Oh.
yeah, Merv Griffin.
I think he's the guy in jeopardy and all that. He's dead. Now I think you talk about making money. You have a TV show like that.
Another creator. Yeah, Alex, for it.
Alex Trebek. He was a real one.
It's pretty good.
I like. He's kind of grown on me. The new guy, Ken Jennings. He's very good, very good for you.
Much jeopardy
at 1 point when you were. If you were like me, like when I was younger, it's like, if I'm good at jeopardy that will translate into success in life. I'm like, no, it just means you're like an oddball who knows a bunch of weird trivia.
And like maybe even in person.
I mean to be honest. Because
also think about this free smartphone, free Internet. Yeah, it was helpful. You're you're you're meeting someone restaurant bar. You have lots of trivia, hey? Did you know?
Yeah, did you know, you're like, yeah. I looked on the movie facts. And I using me, everyone knows that information storage in the tree wall.
Thank you. And you take it for granted.
The whole history of all human knowledge lives in your pocket.
you know. Oh, yeah, Tiktok,
Chad videos
that's amazing. Think about it. Like, if you grew up outside of Brian 150 years ago.
you have a few books in your house. Maybe there's not even a library. The the Carnegie Public Library downtown. Brian wasn't built until like the night. Early 19 hundreds. You can't even libraries.
but maybe they had a better bet. Maybe not.
I think not. The point is, you have access to all this information, and you don't care.
Shame on you!
We should all be studying Shakespeare. We should all be learning about like astronomy.
Last time one of these times we talked about like, why are we here? Yeah, keep going this class, right? We're gonna crack. We're gonna crack the code in this class.
Okay.
that was one document. We don't care about one document. We care about lots of documents. We have a new document, the game game game.
I know these are kind of dumb right cause all I can do. What was what were my the football game, football game.
the Aggie
football
game. So now we have a Doc 2 notice I'm remembering, and I still know that, Aggie, because this is my vocabulary. Keep that in mind sometimes you're like, oh.
all I need are the words that occur here doesn't really, you have a whole vocabulary. These are drawn from.
So this is the game game game game, and then we have to
tf, log it.
So when we do that transformation, it becomes one but sorry
when we do the when we do the Tf transformation, we log the one plus.
So that becomes 1 0 0 2.
That's the log. Tf. then we need to know what is the Idf. let's go back and remember that doesn't change
0 17, 6. We calculated that
0 17, 6,
but then 56. There, 6. Notice
the same
for all docs doesn't change the exact same 0 17, 6
gives us the document 2 vector
0 times 1 0 10 times 0 0
0
12,
a different document, 0 0 0 12. So they'll got nuked away. And basically, the query is only, or the document is only about game. Now.
0 0 0 12,
we can do the same thing we can say, what is this
similarity?
Well.
sorry. What was our query? Again?
I lose my mind here. Where's our query? The football game
and the football game.
He's gonna write it out. The Aggie
football gain bom, bom, bom. query is the football game. 0 Doc. 2, we just said was 0 0 0 12.
So then, the similarity of the query in the document. we just dot product the tops one times 0 0 times 0 one times 0 12.
The magnitude of the query, one squared plus one squared plus one squared square root.
0 squared plus 0 squared plus 0 squared, 12 squared square root. Well, that's 12,
one over root 3. You put that into your magic machine. And this like point, something very similar to our other one 0 point 5 7
done. Okay. do you need to know one over root 3 for the midterm? Of course you do.
Kidding. It's kidding. No.
if we ever did, we would just we would you would, just, if you had a midterm and you got one over root 3, and you were comparing it to
13 over this. You would just say.
I don't know exactly, and say, one of these is probably larger, and that would be the one I would rank higher.
I don't know. So we said, this is point 5 5. We said this one was point 5 7,
so let me hang on
today. He's up all my, he's up all my blanks. I didn't copy them so stupid.
so we would say, so our rank
would be. First would be Doc
2,
and then we'd be Doc. One, because it was a 0 point 5 7 and a 0 point 5 5.
So we've now ranked the 2 documents. According to this query. we did it? We win. Okay.
this is the homework. This whole, like second half of the homework, is basically fabulous. Tf, ids, new cosigns do ranking
questions, concerns
freak out
confusions.
worries, self-doubt.
Existential inks. Yeah, okay.
yeah. Right now, I like circle.
Sometimes I haven't. Sometimes
I would like this to be our circle of trust.
Wow! You're reporting.
That's the the
circle of trust.
No.
And I like that because
the wrong way.
What's all them right now? Maybe wait. That's just
doesn't matter how far you're going in any direction, get brought back down to the same
interesting. Oh, interesting.
I don't believe I'm being allowed to engage with that.
Okay.
I spend a lot of time on this, because. yeah, not on the hydro sphere. I spent a lot of time on. This is so important. Okay? And you really. So what I wanted to make sure you can do is you can do all of this stuff.
It's like one thing to do it like this. Make sure you can do this just like you could write it out. but then you have to do it in the homework. You do it in code a little different. Okay, you sort of reconcile the 2
and kind of related to all of that is is really making sure that like, you got this stuff locked down because this notion of vectors, similarities, representations. We're gonna keep hitting this over and over again.
Okay, so it doesn't just go away. We keep hitting it over and over. Okay.
so we're good. great, awesome. Okay? So
this is the vector, space model, okay.
traditional vector, space model
classically attributed to Jerry Sultan, but with contributions to many others. Okay. what I thought about is, I remember when we preface all this, I said, well, we're also gonna talk about this probabilistic model.
That's kind of debating how we're going to spend our time. And because we all are like chasing the the the golden ring of Llms. Right?
What I thought I'd do is instead, just show you
what this other rancor is. But I'm not going to give you kind of the probabilistic theory underlying it that there is.
So that to me it's not that interesting.
Before you guys, this is a session about Lucine. Anyone know Lucine. anyone use solar. SOL. R,
okay, Lucine is basically the like, open source. Apache search engine.
Lucine has used.
I mean, Google doesn't use Lucine. Okay, being doesn't use Lucine. But, like all these big companies, use lucine internally. A big open source search engine. There are extensions. This huge ecosystem. Okay? In fact, we may have to use Lucine, maybe let me think about that.
Hmm, hmm. Question. Mark how you draw like a light bulb. Too bad.
you know. I'll think about this. We'll come back to Lucine.
but that's why this is from several years ago. That's what you say. There's something new scores text how they do ranking instead of the traditional. Oh, you can see that
instead, the traditional tf, idf, we just switched to BM. 25.
Okay? But
So in practice, all the stuff I just showed you.
you're not really gonna use. Okay. it's ever gonna use this fancy thing called B in 25, Eve. 25 was actually then in the 90 s. Does anyone know what it stands for?
And don't tell me what you think Bm. Stands for we all you guys read about or heard about the the diarrhea flight.
If you haven't heard about it, don't look it up.
There was a flight from Atlanta. It had to return because someone had explosive diarrhea in the plane
down the
imagine. Now, everyone else in the whole vacation.
I mean
B and 25 States were dead match. And you can do and get 25 s.
It was their 20 fifth version of this scoring function.
So these are researchers who are trying to figure out the scoring function. It turns out this thing was invented in the 90 s. Replacing something from the 70 s, and yet
we're gonna come back later in the semester. And we're gonna compare modern deep neural network rankers
that lose to being 25.
Okay, that's changing a little bit now. But 4 or 5 years ago. Bm, 25. Still competitive with the most sophisticated
neural network rinker.
It's just something someone cooked up.
Okay, very powerful. So I'm gonna show it to you. It's not on the homework. I just want you to see it. And in practice we may see this on a future homework in your project we'll talk about later. This is the kind of thing you might want to use as a baseline in your project. And the reason I want to show you it
because I said.
you already know all these concepts, we're gonna use them over and over again. Notice. Okay, here's the formula.
oh, but Whoa. what? Okay.
that's easy. So
this is just a score or query for a document. This is just like the co-time. We're in a document or the similarities. Okay.
Summation. Aha. we know. Idf. okay.
what is F? Oh, hang on, hang on! What is F.
I didn't put it on there, I'll come back to the.
So we know that
that's just tf, we know that.
And so then it also just has these other things that has K, the
this and this
it has some other stuff.
It turns out, B is just the parameter. A number K isn't the parameter a number.
Okay? So just a number, you picked
the and average. This is the average document length.
Okay.
this is the length of this document.
So some sort of document normalization zoom to vary like the length of it. Don't worry about it for right now.
but what I want you to see this is sort of like a modern rancor used in Leucine. And again, don't worry exactly again. We're not giving details like, what does this all mean? Exactly.
though the constituent points are, it just takes Ivs and Ts combines them in a slightly different way.
Okay, so if you understand vector space model.
you're basically halfway home to doing something like being 25, which is a slight variation. Okay.
I give it, you know.
I'll say this. Don't worry.
We may. We may come back to this
and talk about all these details. Don't worry. I'm trying to show you we can build on the stuff we've already learned. Okay, so last bit. What is next? Where are our concerns? What's next?
A quick quiz.
3
out in 1 min.
right? It goes out. The end of class homework. One is, do win.
Wednesday.
Readings will go up over the weekend other than that.
Have a great weekend. Folks see you. Then
it's good.

Oh, yeah, someone hang on. This is now, okay, gang. So now, what I'm gonna do make sure I don't lose my mind here. Okay.
okay. So I got my zoom here. Hang on. I gotta join the zoom from here.
Join
schedule. No, no meetings
and edited. Start
no audio.
No video. Say, participant.
share, content, share my screen.
start broadcast.
That's good. We don't need that.
You can see that. But you can't see that. Yes.
And then we're gonna start recording.
It is recording.
Okay, hey, this is this is like a mirror mirror. Okay, so this is gonna flash on and on every 5 or 6 s, or 10 s, or whatever they miss better than nothing. Okay? So in theory, right now, guys, I have a zoom going, which from theory is recording. I can't attest to the quality of the audio or the video. So zoom experiment, we'll see how it works. Thanks for playing along
canvas slack, homework 0. You got it cool, cool, hey? Did you know that there are readings on canvas?
You did not
their readings on canvas? Did you do the readings? Who did? Who did show yourself?
What's your name.
Gamble?
You want to? What was your interesting fact? Because I just read it.
chile, but you haven't been. You wanna go bike riding. I do read that. Stop, and I try to remember it. Ansley or Ainsley Ansley. You've been called Ainslie many times. And you hate people who say that I don't need it
is, don't hate them. Just cancelling one. Yeah, just very disappointed. Okay.
okay, good. That's great. So yeah, yeah, so there are readings. I encourage you to do them. Remember, a quick quiz one is gonna go out on Friday.
And basically, it's gonna cover what we've talked about this week, including some of the readings. Okay? And again, the quick quizzes are not to freak out about. They're just if you're keeping up. They're sort of self checks. But if you take the quick quiz and you don't know anything
that's just again a reminder to yourself, hey? Maybe I should go back and like, check up on that stuff because it's important. Okay.
any other questions concerns comments freak out on this.
It's on the back. Just say something I was gonna hear. So right now, someone in the back is gonna it's gonna say something. So go ahead.
Say a whole sentence for me. My name is desperate. said Hi, my name is Espin.
and I don't know if we heard that we'll check it later. Thank you so much. Okay.
we're gonna quick history lesson. So again, slides are available for class, you know, grabs. If you wanna follow along, I'll annotate them in class. And hopefully, we'll make this somewhat interactive. We'll see how that works. Also, I may have to fix how the camera works, cause I feel like it's not getting my best angle right now, it's kind of looking up.
Yeah, think about this. Okay. Anyway. anyone here seen the movie? Oppenheimer?
Did anyone walk out like Logan? Paul, they're just talking. They just okay.
Right? It's a movie, right? Is Oppenheimer to start punching people or something. but if you saw it. we're gonna come back to that in a second. So then, ever bush 1,945, writes, as we may think, this is one of your readings.
This is what he looked like. He was the scientific director. I don't know his exact title. Basically, he was kind of the science director who made sure in your context, the movie Oppenheimer, the Manhattan project had scientific support. He was like the guy above all, those people. Okay.
he later on. He basically led and sort of helped form the National Science Foundation.
which is for the last, however, many years, like the main funder and supporter of science in this country, and like many, many, you know, you've heard about like the like, Arpanet Darkpinnet and the Internet. Nsf has funded major major stuff, including the researchers, the research that led to Google, for example. So a lot of this comes out of the Nsf. The reason I mentioned this is because if you saw Oppenheimer, this guy that's that's bo that's that's Van ever Bush.
Aka, Matthew Modine. If you watch the stranger things, he's the guy on stranger things this recent season. I think if you've seen this, this guy is Vana Bush, go back and watch it. That's him. So you can put it together. The guy who wrote, as we may think.
okay, just to kind of connect it to our our daily live reality. If you go read what he wrote about in 1,945. His big idea was this called Mimics? Okay? And so you know, we said, it's a mechanized was interesting about this. He had not foreseen like a social media, and like sharing. So in his mind, it was a private file
also. Once you notice this. It's mechanized.
So he's writing this before. Before the a practical people knew about like a theory of a transistor. But before we had, like an actual transistor. This is like, you know, whatever you know, decade before we have an integrated circuit.
Okay? So he's thinking, like, mechanical. Okay? But the idea was. It was a device in which an individual stored all of your books, records, and communications.
and you can consult it with exceeding speed and flexibility.
and it's an enlarged intimate supplement to his memory.
Okay, so imagine in the olden days you're sitting at your desk. You have some books. Forget all that. You don't have a computer right now. His idea is, you have.
notice, it's like.
it's like years. This is pre-integrated circuit free Transistor. But notice, what we have on top is basically these translucent screens look familiar.
No, excuse me. Okay, what is that?
1945? Okay?
So again, forget it. Room dog, we're thinking about it as like an actual desk. Right? So forget about all this stuff that the idea. And again, he's not thinking about sharing across people. But it's kind of like forerunner for modern
web, modern Internet, modern. All of this right? The idea is, I don't have to remember everything or look in a book. I have this device. I have a phone. I have this ipad on my desk which allows me to access all this information. Okay.
super super fascinating. So next time you go watch Oppenheimer.
And this guy starts talking. You stand up and say, that's the minx guy.
That's the guy. And everyone just say what he'll say, no, he's in the next guy. That's Vannevar Bush. Okay?
So anyway, little history lesson, we're gonna do those throughout the semester. We're gonna touch on like the characters behind all of this stuff that we're talking about. It's gonna be super good. Okay.
let's go. Basics of search.
Still, recording is a git. I don't know if this is working or not.
Okay. okay. So my plan is also, I'm gonna walk over here for a second and see if you can still hit my audio. We'll check this out later. Not an answer where we might.
So my plan is that and we, you guys need to push back on these. Once my thought was.
I want us to use a record store is like our big motivating example for the semester.
and my ideal was this would be kind of like the old school Netflix, where they would mail you the DVD.
Are you guys so so like young, you never received a DVD email.
Not
that was like, your grandpa got to the VPN, yeah, back in the Stone Age. So
breathe and screaming. we would get Dvds mailed to us.
and some Dvds.
that was their original business model. Yeah. So if you want to see the new movie.
you put in your queue and then you wouldn't always get the first thing in your queue
right? So you have a queue of like 20 movies. And so like, I really want to see whatever the Barbies coming out, and Oppenheimer coming out on DVD. And I put those 1, 2, but everyone else did. So. They go to 3, 4 video. They come in the mail.
I want to know my own time. It comes in a little sleep. I drop in the mailbox, it goes back, and the zoom in the next
right? So what happens is this is like 24 into you, because, first of all, you're in the media. You consume. You don't watch 2 h things. You want to have 2, 7 things.
and you swipe right through it, mobile interface. And then we're going to mail people. Lps.
Oh, these are back. Right? You guys listen to records. Who's the record player.
Yup, so? Or we can make the 8 tracks. There's no tracks. So my idea is really make a record store.
Are we good at this? We can change it out if you don't want to be a record short, be something else.
It's a record store. Thank you.
We need a name.
Do you name for our record store? This is a big challenge. So on slack somebody, not me. Someone make a thread.
With names. You guys start putting in
possible names for our record store. We, the class, will decide. It'll be the name of our record store for the duration.
Okay?
So we're gonna do over the course of the semester. Basically, we're gonna build certain capabilities. So if you're a user, you wanna get an album. You gotta come to our website. And you're gonna type in stuff. We're gonna do search and we're gonna find what you're looking for. We're also gonna do recommendation capabilities. So you're gonna come to our store. It's like, Hey, you've already listened. This album here. Other albums. We recommend this artist here. Other artists that we recommend.
So for the next few weeks we're gonna search. Then we're gonna do recommendation.
And then basically around fall break
there about, we're gonna go back. We're gonna do our advanced stuff. And then we're gonna hit. Our lom's okay.
Now
search basic basics. So again, the key distinction between search and recommendation in search. We assume that we have, like an actual query, like an enunciated thing that I'm asking for.
And so, for example, if you came to our record store. What are some of the things that you might be looking for?
Regular versus public records?
So typically when you go to Amazon, and you're like, I'm gonna buy something. You say, Amazon, something to buy.
Yeah. Like an artist. Give me an Ella concrete, real example. Frank Sinatra, Frank Sinatra.
what do you really want to show up to be like for you?
No, maybe maybe you want. What do you really want?
You want to album by yourself?
Yeah. Okay, anyone else.
Do you want to album by artists?
It's it's Friday. It's the weekend you want a special album to listen to on the weekend
postal service. The postal service. Is that a band? Yeah, okay, I don't know these things.
so these are different groups, different artists. Okay.
you.
I'll be playing Christmas music. Great Britain's not your albums.
I would like the latest pop album by the artist
Taylor's name.
I would like a good album to listen to when I am feeling really sad on Sunday nights.
and keeping up the week ahead.
I'm like an album that I can put on on a Saturday morning as I'm getting hyped for the game later in the day.
Yeah.
I would like an album. I can put on Saturday nights
when I'm really depressed because we lost the game
come down from the High.
So what I'm trying to get at is
in my mind. Typically, we have a query. It's the thing I'm asking for. So it might be
we might come in there and say. Frank Sinatra albums
and our documents on the other side is, we imagine we have. I'm just gonna always draw documents like this. We can imagine we have lots and lots of albums. Okay? And typically the query
in our documents. We're gonna somehow, you know, the query is in somehow. And then we're gonna get back from these documents. our hits or our matches.
What I'm getting at is underlying. The query is typically an information need that you don't express the actual words you type in.
And so when you ask for Frank Sinatra albums. The underlying information need may be something like. I want those like classic fiftys, Crooner vibes that make me feel good on a Sunday.
Frank Sinatra albums. Okay, if I go ask for theaters with newest albums.
the underlying information. Need you know the thing that I'm expressing there.
maybe a little richer than just the keywords I typed in. Let me give you a different example. If you're on your phone.
I guess no one ever does this anymore. If you were like to tighten in weather.
Does anyone do that anymore? You said an app to tell you the weather? Then, as you Google like weather.
so the query is, whether but what does the information mean?
Where I am when I am. and not just the weather this moment, that you probably want a sense of like the day's weather, for example. So they're getting at is when you type. In weather
we, the system, have are trying to infer the underlying information we
with oftentimes very little information. So we may know the actual query. We may know a little bit about your context where you are when you are, etc., etc., the language you speak, and so forth. But just keep that in mind a lot of times. We're gonna focus on queries and documents, but really underlying it, the information need. That's the thing that we want to do well on.
Okay. because if we can satisfy the information need. our users are gonna be happening. Okay.
So going back just a second. So again, we have our underlying information need. But typically we express it as a query. So again we have some query, Q. We have some docs
which I'll I'll draw like this.
and what we need to do is somehow represent the query so as the query coming in as text.
Okay, so typically, we have some like function that takes in the query and then gives us some kind of query representation.
Similarly, on the documents, we have a function that takes in a document
and gives us a dock representation.
And so the query representation could literally just be like the words and the queries. Okay, okay, the document representation as well. We'll talk about different ways to do that. It could be. We look at the words in the document. Okay.
as we go through this course, the idea is, we want to find a query representation and a document representation
such that we can build a comparison function. So, for example. we can summon
Hello. So we somehow want to be able to take a similarity between
this query representation. this document representation. and maybe give us some kind of score that we can then rank. The Doc spy! And so the whole point here is like these docs then live in.
We recovered. We recovered also. It's not. It hasn't gone away, has it?
But it hasn't been flashing. Yeah, up until back then.
up until right then. Nice. Thanks. Jeez, you gotta talk. Huh?
So yeah, I realize there's like my faces down here. Yeah, the way
so typically, these docs will have this representation. We'll stick them in our index. We'll score them, and then we'll get out our hits, which could be like, you know, it could be like our top K best matches or something like this.
I mean, it's a little abstract right now. And so we're gonna try to fill in the details. Typically, I want you to think about like this.
The right hand side is typically offline.
The left hand side is online. So what I mean by that is typically, we know of documents. So if we're willing to search engine over our file system, we have all those documents. We can analyze them. We can build representation, put them in an index.
If I'm building a web search engine, we go crawl, select all these documents, we analyze them, index them done. If we're running our album source has no name
the will. In a couple of days you go take all those albums we figure out like the title, the Artists. Here. It's the what we analyze them all offline build. Our index ready
online is now the user comes and issues a query. So at query time
I have my pre-built offline index.
and then, at query time, I can do this similarity between, for example, this representation of a query, this representation of a document and get our matches.
Okay? So right now, like, if you think about like.
how do we build those representations? That's like the key question. Okay? And so as we move through this course, we're gonna start out with very simple super like, not superficially, surface level representations. Okay? But later on, we're gonna replace like, not a query is not just going to be the words like Frank Sinatra.
At least we're probably going to end up having that be represented by some like big, like 1,000 dimensional, dense embedding. Vector
okay? And so Whoa, what? What?
And documents are not just going to be some words we want to understand, like the concept of the documents and this deep representation of the documents. And so we're gonna have some representation of a document which might be some 1,000 dimension dense embedding. Vector okay.
anyone here ever worked with embeddings before?
What? What am I talking about? Embeddings? What is that
like? Basically inventor. That includes whatever meeting
a vector, that encodes meaning.
what's your name? Aaron? Thank you. Aaron. a Aron. Okay, go ahead.
Yeah. So like, for example, you use it to train different language models. You essentially take a bunch of beautiful. Yeah, yeah, yeah. So if you follow the law awesome. But the main thing is like this is not the problem that we've been thinking about since, like the 1,900 fiftys and sixtys. How do we build these similarity functions
very like service level. Now, we're through all this like deep learning. All of them aren't as rich representations. But the fundamental ideas are the same. Okay, they haven't changed very much at all. Okay, so
classic side note here, classic Ir, we think about that information need. That's what I was talking about. Like when I talk about Frank Sinatra, what do I really want? Right?
Okay, if I'm asking for the weather, what does that really want.
Now, some of you may have them readings. And so Andre Broker was talking about in the context of web search.
So can you imagine what was Ir before.
But Google is like in the late 90 s, so what was Ir searching Pre, Google
pre-war.
yeah. Library like, library conscience? Yeah.
a lot of it was librarians. Okay? So there weren't regular users using search engines. They need librarians using search engines. And there was one other big consumer of Ir, or search technology pre-web, who guess who that was, and it made big money
right?
now, my dad was a lawyer. My brother is a lawyer
law firms had lots of money. They would pay big money to have these special custom services. But if you go search legal stuff to find previous case law, right?
So around the web coming in the 90 S. Andre Broder says, wait a minute in web search.
It's not just information need. So information need is like, I need to learn about this thing. Right?
I want to learn about
backpacking in chile. Okay, I want to learn about that topic. So that's classic information needs. So he said, yeah, we still have informational
queries.
I want to learn about a thing. Okay. we identify 2 other kinds of already in our in our like a search of our album store.
Can you name those other 2?
Yeah, there's a transactional and navigational.
my gosh. say for the people in the back there. Loud. transactional. and
navigational, navigational.
What is transaction.
hey?
This? This transaction right now, is like we pay in the money tab
entertaining. That's a transaction. Okay? The transaction is, I want to buy something.
I want to download something. I want to get something. I don't want to learn about it.
Right? So if we go and say, Frank Sinatra albums.
informational query would be frank sinatra albums. Tell me about those albums. I would like to learn more about it.
Who is Frank Sinatra? What album you need, composer, sing on
transactional is friends, not albums. I would like to buy that. Give me a link that I can give the Mp. 3 or a button mailing it out.
I wanna transact. Okay, we're gonna have navigational.
I wanna I wanna go to it. And so if you guys know, like in the olden days
so you know, there was Google that you've got who has an own search engine. And you look like the number one search on Yahoo was
Google for a long time. One of the top searches on Google is like Facebook or whatever. The kind of the interesting service of the day is. So navigational is if I go Google for Facebook, I don't want to learn about Facebook. That's informational.
I don't want to give Zuckerberg my money transactional.
Just take me to Facebook, get me there.
Okay. so just keep in mind
when we think about information need. Rotor is saying. Actually, there's different kinds of needs, informational, transactional, navigational. And so you got to think about when you're building like. For in this class, your project. Okay.
we have. Think about what your users want and what kinds of queries those are, because you're gonna have to be able to
disambiguate and serve them. And so what I mean is, if you come to our album search and someone types in Frank Sinatra albums.
do they want to learn more about breaks, Notch albums. Do they want to download them or buy them from us?
So I'm sorry. Okay. maybe later on we'll add screening. And they're going to navigate to the screening page where we can give them the albums. Okay? So big key questions, we're going to talk about the rest of the semester.
How do we build the representations of queries and docs with that function? How do we compare 2 things?
How do we know if we're doing a good job. it's a little different from a lot of your classes. I think so
like when you guys took it like we took like formal languages through automata.
He took the instructions about it.
So how do you know your data structure is good.
efficient, sufficient. So what would you be measuring
space, complexity, time, complexity. Yeah.
you guys talking about like, O event, login, all this kind of stuff in square. Okay.
we care about in this class. We care about efficiency, of course. but a lot of times we're dealing with real people who are using our system.
And so I can't ask you like, what is the complexity of my users?
I want a lot of users. And what do we want our users to be
real real people?
Do we want them to be sad.
Yeah, we ought to be happy with the product.
When you guys are building out
the query representation, the document representation, the comparison function. All the features on our album search engine
are, are customers happy or not? And so we're gonna have to learn how to measure customer happiness in this class.
the user happiness, like what the Cs class one.
So we're gonna come to that later.
Okay, so let's get into it. Let's use a little more real stuff here for a second. So what times got screwed up?
Okay? So right now, we're gonna do. We have a reference store we have. It's a front page like a landing page. Okay.
you come to our our store, which has no name but which will have a name, maybe. Even now someone's posting on slack.
That would be wonderful.
We're all going to support Boolean queries, Andzors and knots
keywords so you could just have a query, Karma.
run it love or Song. Taylor, Ann Swift, Taylor, and
not Swift, etc. Etc. Based on those queries we give you. Imagine, docs, this is going to be one of the classic
original search models. Boolean Retrieval.
Notice, Boolean queries, return sets. You're used to ranks. I search for something and I get back to top whatever, and I click. The first thing
this world is users for Taylor and Swift. We find albums. We give them that to you
in no particular order. just like purely. We're using your diet.
You don't order them by the most recent, the most relevant it just here's the stuff that matches you. Figure it out.
and and you make this insane. This is basically how search worked into the 1990 s.
this was the classic thing. If you're like a librarian, and you were doing these, you were writing these very complex Boolean queries. And so, in fact, if you went to a library up in 1994, you know I can't find this
some book to help me in this class. The librarians sit there and talk to you that we're looking for exactly to kind of understand your information.
Then they would write because the the query lens is a little more complex than it's booing, and even Boolean is pretty complex for regular people. Okay, do you type in something
and give you back 20,000 results?
Okay? So then you would add one more, turn something in something to give you back 0 results. Oh, man! Trying to find the right queries to give you just enough stuff you could actually make sense of right?
So here in this case, imagine you search for Taylor and Swift, and our album search indicated, returned 1,000 albums in no particular order.
and you're like, let your customers figure out the good stuff, and the taylorflow is not going to be the top.
They'd go insane right? They would drop and leave you very soon. Okay. So here's an example. Alpha.
Artists, we had different attributes. The artist the title of the year, the track listing the lyrics. Okay, so we're building our search engine. Let's go into our index. Remember, only stuff in our index people can search for.
So what do you think is important for our users to search for. What do they care about us?
They hear about song titles. You hear about the artist. I won't take some entrepreneur. so if we did Frank Sinantra? We want to say, Aha!
Only certain artists
attribute. But to do that, what does that mean? Let me take the frank Sinatra as a purpose to look in the Frank Sinatra index.
Okay? Or if I'm looking for by year or track listing
a lot of times, we take all of those stuff. And we just merge it all together.
Okay, one big index. Alternatively, we could run an index for each one of these
and run a search on each different index.
Okay? So there's lots of design choices we have to make. We're going to simplify things. And for this class and kind of just moving forward to make our life easy. We are going to do wooing queries, but we're only gonna do them over song lyrics. Okay? So right now, we can't really support Frank Sinatra albums
unless someone in the lyrics is talking about Frank Santra. Okay?
But we could do that separately.
So now we're doing Boolean queries andsores and nots.
We're doing only oversong lyrics check.
So we have our first data structure of the class. You get a matrix. I told you, we use a linear algebra, not really linear algebra
get a big matrix. Okay? And so basically, we're gonna have all of the keywords.
So right now, all the keywords that we know about, and all of the documents or songs that we've encountered. Okay? And so the idea would be like this, the first song
also, this is binary. It just says, this term occurs in this song. So I'm just going to make this up. You can imagine like.
so the first song has the word any choose and welcome in it does not have the words believe midnight staring or zebra.
Okay. Song 2.
Does have any belief. Love zebra? Not the other terms.
Okay? And I will just fill in.
Okay.
we now have big term document incidence measures.
Could you construct this for whatever domain you care about webpage email files on your file system
products. etc., etc. Yeah.
you just take all of the things the docs. Let's see songs.
You just read song, and you just see which words show up.
Okay? And we're done
now in practice, how big is this matrix?
In other words, how many unique terms. Do we know about how many unique in this case songs or docs, do we know about
a lot? Well, it's a lot really like millions, man
like millions of terms. Notice here, too, what kind of terms did I pick
single words? So that was a tokenization, a parsing decision I made. It's another point where they're designing these systems. I decided I was gonna maybe notice they're all lower caps. They're all in English.
right?
It's not always that easy. We'll come back to that. That depend, that will really affect what goes here. This is big.
How many songs do you have once?
What if this were web pages. How many web pages are there?
At least 20 certainly off the screen here billions, trillion web pages, how many terms, presumably millions of billions to billions returns.
So the point is this matrix is really not this kind of matrix. Really, it's just like, you know, 1 million by 1 million, or at least hundreds of thousands by thousands and thousands. Big big matrix.
Okay.
so what are the characteristics of this matrix?
Like, I'll do a simple one. Is it a dense matrix?
Or is it a sparse matrix? Not this one. but the
probably sparse. So what I mean by that dense or sparse didn't mean we have lots of oneness.
So if I really look at this thing by like hundreds of thousands of terms, like 100,000 songs, that's the whole thing, are they mostly ones or the most zeros.
What's the Zeros? And why are they mostly zeros?
Because, like any song only has a fraction of all the words
right? So like
any song has a number of what? 30, 40, 50, maybe 200 words.
How many words are there? Total hundreds of thousands, if not millions.
So really, this thing's going to be super super sparse. Keep that in mind. That that kind of informs the design of what we're trying to do here. Okay, so
given this term document, incidence matrix. Can we do Boolean search or Boolean queries on this?
Of course we can. So here comes the query. The query is midnight and staring.
Okay, so I come back to my index.
I need midnight and staring. And you guys ready for this.
he's oh, crap!
Oh, crap! It's not working. So I gotta I gotta get the whole thing.
I didn't write straight
copy
now it's shrinking.
Oh, you try to do it. It's hard. Did I get the right ones? Midnight serving 0 0 one. Yeah.
said. can you put
the end of the thank you. Oh, I have
midnight steering.
So how do I do this? Be brief.
I have 2 bit vectors, I just and the 2 bit bit vectors. So we just and them
0. 0. How about that one.
Oh. do
so. We're just Andy, right, we just and
and the 2 bit vectors
so midnight of steering, there's only one document that has both midnight and steering on Psalm 6.
Right? So the result here is
Song 6. We did it, we just and attributed vectors. Okay, we can do query of not love.
So let's just do that. Here we have.
Here's love. Let's grab it. Let's let's let's do a ride.
You can kind of see what I'm talking about.
Okay. So there's love
vote. Both
love. So let's do not love 0, 0, 1, 0, 0 one. So wherever that is, song
song 3 and Psalm 6. We did it.
Okay. And you can do, believe or choose. You take, believe and choose you or them. You get the results. You're done. Okay?
So I would count that as a big victory. For like Day 2, that
we can build a big boolean term documents in this matrix.
we could do bitwise manipulation, and we can get results. And we now have a Boolean search engine.
It's pretty sick. Okay. However, you notice, there's a problem with this, which is in practice right? There's a really long bit factors there, lots of zeros.
right? This was very, very sparse. And so, coming back to our efficiency story. It seems like if he doesn't make a lot of sense
one in terms of just representing this big matrix because we're storing all these zeros
and 2 to make make our life difficult. If we're trying to do bitwise operations.
Okay? So the we're never gonna use that big matrix.
Yeah, it's sparse.
You know why, store
all those zeros?
it's also very large.
So what instead we can do is we just store the ones just store the actual
terms that you know the terms that actually occur.
So instead, we'll build this called an inverted index, or just an index. But for
kind of historical reasons, we call it inverted index. And basically, instead of storing the big matrix, we'll say, Aha! We'll just store the ones. So any let's go back and look at any
any of them.
Psalm 1, 2 and 5,
1, 2, 5, 1, 2, 5, 1, 2, 5, 1, 2, 5, 1, 2.
So I'm gonna write like Doc one
Doc Tube like 5. And for now
there's lots of ways to store this. We could store this as like a fixed length array. But for now I'm just going to store it as like a little linked list.
Okay? Done. Now believe
it's in Doc. 2 or song 2 and song, 3.
Yeah.
Doc, 2, Doc. 3. So choose.
Doc, 1, 5 and 6. I dock one. Doc, 5.
Doc. 6. Okay, this is our inverted index. So what you'll notice is
if in our original matrix, let's say it was a million by a million, what's a million times a million
invisible?
Thank you, or a trillion. So this matrix, if this were.
if this matrix were 1 million by 1 million.
we have a trillion entries.
And if on average, let's say a song only has, let's say 100 words in it. So there are a million times a hundred. So there's a hundred 1 million ones.
But we're storing a trillion spots.
So we're wasting all of this space. You buy that
now in our new world, if we have on average a hundred terms per song.
instead of throwing a trillion. Now we're back to just storing the 100 million or whatever much yeah 100 million, much smaller. like orders of magnitude smaller.
It's a big victory for us, much more compact. You only score this.
you're like, well, storage is cheap. Memory is cheap. Who cares? Man
like? Yeah, if it's a trillion versus, you know, on the order of millions. There's a big difference, right?
But 2. How are we going to do queries now?
We started late. It makes me so mad I can't keep you late. That was the deal we made the deal.
How do we do queries on this thing?
It's a little more complicated. So what was that query? Just did midnight and steering?
Let's make it like any and and believe, okay, any. And believe.
But I have this here. So now we have to have actual like pointers.
and we're going to advance them. So we can do any notice. II sort of these in order of the Doc. Id. So imagine I have a pointer here, and here.
So is that any and believe do they match?
Those are different documents. So what do I gotta do? I'd be in this one
any and believe, yeah, 2. Now we're ready.
Wanna see the rest of the game.
I could have a little look ahead and imagine, Okay, we're moving both forward.
Do they match? Okay, now, give me the thing we're done.
So I'm not into the details of this, because in practice on your homework and stuff, you're not actually gonna do any of this winter stuff. You'll just say, like I have sets and intersect them and get me out of here. But in theory, that's how you would do it. Sort of clever kind of pointer advancement.
Okay. so
wrap it up. Boolean. Retrieval Day one. You can build your own search engine. It's a miracle.
Why is it good if you do know how to write a bulletin query. It can be super precise. I get exactly what I want.
Typically, they're very efficient in practice to implement it. Why is Boolean bad? And why are we not going to do it ever again?
One user must understand Boolean logic. Some people in this class don't understand Boolean logic, and your Cs majors think about regular people.
feature family, like I mentioned before, you either get nothing or you get thousands of results. And a key thing is, what about personal managers. Booing is very rigid. It has no notion of closeness. Okay.
we're going to fix that next time. We're also going to talk about
what are the right tokens to put into our index.
How do we parse this? So think about that. See you all on Friday.
I hope the recording works.

Audio start video.
Okay.
we'd love to have them. We have been recording. It was recorded on Wednesday. I've heard someone actually watch the video. So the audio is reasonable. Okay? So the video does live. So if you missed class, you can go check that out if you're gonna Miss class.
Do I want you to email me
if you want to? I may not respond. I may respond. I like, I really care about each one of you. And I wanna make sure that you're healthy and doing great. But if you're not gonna show up to class just like this, don't come. It's fine. Okay, let's get into it.
basically. The way this, the way we structured this week is, I know people are coming and going from class. That's why, the homework 0, which is out, it's not due until, like I mentioned my Tuesday. That's way. Too much time to get it done. But that way, if someone were to add, today.
they're not going to freak out. Okay. So you deal with gonna go out after class today again. It'll take you less than 5 min. But I give you 48 h to do it. Okay, it's gonna be like that every time. So get into a habit of just finding time to do it no big deal if you don't get all of the questions right. In fact, I would hope you wouldn't. Okay, normal. Okay.
let's get into it. First off any questions, concerns, comments, or freak out.
You need comments
per week. Yeah. Be freaking out at not bad.
Our slack only has 80 people in it. I think our class is like 80 89 people. So if you're not on our slack, get in there. How do you find our slack? You gotta go to canvas, and there's a link in there somewhere. Ask your friends
if you're not on canvas.
Big problems.
No, no questions. We're good.
Okay. You're the best.
We're opening a record store, as you remember. Here are the current names.
Let me tell you, people they're not great. We can do better.
We can do better. So here's a challenge. This is on slack. We're going to take votes votes over the weekend
votes over the weekend. and the way we are going to vote is just like by Emoji.
So go in there if you like. One of these names put a heart, a thumb a something. Okay?
And I'll try to figure out. So get more names in there. But this we're gonna live with this, the rest of the semester.
So what I mean is every class you want me to stand up here and say record in the record face, vote for it.
if you want me to say, have Renee Savignon every time vote for it. I don't recommend either of those. Maybe I you will vote for them. Now
get that done. I'll post something about this, but we'll close it, probably by Monday. Get your votes in. They'll stick with us the whole semester.
Okay, back to yeah. Question to do that. But I tried. I tried.
Okay. so let's get into it. So remember, our goal right now is at our record store, which is old school people come to our website. All we have is a search bar.
You don't have any recommendations. We have nothing to do with search bar you type in something.
Mateo Ruiz (23005367)
Okay. I guess you could mail an album. Let's think about that because you don't want to break right?
Gotta think about this.
Mateo Ruiz (23005367)
When we were getting Dvds from that
regular. It's fine
Fisher, Mateo. Call them after your friends call them out.
Sorry, guys. I had to mute you. I hope I hadn't muted myself, I think we're fine.
Okay, wow, that's crazy.
our goal help users search. Yeah? Question.
We should do this. We should do this. Okay.
what are we doing? And I roof this all out. So I don't want to lose anybody. Okay, number one, we make a big assumption. We have access to album data. artists, tracks, lyrics. Where did that come from? I don't know. Sometimes it's a problem. Okay.
in many real world scenarios. If you're actually going to build a search engine like on your own. If you want to build a competitor to Google, you have to go crawl the whole web.
Okay? Or you're gonna do something on social media. You have to go build
crawlers or spiders to go collect all of that.
This is the whole like area of research and lots of work and lots of tools just to get the data. Okay, if you're in a company. And you're like, Hey, we're gonna help build some search capabilities for some like internal product.
You're gonna just have to fight with different product teams to get their data
right, cause the way it works. And all these big companies. If you've been there, you're like, if I'm at Google right now, I'm like, Hey, Youtube, give me all of your user data like. Who are you? Bro. No, no.
So I have to work and negotiate and talk and find and make sure I don't screw up anything they're trying to do to get access. So this is a big, big problem. We've just assumed it away. Okay? And so if you're thinking about a project.
and we will do a kind of fun project.
Let me keep in mind which is like just getting the data for a project could be a project. So it's a big big problem. 2,
assuming we have the data. Okay.
we have to figure out what our users care about. What do they want to search for? Okay.
do they want to search for artists? They want to search for titles. They want to search for lyrics. We're simplifying it by just doing lyrics, because I have some data on lyrics we may use on the homework.
But this is a big decision point we have to make. So we're making simplifying assumptions as we go, we may revisit these later in the semester.
Now, we want to build our index right. which we're gonna have to tokenize the lyrics
as we're gonna talk about today, a lot of this today.
Once we have the index, we can go back to last class. And then we can actually build out our Boolean search capabilities. So, for example, we could build our large term document incidence matrix. And we said, Not smart. Why not smart?
Wi-fi, dumb, dumb.
sparse, it's sparse. And so what?
Yeah, we're showing a lot of zeros. It's really, really ginormous, in fact, so big we probably couldn't store it simply.
And we're storing a lot of zeros. That's the sparsity. Don't do that. So instead, we we move to the inverted index.
These were the ones.
And then, once we've got all this we can now do, Boolean, search over lyrics, and maybe we can go back and revisit some of these assumptions and actually help our users not just do Boolean queries maybe do more sophisticated queries. Oh, we'll talk about that today, and probably on Monday. Okay.
so that's kind of where we are in helping our users. So last time we left it with this inverted index. Okay.
so what do we have? We have songs, we have exciting songs with lyrics, life, any choose love or song. 2. Zebra, any love
he's like, you know these songs. If anyone can sing these songs
that would be great.
we'll ask Chad Gpt to write us a song. See Breathing love! Why, don't you notice? Sometimes I don't call the song, or I'll call the Docs or D's.
So these songs right here, and that's could be just. Or Doc, one dot 2, I'll kind of use that interchangeably. Okay, because later on we may talk about like a video, or a Tiktok, or a Tweet or X, or a what a Facebook post, or whatever. And oftentimes we'll just call them docs. That's just how we think about the world. So the point is, you'll notice any. Choose, love any shows up in dock one
there's any choose love. If you're looking at like Doc 2. There's Zebra any.
Oh, and I misbelieve
love.
believe on this belief, believe?
And so on and so forth. Okay. And so something we didn't talk about last time
is that this thing on the left we call this our dictionary.
Okay?
So we have our inverted index. And I'm not showing all the little pointers in between.
Okay, but imagine they're there.
And so I want to rewind for a second because a lot of times we're going to talk about stuff and just sort of assume away things that in practice you can't just assume away.
So, for example, if the query comes in and the query is. Let's do a real query. Here.
let's say the query is.
believe
and
love.
So before I said, Okay, believe in love. So we're we're here believe we're here at love. Walk along. We advance our pointers, and we say, D one and d 2 match no advanced d. 20, d. 2 is a match, so we return. D. 2. Then we advance. Oh, no, we're no matches, and we're done right.
But if we rewind, how did I figure out I was at believe and love like, how did I get pointers into these guys to even begin the traversal?
So I didn't. I just put my hands up right.
But in practice, what you have to think about is you've tokenized all of these documents and parks them all. You built this dictionary
so you can imagine you have a data structure. It could just be a big list.
right?
And now we have to imagine.
given this data structure, you can imagine. Like, you know, in one version there's sort of like a pointer into the top of it.
and I've got to get to believe and love. So, in fact, this dictionary requires its own data structure just at query time to find the terms that I care about.
So any suggestions on how we should find believe and love. Assuming I have a pointer here.
how do I get to believe?
Yeah, I'm here.
and right, and you have a half in love. And it wants to hear something like this. Right? Let's do it great. This is good.
So we actually need some data structure to support this. So, for example.
we could do some sort of hashing. What are the good things of hashing in this case?
It's like, bang! You're there. right, that's pretty cool.
We'll talk about a negative in just a second. What's another? What's the simplest way you could navigate this
list?
Just reverse it so we could say.
Are you believe? No. Are you believe Yup done found it? Okay, are you? Love? Note, are you love? Nope? Are you love? Note, are you? Yes.
so I can find them. I can do a linear scan
great, so we can just we can just do kind of like a list like list traversal.
What if we did, we could build a tree of some kind on top of this.
So I'm not gonna draw it. You can imagine you have some kind of search tree on top of it, and you say I'm looking for. Believe it's to the left branch. It's to the left branch to the right branch. Believe
right
and same thing with log. I'm using traversal right? So with the lookup in the tree version is going to be some log in. Let's say
the list. Traversal is going to be some order in the hashing one. We're right there.
but could you argue why, you might prefer a list, traversal or tree to Hashing.
Is there a world in which hashing seems like a problem?
There could be conflicts, and we have to deal with that. But assuming that away. Let's say we can take care of that. Yeah.
what if there's some kind of like locality or some kind of relationship amongst the ones? And so you could imagine like in this case, it's so simple it doesn't really show up. But if you have like.
you can imagine, like it is resorted like lexicographically all around believe might be believes, or believed, or believer, or belief.
And you can imagine a world where your query you might want to do some sort of local search around. That's hacking, believe is here. But like believed is somewhere far away. Right? Yeah.
we do like a tree where? Yeah? So these are all awesome points. So the
sort of the point here is something like
right now, we have no notion of like the importance or the popularity, or any of these other characteristics of the words or the queries.
And so our data structure is kind of like oblivious to the real world, right? And so if at our website, like everyone, is always searching for taylor, Swift albums.
right and Taylor swift lyrics, we might want to optimize the design of all of this for that. So we're not going to get into the details of it. But 100%. Yes, we can optimize all of these structures
to enable, like our popular queries. Another related point of that is not just in terms of these words, but also how these documents are arranged right now. They're just documents in the order in which, like, I enumerated them.
But you can imagine if we have millions of documents. Okay.
we might want to order it by the most popular first.
So the idea is, if you're looking for any, choose love whatever we find or sorry, believe and love. Imagine these lizards are quite wrong. It would be great if you know, if we had to enumerate every like all 1,000 albums that match.
But what if our user. Only just give you the top 5. So we could do it. We could basically do, believe in love, believe in love. And if that, too, is super popular. Go ahead and return it. It might do early termination.
So in other words, we don't have to get all 1,000. We get just the first 5. Give them to the user. And we're done.
These are good points. There's a lot of tricks you can do in here. We're kind of simplifying for our discussion.
Okay, cool. cool other questions.
Yeah, yeah.
Okay.
so today, what's in the index?
Right? We can only query like the stuff, like, what what are the tokens in that dictionary. We can only query for the stuff in there.
Okay? And then we'll hopefully get to. How can we improve the index? Can we do a better job and do more than just simple queries. Okay.
so tokenization.
I give you a document. You have to parse it and find the valid tokens. So I give you an input here. Welcome to class for 70 years.
Exclamation.
And I have to design a set of rules that are going to tokenize it. And so I just put 3 approaches here. There could be, you know.
kind of like infinitely many more. The first when I say.
basically just slid on white space. So welcome to class for 7 years. Exclamation stays here.
Let's see that that's a tokenization scheme. I cooked up
the second one here. What's the difference between this one and this?
I separate the punctuation, and I lowered welcome
right? So that was a decision split on white space lowercase. Everything split out punctuation.
So there's got 3 rules.
It's like this different set of tokens. Okay, how about what am I doing from the third one?
II I'm lowercasing everything.
I've removed white space, and I'm doing all like 3 characters in a row. So I have like a sliding window. So I say, wbl as a token. Elc. Lcov. E. And notice I don't do this another trick. I could have kind of like buffers hoping on either side, so I don't have like
Starstar W. Star WEWI. Sorry WEL.
It could affect things depending on your application, but that's like a different way to organize it could be totally reasonable, depending on what you're trying to do. Okay?
So the key is, we have to decide how we're going to tokenize the text that goes into the index. But reminder.
also need
the same rules
at query time.
And so what I mean is this. if we lower case everything. If a query comes in which is capital welcoming.
we'll go to our index and say it doesn't exist.
Remember, we're stupid. It's like it has to be lowercase.
So if our approach is lowercase, everything split out punctuation at query time. If the query is welcome.
class
capital W. If I lower case it, I'll be able to match this this document, whereas if I don't lowercase it.
I'm in big trouble. Okay.
so important. Why, it determines what our users can search for again if the tokens on the index doesn't exist. Okay? Secondly, oh, yeah. Yeah. Lls a little. Give you a little spice little flavor.
So Lms are trained over lots and lots of lots and lots of text. So they have special tokenizers on the front end that basically characterize what is it like? GPT. 4. What does the model understand? Okay? And so we'll talk about some of those techniques which are a little fancier later in the semester. It's super super critical. This is all like foundational. We gotta do tokenization. Okay, so
is challenge time.
David Hoffman. where's the house goes?
So what we're gonna do now is we're gonna take it a couple of minutes. And you're gonna talk amongst yourselves. Okay, not just yet. So here are the challenges. Okay?
Challenge number one. This is probably the hardest one. Challenge one. Introduce yourself to your neighbors.
I know. I'm sorry these guys dropped things. He just walks out immediately. Right, I'm out. I'm out.
Introduce yourself to your neighbors. 2
I want you to identify
some rules
for tokenizing the following, and on the next slide there'll be some text. Okay? So you have 2 jobs.
Talk to your neighbors, little pods, at least 2. It could be the whole row if you want to, and you can kind of quiet on the end. Okay.
introduce yourself to your neighbors. Then I'm going to give you some text, and I want you to identify rules. Okay, like, before we said the rules were lowercase split on white space
whatever. I want actual concrete rules on how you would tokenize. Does this make sense?
Okay? So I have a timer.
It starts right now. So begin the introduction. And here's your text. You may have to move a little bit too far.
That is better.
Hey? One more minute, people, one more minute
like computers.
Okay, gang. Let's come back. Let's come back.
Alright. Okay? So first, challenge,
oops. Okay. Yeah. First challenge. Okay, so
someone named someone that they met just now
probably hear his name. And you didn't know before.
Is that right. hey? It's hard. I know how this goes next to someone all semester, and you forget your name. That's new nature, like I'll introduce myself with someone, and it's like your name is already gone.
So we're gonna do this several times. So don't be embarrassed in next class. You're like
here
within the hour, we would play another trick of you to say, Hey, I'm James nice to meet you, or Hey, James! Here, what's up
or out of it. You have a whole game, too.
Okay, cool. So hopefully, you kind of met some people. Let's hear some rules. Here.
let's start with this. I see one rule from one team just to get us started. Simple rule, not sophisticated.
You're split on whites.
I love it so. Your current tokens are capital a.
21 dash, 2. Run in the first to put the
this is a token
open, for in 13 to 3 comma cool, so split on white space is a great start. So let's put that here
split on white space. Give me another one. We're gonna lowercase everything. Why would we want to lowercase.
Many consistent capital. Now imagine it's lowercase the is there a problem by making capital lowercase
so
of
well, what? What do you know about this capital, though.
sort of sent start a sentence that could be interesting.
Okay.
how about how about capital tigers? Why is that a problem differentiate between like animal tiger. And like the team, the tigers.
Okay, so this is all good. We're gonna have to make simplifying decisions. So lower casing
could be problematic.
The other thing is like, we're gonna build these rules. But we can also build like multiple passes. So you can imagine different versions of this different kind of analysis we're gonna do. But for now cool, give me another rule.
Like the premises we call us.
Okay. So I like this, you want to ignore all punctuation or ignore, just for ignore parens and and commas and
full stops. Okay. you guys like that.
Oh, because we 21.
Because, again, remember, we write this rule naively. If we split there, we would say, 21 is a token. One is a token. But maybe you want to be 21, so it could be
okay.
ignore periods
unless
followed by a number or whatever.
or non-whitespace.
Okay, so kind of like a sub rule off this rule, cool. Give me another parsing rule. We like.
okay, I like this, okay, ignore dot dot dot, something something like this. Okay, I like it. Give me another one.
Yes.
yeah, they're like, well, we don't split on Anderson's.
These are super important is going to be together
always. Maybe. Okay. pulling back for a second.
Right now, we're talking at like tokenization for very like sort of smallest piece level. Can you? Can you identify any sort of pieces here? That sort of semantically have like there are multiple tokens. But they have like a bigger meaning.
Texas, A and M
is like a concept. Right? Currently, we're parsing that as Texas A and M.
And possibly, if you ignore all punctuation, Texas, A. M. Okay. So keep that in mind, like.
how would we know Texas? A. And M is a thing. an entity.
legal and discussions.
So we could do a rule where we preserve capitalization if it's not the first of the sentence, and then try to make it like a longer block of things.
So right now you can come examples and counter example. This is this is part of the tricky part of this, and in practice what you'll end up doing like on a homework. And then a project is, you'll try some of these rules, and you'll look at what falls out, you'll go. Oh.
crap! Yeah, let me fix that rule. Oh.
graphs! Any other kind of larger concepts you see here.
aside from Texas A. And M, or you're so blinded by like you're focusing on Texas people. That's the one
I mean. You mentioned, like, like the tigers is kind of a thing
is like kind of one of 10. I don't know about you. 3 point attempts is like a conceptual unit.
Also notice the numbers mean different things. These are. These are points.
Those are like wins and losses.
And that's a, you know, that's a percent.
See lots of different kinds of numbers. But they're all kind of maybe just turning into numbers, you know, like, they're all sort of being flattened out. Okay, this is really, really good. I like it
any other last rules anyone wants to throw at me. They're really proud of
yeah numbers. After a teammate signed by the office numbers, you can have a rule, which is, if I know something about after it means wins and losses, or if it has, you can do some regular expression, something like this.
So. Yes, so right now we have kind of like lots of rules, but I'd say plus, you know, it could be some Reg. X, plus some kind of you call it like a domain
knowledge
that you could use to help. Kind of guide this. Now take all of these rules. Okay, and pack them in your brain. Okay?
And then we're gonna do another challenge. Okay. this one will do a little shorter.
You don't have to introduce yourself because you already know each other.
Now, we're not indexing news articles about sports. We're going to be indexing. We're going to build a code search engine. Okay? So now, when you think about what are the right rules?
What do we want our users to be able to search for when we had to tokenize this?
Okay?
So I'm gonna give you guys 2 min to talk amongst yourselves and come back with one of the rules for building a search engine over this code window.
Okay, your time starts.
Now.
alright, that's about 90 s.
Okay, so let's go back and apply our previous parser to this text.
So we split on white space.
We lower case. We ignore
positive punctuation. we index everything else, something, something, something. What are the new rules you have?
Yes, the name of like an array or a list.
So if I'm here.
So if I know like this, open
this, have any other meeting here
and the whole language.
Yeah. So we have some special like. So if I see something like open.
then some rule which is something like, ignore.
ignore the commas
and and split something something. Yeah.
Another example. Another idea.
All that time you got nothing. Yeah. You probably wanna see
the initial like 4 statements
pretty much. In fact, I'm not sure the first part would matter. But like, are you pasting this entire day into a
so this is the good point. So it's like what we're trying to say is right. Now. we're making decisions on what gets tokenized. It's built into our index right now. We have a very simple Boolean search.
So if our you, this is a good question. What do our users want to do
if they come to our code search engine? Are they gonna type in 4 x. in which case I want to find them. This document that has a 4 x. or do they want to search for just for loops
which could be, you might say, for loops which isn't here. But we have to have some kind of mapping.
or are they looking for fruits? The particular
variable name? Are they searching for this banana string? Are they? Are we looking for anytime? We have any quality?
Are we looking for any time? You have a friend's staff?
That's the point. II don't know. This is kind of a hokey, because none of us know right.
But what I'm trying to get you to think about is
these decisions you made that seem silly
are gonna greatly influence the whole downstream pipeline.
Because, for example, we know, like, continue with like a reserve word right? It's a special order that's different from, and print is a function.
This is a
an argument to a function. But we know they have meaning.
But right now we're tokenizing, and we're kind of flattening all that knowledge away.
And so II don't need to come out of this thinking like I have cracked the right way to parse this. I don't know.
I just want you to come away thinking, yeesh.
yeah, I gotta be careful. I got to be very careful. And, in fact, the design of this may need to go beyond simple rules to more sophisticated facts.
We may have to have a very sophisticated, like multiple passes to to really understand
what's going on. Okay, that's the big takeaway. This is non-trivial, very challenging.
We're gonna spend, you know, basically today on it, and then never again. But the point is, when you get on the homework, when you do your project, when you're out in the real world when you're on your internship.
and you don't think about your token as like tokenization.
And then, at the end of it, your boss, your users, whoever is like this sucks right?
And you're like, but my my machine learning is so awesome.
My! Whatever my deep learning is so sick.
And then you actually go look at the index and you go.
Oh.
oh, oh, crap! Like I didn't do what I thought I was doing. Okay, so kind of the lesson here is.
you got to check this stuff. Okay.
cool other issues that we'll face in tokenization. And I looked, listed a bunch of these. You do the readings and give lots of examples. I was gonna try to tell you this to tell you why this stuff is so hard.
And you know, like earlier, when I was asking you guys about name structures. Everyone's like, now we're talking tab. I know my name instructions.
This is like, no, this is like the real, like, grotesque, like human output. That's so hard to deal with. Okay, so, for example, language.
you may be indexing content that has been written in multiple languages. sometimes in the same document.
So now I have to have, like an English Pocono Spanish language, tokenizer. the German organizer, and I have to also know, like within a dozen they may be switching between.
There's an example in the readings like like Mit
is like a university, but Mit is like the German like with Mit.
you gotta know what's going on right? That's tricky. So this is the whole point. Here is like.
kind of, you know, hair on fire. Okay.
hyphens. You know, we've seen some examples. But this is like,
typically, you would say, Oh, hyphens don't matter. We just saw an example. We said, you know they were 13 3. Well, they're the hyphen sort of indicating wins and losses.
But sometimes you have. You know, someone might write they don't really write this. They may write, you know, database or
you know, pick me up, and whatever and the idea is like is that one long
string that gets indexed.
or do I chunk the hyphens?
Or if you're doing flights, people will say, you know I'm I'm flying. Houston.
to like Los Angeles.
where I'm doing
San Francisco
like Los Angeles. So there's lots of ways pythons can show up. It's just different kinds of punctuation. So you're like.
if you keep it, Francisco. Loss is a token. If you drop it, you lose that connection. Sometimes it's met. It matters. This is tricky. Numbers.
Numbers have lots of semantics. So if I show you this number.
you know what that is.
Okay, then what if I show you this number?
What's that?
Yeah, let me tell you what. It's not an IP. Because you know. but that's too big. I think it's 2, 56 is the highest it could be. But if it was like 2, 21,
it could be some IP, it could be something. It was probably not a phone number. But you'd have domain knowledge to know that.
Okay, so again, is it a street address? Is it a phone number? Is it an IP address? What is it? Is it a score? Is it a record?
Okay, white space we talked about? In many cases white space is non-informative.
How about on our python example?
White space, very important, defines the indentation
defines the nested code blocks which defines the program and how it runs. So white space can be important. You can't throw it away.
This is just to sort of make you kind of pull your hair out. That's all
other examples.
So normalization this is, do you know that, like u dot S. Dot a equals U.S.A.
So you 2 try to normalize these down to the same thing.
You also have, you know.
is windows equal to windows.
Why not. and there's a oh, man!
Windows could be like like Microsoft windows, or it could be the window that I want to jump out of right now, because this is driving me crazy. Okay.
so
pace folding. We've talked about like lowercasing everything. Oftentimes it can make sense in the windows case it doesn't always make sense.
But when you lower case you may lose something significant. Stop words, you guys know. Stop words heard that before.
So in English, these are like, usually, you know, words we can
safely remove.
And so in the early search engines back, when
both Disney memory were in short, supply.
the idea was,
Every time I see the just get rid of it. and we'll talk about this later. But there's what's called a Zitane distribution of words. and so if I went through, like
all in your text messages, and I count all the words.
and that looked at the distribution. There be a few words you use a ton
examples. I don't know how you guys. Fact. So what were some words you would say a lot.
You don't know if you should say it.
in the event.
So let's do this. Suppose you index.
all over the media.
Okay? And we kind of all the words, the the most frequent occurring words on Wikipedia. What do you think the top 5 most frequent words on Wikipedia would be?
And it turns out those tenant not be very informative words so historically, you know the of a an dot dot dot. Those are words that didn't really convey much interesting information.
And so in the olden days. You can just wipe them all out, and it saves you a ton of space, and we'll quantify that later on, probably next week.
Okay, that's the trick. The problem is, if you get rid of stop words.
And can you give me a query where, if you've lost these stop words, you're in big trouble.
The the band there is a band called The.
There's a more popular band called
The Who they're Gone.
They're gone. So the who is gone
said.
Okay. you can't read this one. This last one is stimming.
or sometimes you'll see we'll talk about limitization in the readings.
The main thing here is we're not doing anything with, since we're talking about English, all kind of the different versions of a word.
And so so limitation would be like
to be. How do you conjugate to be like I am?
You are. He is. That's like all that am are is like they're all different, like first, second, or third, or singular or plural versions of to be.
And because we know that if someone says like, I am walking.
and then someone else says you are walking where it says I am walking. You say cab is walking.
or I am, or you are, that's what I do. I am walking. You are walking, am. And R. Don't match.
even though they're both
versions of to be
so. You might want to know that to capture that
oftentimes we do what's called stimming, which is very crude, and we do things like this. We have rules like, if a word ends at SSES.
Drop it and turn it into Sfs
example Corretha's caress possesses possess.
If a word is in Ies turn into an I
ponies becomes pony with an hive.
why is that helpful?
Well, you have the rule that, like y turns into. I so like Pony Pony can match.
And this way caresses also knows we just sf, you don't pay off.
This is not a good example, but like possesses, you know. You don't keep dropping until you have nothing but in this case you drop off. The s's so cats becomes cat.
Now, I'm not saying this is the right way to do it. But there is a thing called Porter Stemmer. A lot of people have used it.
The reason we would want to do this if if a user comes to our to our search engine and searches for like meet me at midnight.
Okay? And then someone else types in meet me midnights.
We may want to treat them the same. I know how Cs people work. You're like, oh, no, I have a case where it doesn't work.
We know that
we know this is not perfect. This is like a trick you might try to do. So. Your users are happy, doesn't always work.
Okay. questions about any of this tokenization business.
Okay? So yeah, we only spend today on it. Never again. It's sort of a live cause on the homework we get to play.
and the main thing is just that you can suffer like generations have suffered before you. Okay, that's but in a fun way. I mean, it's more like fun. You're like, Oh.
like we saw it today. I was just talking to someone about the on the homework 0. It's parse, the web page, right? Some Aggie traditions, web page.
And if you just parse it. You get all this HTML back. and you may decide. No one cares about that, so you might want to kind of stop word out all that
as a possibility. Okay.
The main thing is is life. When you get into like, how people really like generate like user-generated content. it's
totally noisy and totally will make you pull your hair out. Very, very challenging. Okay. so last few minutes.
how do we know
we've we've held rules? We've done these tricks. How do we know? Whatever we've decided
is a good idea. I don't know that
you make the same real. So we have our
record email record face or whatever name we give our records.
We're live. We have people coming there. How do we know we're doing a good job? Yeah.
okay, maybe before we even expose it to our real users. Maybe we empanel like our employees or some friends and have them use it, and we gauge their happiness and success
sound, sound good. That's going to go to go to the users. I like that.
Go to our users. you know. Run tests.
evaluate somehow. Okay. suppose our users seem to be happy. And now we've gone. Live. Now, what do we do?
What would you measure? Yeah.
collect information about, I guess. Oh, yeah, so this is sort of related to that. So we can go to our users. We can also, you know, we can sort of construct
our own kind of test cases.
right? So we may know enough about our albums. Okay, I know this query, and I know what I ought to see. do I see what I think I ought to see? And if I don't, we have a problem, and we may want to fix this
okay, cool. You got one, and you're stretching back there. You got a you got a comment to make, just stretching, showing off your guns. I like it doing great.
yeah, in practice, right? You want to keep this kind of you don't want your users to see junk
right? So we wanna hold it back as much as possible. So again, run tests on our friends and family, others, engineers, people who you think are gonna use this thing prior test cases. Later on.
Later on we will talk about. We will do proper. The evaluation.
Okay, that's coming now.
So so far.
our our index supports Boolean theories.
So as a teaser to come back on Monday and not to drop the course today.
Can we improve the index? Can we support phrase queries? For example.
Taylor Swift exactly matches or or 70 homework solutions.
exactly matches.
Can we support proximity queries? These words are near those words, Wildcard queries
they. or spelling errors.
see you all on Monday have a great weekend.
We got a different.

Okay. Howdy? I hope everyone had a great weekend nice to see you all. You're back to court.
Thanks for coming to class. Hopefully, the recording on Friday made it up. I think it is so every yeah. So every class is gonna be recorded moving forward. Something to kind of say, cause I noticed there are some people on the zoom right now. So like technically, that's not how this is supposed to go.
So right now, I think we maybe have, like 50, some odd people here. This is a class of 91 So if it becomes like, people are just gonna like hang out in their
dorms or apartments and then zoom in. I'm gonna have to turn on the waiting room
and not let anybody into the waiting room, because the idea this is supposed to be like a recording as like a nice to have if you missed class, but not as a
alternative. Think that's
so. You know, the regents are real interested in, like what we say in class these days. So I think we're not supposed to do that. So anyway, we'll see.
Okay, let's get started. That's our search. Start on a down, though. What? The heck man? Okay. So we're back at it. Couple of announcements quick. Quiz. One went out on Friday. I think most people did it, I think, do it this week. It's okay to miss one. It's okay, even to miss like a couple. But you start regularly missing them. Remember engagement 10. You miss a bunch of quick quizzes.
Bad news. Was it hard?
Did you take it? Take you 10 min an hour. It probably took you 3 min or faster. Okay, that's the idea. Quick, quick, quick! If you don't know the question. You don't know the answer.
Now you just tell yourself, hey? I better go check on that. This. The first one was kind of like some syllabus questions. So not a big deal, but moving forward, I'll ask you real questions or kind of previews of what you might see on a midterm or a final.
so you know, not gonna be hard to work them out. But like you're like, Oh, I asked you a question. You don't know what's up. Note to self
homework. 0 due tomorrow by 1159. All our homeworks have late days. Not this one. Okay, this one. Just get it in. Remember, you don't even have to do anything.
Just name it the way we want you to name it. Submit it, and you'll get credit. Remember, this is a 1% of your final grade.
So I've had people in the past. They're like, I'll eat that one. Let me tell you what they will come back and watch. Right? It's during the final. And you're like, Oh, crap! That 1% would be nice right in my pocket.
Any other questions concerns, comments about the class.
What's up anything? This is your chance. We'll do this every week. Every class.
everyone is fast. I contact
is that back road?
Yeah, I love it. I love it. Okay, so we're good. Let's go. Let's go. Keep cooking. Okay? So
we're opening a record store I plead with the class last time, hey? Go into slack and vote for your favorite name and like nobody did. Okay, super lame. Someone put a big, sad face here. Let's fix that whoops.
Let's fix that we need. I need you to go on slack and vote, because right now we have like 8 or 9 names. Some are okay.
Some are pretty good. Some are. So please vote on slack.
And the way you vote just Emoji heart thumbs up. Whatever get it going. We need a name, otherwise we'll just be an unnamed record store. No fun.
Now, what are we doing, folks? Remember our goal is to help right now. Help our users search our store right? That's all we have as a front interface. You type in the search. In a few weeks we're gonna come back and read the recommendation side. You'd like, you know, you bought this album here. Other albums.
hey? It's the weekend. Because of that context. We recommend these albums, etc., etc. Right now, all we're doing is search and all we can do right now is basic only in search ands. Or is it not over keywords of stuff that's in the lyrics that we've managed to index and shove into our inverted index. And we can do our Boolean queries right? So we've got our ands. We can do our ors and our not. And remember only
you know, words in the index
as then, depending on how we Parse, and we talked about that last week. If we do a poor job of parsing, the things that our users care about don't make it into the index. We can't find it for them. It's just like
so again, if we did stop words and we stop worded out Duh, and who?
There's no the who albums ever. It's just invisible.
Okay, if you're indexing Shakespeare and someone is looking for to be or not to be.
that's all. Stop words. If you were so inclined. Okay, you build a code search engine.
and you treat all punctuation as like junk and you throw it away.
That could be a problem. I can't figure out what's going on. Okay, keep this in. Mind.
Now.
what we're going to talk about today is how we can make our index better how we can support our users to do more interesting stuff, for example, to support phrase, queries.
proximity queries this near that wild card queries.
or even to tolerate spelling errors, because we all do it all the time, and we don't even think about it anymore. Right? We just like the thing. The machine will figure it out right. But you have to build some infrastructure to help our users
and to tolerate their spelling errors. Right?
So let's start off with our favorite phrase, queries. So the phrase query here is like in quotes, typically Taylor Swift, meaning, I want exactly tailored. Next to Swift.
I don't want a document that says I was
talking to Taylor, and later on I took a swift race around the block. I want Taylor next to Swift. Okay, or meet me at midnight. I don't want those words, but I want those words in that particular sequence. So first question, why might we even care? Why would we want to support
phrase query, why is this an interesting thing to do?
But let's really buy it. By the way, because I'm right right here.
So I think when you're ever
Donna Fraser. Historically, you looked at search had been wrong. There were lots of grace periods.
It could be more slow, tolerant. You take whatever. And to me anyone never done it. No one's ever done that.
So is really
this was the
if you're trying to find like an exact match product name. I don't want something approximate. I want exactly this
good answer. Some other hands. I saw you earlier
exactly what I want. Right?
So
so how are we going to do it? So one idea is, we already know how to build like a unogram or a single term index.
What if we made it a byword index. So we're not going to index every single word. We'll index every pair of words. Okay, so here's our lyrics in midnight, staring@theceiling.dot.
So before, when we indexed this, you know, we would have like
who would say, You know, meet.
that's in Doc. You know we have dock one somewhere. That would be me
and duck wand instead. What we can do now
is, we'll say, Aha! So meet me
me at at
midnight.
and depending on how we do our parsing, I might consider the comma. Maybe it goes away, maybe becomes midnight.
staring.
staring at.
And so the idea is. My index is no longer just single words, but it's like pairs of words right? And so it could be. Meet me as in Doc. One. Maybe it's in Doc, you know, 100,
and maybe it's in Doc 500, and so on and so forth. Okay?
And so now we have a byword index. and indeed we can do a bite. We can do a try word index.
And in fact, we could do we could put them all together right. We could have
a single word by word and try word. And so our dictionary just becomes all the kind of single words, all the pairs of words we've ever seen all the threes of words we've ever seen. And so if I want to go back and do
the query, is Taylor Swift? Can I do it in my Bible word, space. Yeah. So somewhere in there is exactly Taylor Swift, and we know they came in that order.
like. In other words, it wasn't Swift, Taylor
Taylor Swift actually occurs because it's in our index. So we solve it right? Yes, but doesn't have exponential space of webinar. Well, there's an issue here, which is certainly we're gonna keep blowing up how much space we need, because now we're dealing with every pair of words, and then threes of words. And so certainly the dictionary. Remember the dictionary. Is this data structure here? The whole thing we usually call our inverted index. But the dictionary is like all the words or tokens we know about.
So right now, yeah, it's all single words. And so if they're in of those you could imagine, there could be in squared pairs of words.
Okay, you need to do the threes now, in practice, not every possible word will be paired up, but you're on the right track, which is suddenly, we're now making this thing much bigger. Okay.
so that's an issue.
Another issue is, does it do what we want it to do? Does this support phrase queries.
it supports. Okay, let's do So if the query is. meet me
at midnight. here's my query.
how do I run this on my byword index.
Okay? So we're gonna we're gonna translate that into meet me. And what
at the end.
Okay, let's do this version. And at
midnight. Yes.
what? Okay? So let's freeze it right now. So we can date. They basically take a forward phrase, break it into 2. And in them together.
Will this find all of the free, all the documents that have the phrase meet me at midnight? It will find all of them that haven't seen midnight. But other documents, for example, the documents, hey? Meet me later, when I don't know. Admit
with your own.
So it has the words meet me, and it has midnight, but it doesn't have them meet me at midnight.
Okay. so, and you could. So then you say, well, we could do meet me and
meeting at.
And
will that work?
It could also find false positives. Because here's this other version, right? We're saying, well, we could do. Meet me
and
me at and
at
midnight. Better, it seems better.
but we still have the case where it could be a document which is like, Hey, meet me later on.
me! At home.
whatever at midnight.
So you can cook up sentences that have all of those byword that don't have them in sequence. So the problem with this, you know, one is the space
one is the you know
can generate. We call false positives.
you know. It will find all of the actual documents that have the phrase, but it may generate other find other documents that don't have the phrase
meaning, if I do, my phrase, query, Taylor Swift, or meet me at midnight. Basically, we're going to give you back something. And then we're gonna say, the user, you have to go figure it out.
Does it really have what we thought it had. So there's a little extra work. Okay. alternatively right? Even if we have false positives at query time, we could then do a linear scan of the document.
So think about it. We run the query.
we look at the index. We say, here are the matching documents. Here's a hundred matching documents.
Some of them might actually have meet me at midnight. Exactly. Some of them may not. So at query time. We could then take each document
and then do a linear scan of it and see if the phrases in there. Okay?
And again, we can do that.
But again, that's like a lot of time we're spending at a query. Time doesn't make a lot of sense, doesn't really do what we want to do. So like you say, well, what about? You know? Not just by word.
Well, this is a 4 phrase, so let's make it, you know. 3 word. But let's make it a forward index.
So we solve the problem right? Meet me at midnight is 4 words. So we can index every 4 words. So we solve the problem.
But what about 5 words? But we'll make a five-word index.
Is there always just the one more? Yeah.
it's very like Csi things like the plus one more thing.
So now we have a 6 word index.
There's always phrases that are a little bit longer. Okay? And so in practice.
there's always trade-offs. Right?
So we could decide. Most of our users only ever ask for like empirically we may discover, they only ask for 3 or 4 word phrases, so we might choose to do a 3 or 4 word phrase index, and if it's anything longer we just don't support it.
That's a trade off. We can do that right cool. Now.
instead of doing this kind of increasingly larger like units, a different idea is to build the position directly into the index with something called a positional index.
Now, compared to what we did before, where we just said. Okay, dot one. Okay, we took for a term. We said, it's in dot one and dot 2. Now we say for that term it's in Doc. One at this position within that document.
and it's in Doc. 2 at this position in that.so notice, we're storing more information.
So back to the space issue, nothing for free. Okay? So now we're storing. Not just that. The term is in a document. But exactly where in the document it is.
Okay. Let's look at an example of this. We have 2 documents now meeting at midnight, staring at the ceiling. Another one. I have this thing where I get older dot-dot-dot. We have these 2 documents.
and we'd like to put them into our positional index, all of the terms. So remember what we're gonna do is for every term we're gonna say, well, how many this is like a little bookkeeping we can use for later. How many dots contain the term? Just a little prefix. And then for every document, what exact position? Okay?
So if we're going to build a positional index for these 2 documents, so I'll put them there at the bottom.
So. for example, we have the word meet.
And
right now we can put like a little like a like a prefix, like how many documents is meet in. So in this one, it's only in the first one.
So I'm just gonna put like a little like one there. Just so like, I know, it's the one document.
And then what do we say? We said, okay for that document. What position is it in? So I'm going to say, it's in Doc one.
And it's a position. What? But depends on how we number it, maybe position 0,
because we can say, like, that's 0 1, 2, 3, dot dot 0 1, 2, 3 dot. Dot.
Okay?
Did I do an example where there's no words in common.
Well, we're gonna pass. No, no, no. So we're gonna say midnight.
And I'm assuming we've parsed it down. We've maybe chopped the Esses, and we've lowercased it.
So here we're gonna say, well, midnight is in 2 documents. And so in Doc one, it's at position 0 1, 2, 3,
and at Doc 2, it's in position. 0, 1, 2, 3, 4, 5, 6, 7,
12.
Okay.
And you can imagine. So you're clear light. If
I did terrible example. Because you don't have any repeats you can imagine. Like if meet showed up again here, I would say, need is it dot one position, 0 in dot one position 20 or whatever.
Okay, the point is now recording the document and the specific spot where it occurs.
So this is a positional index. We have more information in there.
But it doesn't exactly do a phrase query exactly right. We're gonna we're gonna do a little more work to do a phrase query.
okay.
see it.
We gotta do a little more work. We gotta check all those positions. So
if the query was something like, midnight's become okay. So I got to go into my index. Find every instance of midnight's.
So back here, we said, Okay, midnight is in Doc. One
at position 3. And Doc 2 at position 12
oops.
That's midnight's. How about become
whoops
is becoming here? It's in the document. 2.
A
I erased that. Sorry that's not duck to what was the specific 12. I erased it.
and this one's at what 13.
So there was position 12 and position 13. So we say, I've become.
That's midnight's become. Is that doc to position 13.
So I just have to check. Okay. is there a
one right after the other one check.
Doc, 2 as a hit.
Right? So again, I mean looking at my additional index, keep counting of the words and the docs in which they occur in. And then, due to this very fine-grained, positional sort of kind of logic.
Okay. how about meet Bob? And actually, this is not gonna work. This is nonsense.
but again we would say, You know, meet me at midnight, so we find meet. We find ads and midnight. They all work, but when we looked up Bob.
Presumably there's not a document that has, Bob. And you know that exact position. So it would return nothing. Okay.
okay. that's a positional index.
We eat more in terms of space. We gotta keep more accounting.
It's a little harder at query time. We have to kind of keep up with all this stuff. But is this guaranteed to give me exactly
any length phrase that I'm looking for?
Yeah. Well, what's this
notice and subtleties here which are.
This is where it gets a little tricky notice we have like a comma.
You can imagine period you can imagine, quotes. This is where the whole like tokenization, pipeline and our query support need to be the
good friends.
Because, again, if it could be such that we want exactly
these words in, you know, next to each other, and it maybe we tokenized that there were quotes in between. And so they're not really next to each other, the way human would read it somehow the quotes in between. But when we tokenized it and through the quotes away.
so it looks like a phrase, it really wasn't a phrase.
So I'm getting at. There is like, yeah on the surface. It seems like awesome. You've got to be careful and some of the decisions you make up up upstream in your pipeline, you may end up giving back documents that don't actually have what you and I would consider the phrase
based on the tokenization cool.
Okay.
you know what's cool about positional index. is it gives you
something for free gives you proximity, query for free.
So again. This is more on the old days.
We don't do it as much. But it's like, basically, you know, like, I'm looking for a word.
something like near
another word. or it's like I'm looking for word one.
you know, within. Let's say 5 words
of word 2. So proximity query is.
you know.
I don't know the exact phrase I'm looking for. So go back to kind of our like. One of our economical phrase, theories is about like 4 70 homeworks.
Okay. 4, 70, 4, 3, 3, 3 filings. Now.
I didn't. Google that sorry. So we wanted that, you know the exact phrase to look for, because you don't exactly how I wrote it, but you're like I bet it has been there. I've been s. 4 70, and I bet as final in there
you don't know the exact phrase to use. You could say something like, I know I'm looking for Caverly. and we can define it like near 4 70, and maybe 4, 70 near
final.
And how do we define? Near? We could give a like a parameter. K. That says within so many words. or you know it could be some system inferred parameter. Okay. But if you go back to our positional index, right? So go back to here. And he said, well, I'm looking for. You know
meat
that is within so many words of staring.
Well, I know meat is exactly at position 0, and a staring is at position 0 1, 2, 3, 4,
and so you can just do a simple arithmetic and say like, Well, me, is it 0? So I can find sharing within like 1, 2, 3, 4, 5.
It's a hit. Okay. so it doesn't have to be right next to each other, plus or minus one. It could be within some plus or minus. K, okay.
so that's what's cool about additional index. We can do phrase query. We can also do proximity query out the box
questions concerns freak out. This is easy. It's kind of cool.
Yes, so digital index. Right now, you can use it without positions.
So you can just say I'm looking for midnight. Oh, midnight's and dark one, Doc. 2, just like our old midnight.
But with midnight now we're saying it's like, Oh, I know it's a dot one at position 3, and I know midnight is a dot, 2 in position. 12. So I have that extra information.
And so if I want to use that information. I can use it so in a brace query we would want to save, like, you know. if it's meet me at midnight. This is 12 midnight.
I better find at position 11, or it's not like this document does not have that phrase. That's the whole thing. So we're adding a little more logic on the query side and doing this resolution. And so right now we're saying like, those words need to be next to each other according to the exact position. Not in the document.
Cool.
Okay.
I think it's kind of neat in your back pocket. We got Boolean query. We can now do
phrase query. We can now do proximity? Query.
how about wildcard query?
I want to do a star Midstar. Find every document that has the word somewhere in it starting mit.
so it could be. The document has midnight midnight, midnight Midas, other midwords, mid-world. middle
middling
to say, mind numbing, but that doesn't work middling
bit one
like what? Midland Midland.
It's a place anyone hear from that
mid midair midair. I'm I do crosswords every day. I do like little word puzzles. So I like this kind of stuff everywhere that starts with Mit.
how do we do this?
Build a new indexing structure that allows us to do this, or can we do it in our existing index?
Well, yeah, we could, if we sorted so suppose our dictionary was sorted. So suppose we have a dictionary.
Remember, it looks like this, right? This is our dictionary, and it's like A and and somewhere down there it's like Midas. whatever midnight all these mit's, and then it's like zebra at the bottom.
So if I could come down here and find like the first mit
I could just do like a linear scan and get all of the mit words.
yeah.
So I think we can do. I think we can do this. Anybody in the middle, too.
And I put it in the middle, too. So instead of doing a linear scan, we said, Well, we could just do Hashing
directly to it. And this is a case where maybe Hashing doesn't work
right because we doesn't maintain the locality.
I can't. I can't hash mid something and find the other mid somethings. Instead, we said, before we could build some kind of data structure on top of this, like some kind of search tree. So indeed.
let's do that right. So we have our dictionary.
You guys understand? When I draw this, it's like where we go about order sorted. So a to zebra.
Those are all the documents. Right? That's just like
those are the docs in which it occurs. And now we have some kind of you know.
we have some kind of binary search tree on top of that. And so if I say, okay, to find anything that starts MID. When you find all the words between mit to whoever becomes MIE.
So we have some search tree on top of it, which you can imagine. you know. Somehow we end up here. and then we can start scanning down
and find all of the mit words and then then become the mide words.
okay, so we can do wildcard queries easy. We just did it.
Does everyone get this any concerns or questions or worries about this?
Yeah.
it could be the first letter.
So the it depends on how we want to design this. A lot of design choices you have to make right now. We can assume it could be based on, like, you know, half the you know.
the first queries are gone
right now. We're just trying to support wildcard queries. So if we want to do phrase queries, we build a positional index. Done. Okay? Now, we're working on wildcard queers. Yes, we could unify them.
Okay, we can
right now to treat them in isolation. We can worry later about. Does it still work? I think it does. But keep it in mind right now. Wildcard period. we just have some kind of indexing structure on the top of it over the dictionary.
So we would say if the very first decision might be, you know, like I don't know, the split word is, let's say it's like noon.
So basically, it's like less than noon
whoops
greater than equal to noon. So you can imagine noon is somewhere here that's like noon splits it in half.
So it's like, Oh, I'm looking for mid something that's less than noon.
You know Google, that in my search tree, and then, whatever the next split is. Oh, it's a word lexicographically between a and Ni don't know
house
less than house. Oh, mid something's greater than house. So we just traverse this tree until eventually we get down there. We do that build linear scan to find the very first mit word. And we just are reading them off in turn.
What were our words? Midnight, midnight, Sir Midas. Midnight. Midnight's mid noon.
At some point we run out of Mit's. Let me hit MIE. What's miu? Main is a word.
It is.
Yeah. Oh.
not gonna repeat that genius. You got it right? So the point is. we can do this. So we just did wildcard queries. We can end the class early.
Who said, Yeah, who said, Yeah, it'd be cool.
He's get out of here. Get out of here, please stay, please.
I'm gonna convince you today
I love couldn't.
What if the wildcard is on the other side?
It's not
midnight, but it's like ends. And if T. So I want to match swift lift rift
words that end in. Ift don't start with. Ift.
how do we do that. but that's simmering your love in your brain for a second. Don't do that. We don't.
This is like you go for your programming interview and ask me this question, yeah, I wanna do that. We just go laptop. Yet. You ever done that with power, boot, or
or why would anyone ever want to do that. You may have this dumb question
close laptop disclosing
power move
you ever do this? I know you've got you guys. We're in the middle of the zoom or talking to someone is boring. And you just need it real quick. And then later on, they text you or email you, you're like, Oh, yeah, sorry. I guess. My wife, you know that trick.
That's a power move right there. You don't explain it to them. You just so. Anyway, I was thinking another.
it's just gone. I don't do this. We don't do it. We quit. We leave the class.
What's our trick? Yeah, Nixon. I don't think you're like the open, the back.
do it reverse? Yeah, reverse. It's like the same thing, but it reverse.
Oh. how how do you do that? As a person. I like to say it, but
but we'll keep another binary tree. What do we?
We turned into Tfi Star. but that will find words that start with tfi.
but I like it. We we know how to do mit Star. How do we do? How do we make it? Tfi Star?
So
we take all the words and we reverse them.
So you know, zebra becomes Harvez. Follow that.
Okay.
taco cat becomes what
midnight becomes
swift becomes
TFIW. F. Oh, we just reverse. Yeah, okay? Idea. Yeah.
reverse. You know all words in the dictionary.
the kind of
done I mean, just
basically take the reverse the the wild card?
Query.
so what was the we said? It was star ift
star. If t make that TFI star.
So now in my dictionary
again, I'm gonna draw like this somewhere in there is
TFI. Swift. Where is he?
TFI lyft.
or my other words, rift. it's not an order. Tfi!
What'd I say? Rift somewhere in there are all the Tfi words
they all end in. If T. Meaning will reverse them. They start with. Tfi, so yeah, you said, have a binary search tree. Yeah, same deal. You have a big search tree on top. And I say, okay, I'm running the query, Tfi star
on my reverse dictionary. Reverse all words in dictionary paint my biography, flip it TFI. Star.
Find somewhere in the gray versus Cfi were read down. I did it.
That's pretty sick. pretty cool.
Now
everyone got this.
This is not just for this class. This is also for those coding interviews.
also another trick. The Co. The interviewer asked him something. And you think it's dumb? You say that's a nice question.
What if we were doing wildcard theories? And I go? What do you mean? And you, you know, like something. Star. How would you do that? And you get them on the back foot.
Also power move. And when they say, that's not how interviews work. Stop this, you click in the
okay. Yeah. Given that.
Well, Elkhart, query's easy. Now. Are you ready for your head to truly explode? What about wildcards at any point of a word?
You can't do it.
So just to be clear. TA. Star r rib Taylor, TA. Ray, Taylor.
the end star. I think nights
Mike
might depends on if we allow null like a nothing. A good point, I'm not sure
is a star, a null or not. We decide we build the thing.
Now. We set the computer up finally. I can't do it. Impossible
ideas! We know how to do. Forward, you know how to do backwards. Combine them. Let's do that. Let's combine them.
So let's do.
We could do forward and backwards and do the intersection. So let's do that.
So let's do a forward. So let's make it so
so we could look up TA. Star and our star on the reverse list.
or we can look up in Star, and then th GI
on the reverse list.
and then we intersect.
That's a solution. Will it work?
I think. So.
Question, concern.
What if you had R before the
so it's a different word. You mean a different query.
a RTAR,
okay, so hang on. Hang on, hang on
a RTAZ.
That's your word.
I can't care, but
and so
does it support TA. Star. So they're saying, this means it starts with Ta.
The very first letter is Ta. So you're you're you're you're scratching on something which I like a lot you're like, wait a minute. So right now does this work.
It starts with Ta. We find all of those it ends in R. We find all of those we intersect them.
It could be like.
it seems like he works. Yes.
we were eating a little bit because there could have given me lots of false positives that intersection.
There's all the words that start with TA.
Hab
tack, tacky artifacts, tackle. tattoo tax
do this all day. Tarp? I could do words, too. Yeah. So this is all. This could be lots of words. And how many words in the bar. A lot
so, but how many are start with ta and end with R.
Not as many.
So this works, but the intersection is going to have to do a lot of heavy lifting. You got a point here.
Could I run ta star, get those words back, and then only do this thing on it.
Yeah, all of these. So that's a good point about discuss a lot of stuff. Yeah, we can do that. Okay, so we get all the ta words.
and there could be, let's say, a thousand of them.
and then for those we could check the 1,000 and see which ones end in are done.
So we can. We can. We can stack them and then and then check first.
Okay, that works.
It's kind of interesting. That was such.
Well, yeah, right now, we're just doing yeah. Right now, we gotta be how we define this thing. Is it like, could this be across multiple? We're right now, we're saying it's a single words.
Okay? Well. what if I told you there's a way we can do it all in the index, with no intersections and no checks.
Yeah, yeah, yeah, it is a great crowd. Right? I set them up, you know. Okay.
let's use something called a permit term index. We're gonna have a special token. We're gonna call it Dollar sign to be something else.
Taylor, Dollar, midnight
dollar, and then we rotate every term.
Taylor dollar.
baler, dollar tea, euler dollar, ta lower dollar tape or dollar Payle $4 below the other right? So we're gonna
and now.
so we're going to index all the words multiple times, now. all the rotations. And so, yeah, so this is now all in the index.
And so if the query is TA star. R. We're going to rotate the query so it becomes, our dollar. Dollar is end of word, and this is end of word.
end of word.
TA. Star. and so the star at the end. We know how to do wild card theories with the star at the end, because in our dictionary everything is in the lexicographic sorted order.
and once we find the first one we get all of them.
So
the point here is like Taylor has been
rotated. So there's a version of Taylor that is, in our index, which is our dollar. TA. Y-l o.
and what other words would show up there? Well, by golly. Taylor, TAIL. OR.
Was my other. I had a couple of other examples. I forget them
tater.
So now in our end. I don't have a view. Yeah. So, for example. our dollar tailor, our dollar teacher, our dollar tailor in the index. They all live right next to each other
because we had rotated every turn.
Okay.
that's pretty cool. That's pretty cool, not obvious at all.
But once you see it, you're like, okay, okay?
Pretty cool
question, stretching questions, questions.
questions. Yeah.
So what I'm about to say, yeah, is
like, far science tailored. And it's still another part.
Oh, okay, no, no. So there's not. Yeah. Yeah. There's not. So I'm just writing it out so you can see the words. So you're you're right in the index. It looks like it looks like this.
you know. That's a very good point. Sorry my mistake. So
you're right. This is the word tailor that was indexed like that I was sort of writing it out to sort of clear. It's like the idea in my mind. It's like.
yeah, but your point is correct. So when I encounter these in the index. Unless you like this.
Thank you. Very good point. So when I'm walking down this thing at some point, there's going to be our dollar.
Taylor. our dollar.
Tate. our dollars
halo.
And similarly, you know, if you go a little farther, there's going to be our dollar
to where that ends it.
figure.
thank you.
And then up above it, before the R. Dollar, what's the what's the letter before? Rpq.
How to do that. So somewhere up there there's a P. Let's say.
and then O, PQR.
P. Right? So Q is hard that words that end with Q. But words at M with P would be like.
Yeah, zap
all the way up. Okay. But again, not just that. We're also going to have all the other versions of this in this index which are going to be things like. the
are a dollar
have
Abra? Dab
raw.
It's hard. Okay. Similarly, there's gonna be all these other versions of the R or something, something. Dollars. Yeah.
there's a question over here. So they're gonna have. or like making it together. Sort of giving
OR dot time TAYL, and then but also have
LOR dot, CAY. So, and let me turn
generates lots of them. Correct, correct.
So you're right. So this is like like everything. nothing for free.
Right? So before I just tailor was indexed, done.
or maybe I reversed it. Taylor and Rol, yet
we're indexed. I could do front front. Wildcards and reverse wildcards. Now we're saying we can do any wild card, but the cost of it is.
I gotta do. I gotta pay this price, which is, I gotta rotate every term which makes my index much, much bigger. Okay.
so again, this comes back to is this something our users want? If they want it and they need it. We make it work.
If it's not something anyone ever does, we may determine we're spending all these resources to maintain this big thing. No one ever uses it.
Get rid of it.
Okay?
Premiered term questions.
They're pretty cool.
Last bit
spelling errors. Okay, okay? So Google used to have, I think this down on the surface. Yeah. URL, but they had a whole list of actual
Misspellings of Britney spears.
And this is like 20. This is 20 years old. Okay?
And you can see, like the majority of people, at least at the time, spelled it correctly. You see, Brittany, Brittany.
how many times we saw a query.
you know it drops off
Bidney spears
for timing.
Britain's a name.
What's your name?
Yeah.
So that's what this is. Yeah. So what I want to put you in your in your head here in the last 3 min. Is that
a lot of certainologies? You are so in your DNA now, and you always use it always kind of work.
But again, for free Google pre- web search.
We weren't. It wasn't people searching weren't regular schmos like you and me.
people generating content weren't regular slows like you and me. It was like librarians helping people serve public works.
It was legal, like paralegals searching over case law, and and legal precedent. So imagine the content that's being generated is of a kind of higher quality. People using it typically have some kind of graduate degree, like, there's sort of experts in this.
and we see in the last 2025 years is a total flattening of all of that. And now anyone can generate content on Reddit, for example, on Tiktok, on Youtube, etc., etc. Anyone can now access it, all of us. And so as a result, all of these algorithms and methods. They were designed
free. All of this. And so we see in the last 25 years, is this huge transformation. We now need methods that are tolerant of basically people being idiots and saying prejudge maps, was the word.
So the question is.
how do we now, knowing that when our users come to our you know, they're not gonna search for free Sinatra albums. There is search for great Sinatra.
or they come in. Notice these all end in spears because they're trying to keep it like, keep themselves sane. But imagine the version that don't even get spears right.
Do you know this trick about like, you know, when you get these emails that are like, you don't get too many more recent spam emails and both like mistakes in them.
You had a theory about this. like.
If someone writes me email, it has lots of spelling errors. The idea is like, if it's a scam.
it's sort of self selecting the people who actually like click, the link are the people who disregarded the spelling errors. So they're more likely to be kind of rubes and to be taken right?
So in here. We're not even showing the not Brian spheres you can imagine the nightmare of like
Bryot needs eater, Gla, you know. Okay.
so we're all done. Come back on Wednesday, I mean, have to update the reading through moving so fast. But we got some hot stuff coming. See you all on Wednesday audios.

I think
so
cool.
Hey? Howdy?
Well, have fun and stay hydrated.
There are new rules about water you can bring in how you can refill, so make sure you stay hydrated. We had a game some years ago. Everyone started having beat stroke. So
it was like one of these afternoon, like overtime games. So
stay safe, be good.
etc., etc. That reminds me, yeah. One of the quick quiz questions will be to guess the score of the game. We'll see who does the best we're gonna try to do like a like side betting class.
So we'll see. Yeah, I do want to talk about a number of issues. So Homework 0 came in. Thank you. Great homework. One is about to go out. And so I have kind of 2 big issues. I wanted to ask you guys about
number one is
stylistically, I can do kind of bigger homeworks viewers or smaller homeworks, more like. In other words, smaller home could be.
It's like half of the current homework. That's the style. So I want to get the 5 from the class. How do you feel about that?
Usually you like? Do your homework more time
that basically with that the consensus?
Well, it's hard to judge. Because
but if you knew what you were doing is is not my question here.
So my issue here is like bigger homeworks
with more stuff means you. A lot of people will then not work on it for a while, and it's compressed at the end. not smaller, but more.
Who prefers bigger, but fewer.
Wow.
okay, okay, let me think about that. Question. I gotta figure this out. Okay, the the second issue is this whole. So this is our personal homework. I'm gonna get out today. Is about collaboration policy like, what can we use? And so forth? My style always has been using Google stuff.
Right? You have historically.
you could Google stuff, hey? Like for like on this one, you're gonna have to ingest some awesome basic and do some manipulation. Well, gosh, I've never done that before. I don't know how to do that.
If it were me, I would Google, how do I do this?
And probably there's not exactly what I want. They help me figure it out, find the right function call. And I kind of put it together. Okay, that's me is only fine.
totally fine
where I have a problem is, you know, I don't want you to Google something. And someone's like, Yeah, I already saw exactly what you're doing. And here's the code we copy on Facebook right.
But if you want, for example, when you're building like
like, for example, we're gonna do some parsing those documents, we're gonna do some like a Boolean retrieval. Maybe some other kinds of things
people like in life have done that stuff. And it's online. It doesn't do exactly what we're doing. But you certainly I feel, should be able to look at that and take inspiration. That sounds reasonable.
So if you do that, though, you're just gonna have in collaboration. We have old spot where you're using. Hey? I refer to this your own code.
We're getting a little Fuzzy, or is on the chat Tbt co-pilot co-generation side. And so I tried to start a discussion on slack. There are a few comments.
Do we have any feeling about this
like II don't want you guys getting the homework. You just feed it into that Gbt and get the solution. But I also don't know how to. Not II don't know how to disallow that right.
but I can't observe this
trust, but we care about
trials.
homework. Later
on the midterm on the final right.
That's that's true.
I want you to learn this stuff. That's why I tried to like, not put impedance to you. So like I've had this happen to me in my life. You're like, I can't figure out how to do something, and it's literally there's some stupid function or some parameter. I didn't know
if I knew it, but it kind of blocked me, and I got so frustrated. So I'm trying to take away this block so you can really learn the stuff through the point here somewhere.
I was also gonna say, cause like.
because even though another trick people
people post that big reviews they write. Yeah, code, it will put in there as an AI type thing, such and such. So at least be smart enough to believe that. I will tell you.
This is this is sort of my canonical cheating story. I had 2 students years and years ago.
Submitted the same exact like literally copy is the file copy. Same exact homework.
right? Is the same. Everything's the same at the top of it. You had to put your name in Uin one guy hadn't replaced. He replaced like one of them, but not both of them, and he sat there and he said, No, no, I really wrote this, and I said, Well, how did you write like literally every character? It was like Hunt, not not hundreds of lines. But you know it was long enough.
And he's like, Yeah, no, we work together. We collaborated, but we just sat next to each other, and I happened to like Key in every like
sequence. Exactly. So no, that's interesting. So like, what's your ui in again? Don't do that. At least, if you're gonna cheat, be smart about it. Okay, I'll try to clarify this. You know, the homework will go out later. Today it'll be fun.
No sweat
good. We also have office hours. I posted those on
They're on slack.
and you can find them on in canvas. There's like a button in the front about office hours. And so our ta also has office hours. I think we have much Monday. We have almost every day of the week.
So check that out. Okay.
we're opening a record store. Get a Samuel for pads algorithms. That was by far the most vote bidder.
I'm not sure we're gonna keep calling it have algorithms. It may just become algorithms. So we can figure out how we wanna do that. Also wanna give a shout out to Daniel here.
The the final countdown was my personal favorite.
So look at some of the final countdown. They always play games.
That's a good one. I like that a lot, anyway, that. So we're gonna be algorithms or tabs tabs algorithms. Do we want to keep the cab? Seems a little. We have to algorithms seems too short.
Okay, so we'll keep cabs unless you guys have a better kind of like something in the front. Anyway, we have a name.
and I gotta say it for the rest of the semester. Oh, no. Okay. Last time, where were we? We were doing improving our index right? So we learned how to support phrase queries.
We could do proximity queries, both with positional indexes learned about for wildcard queries how to do a permit term index
where I left it was. We were talking about spelling, and I want to spend a few minutes talking about spelling errors. Then we're going to talk about 2 major statistical properties of text. And why, that's important. But where we're headed to is moving towards ranking.
Okay? So we're gonna spend, probably Friday and all of next week at least talking about ranking. So everything we do now I search, I do a phrase query. We just give the user back 100 h in no particular word right. But your life and the way we engage is always ranked list right when I go into any app, any service
whatever. The first stuff is is the stuff I want.
So there's a rank order to it. So we're gonna learn how to do ranking very, very soon. Very exciting, very cool stuff. Okay.
spelling errors. We know our users make spelling errors all the time.
Britain spears. So how do we know, like someone wrote this? How do we know they meant that. Does anyone else have any kind of like canonical selling errors you make.
But no.
no, I don't know.
Does anyone have any canonical spelling errors. You always make like texting. Or
so.
Okay. 1 s.
Ms, Diaz.
yeah, I used to have little nubs. But I
okay, it's still gonna work. I have replacement tips in my office. So I think it'll work for now. So anyone else have any canonical errors you make, or that you've experienced. Yeah.
so it's like, Look, I don't want to say W or this stuff in class. It's like, learn it already. Right? What's the problem? So part of the issue. There, do you go back and repair it? Yeah. So you you type it one way, it auto correct and you fix it. That's a signal you could use do a rewriting
all you think about in a query world. Imagine in query, world, you type this in, and our query didn't try to translate it. It just says.
I have no matches. Yeah, what might the user do then?
They might type it again, because they're a big kind of but sometimes it's not like lack of knowledge is just. You were being fast. You didn't write it properly. And so in a search engine context, sometimes people will rewrite the query.
So sometimes, you see, you know, query. one is Britain.
and then maybe you get like 0 results. And then maybe they come back and they say, Oh, Britney. they get back a thousand results.
and then they're done.
And so if you saw that in your query log, you have some evidence that like. Oh, maybe this is a rewrite
like they wrote Britons. They meant Britain's knee.
Okay, you wrote duck. You meant
whatever
etc., etc. So it turns out in practice, how do you do spelling correction? If you're a Google or one of these things, you have lots of human provided massive scale of kind of query corrections. Historically, people are getting away from that because the services are getting so good that we type in Richard. It says, did you mean Britney spears? Here are the results for Britney spears, right?
But the idea is historically, we have in query logs, lots and lots of this. So that's just one direction. We're not going to talk much about it.
That's like query, log
mining. So go look in the query logs. See if they're going to rewrite and use that to try to learn these mappings. Okay.
absent that if we're trying to do this kind of from yeah. Oh, yeah.
the news is great.
I thought, that's like VRT. And y.
I didn't know balance.
This is almost like annual visit or something. Nothing was set. Something was happening there. This up really, really does happen. Okay? So the point is like, these are a bunch of Google examples. But like this is, you know
if you look at, you know, by and large most people do properly.
but still this long tail of misspellings adds up to a lot. And so we want those users to be happy. Okay. so does anyone have any ideas on how we might do spell correction in addition to those topics we've already discussed.
Yeah, go ahead. And maybe the
we didn't have some kind of waiting. So if I type in bricks, it's maybe 90 Britney.
10%, something else had some sort of weights.
Yeah, where were those weights come from?
I guess the bottom of the place?
Maybe we're looking some overlaps, and so we're trying to figure it out. That's a really good point. So if someone wrote
I'll go.
That's a terrible example.
if someone wrote
based
baseball.
it's like. Well, it could be. They were trying to say baseball. Maybe they're trying to say basketball. Maybe they were trying to say.
maybe they're trying to say there's 2 words.
it was like base t-ball. There's lots of possible corrections it could be. And so yeah, we could try to measure some kind of overlap that's cool other ideas.
Yeah. Because James, account waits for it is
so maybe somehow. Yeah, you have some kind of like,
you have some sort of like cost model
or something. And you realize, okay? Or something is like a significant letter. Yeah, yeah, this is a very good point. There's lots of other tricks you can even consider. Sometimes people consider like the keyboard, or if you're texting like a little keyboard.
like what letters are near each other to try to figure out like, Oh, you wrote a K on my keyboard that's next to J and L. And I and M.
And so it could just be like, literally, the way the keyboard is laid out, you're more likely to see certain letters than others. So in this context, maybe giving a K. Is quite rare, right? Like you'd have to really find it, because there are no words that have
all the letters around. K. Instead. Right? I end up trick. So what I want to get out with spelling friction, I'm not gonna spend a lot of time. In fact, I'm gonna leave this as a possible bonus on the homework.
What is a bonus cause? I haven't decided yet. If it's gonna be like
like 10 points that are part of the 100
that you get to decide. Or if it's like 10 points on top of the 100, you see what I mean. But we're gonna have a component where you're like, you might want to do this.
Okay?
So one approach, I just wanna give you 2 flavors. We're not gonna spend much time. So I want to move forward. One idea is exactly this sort of previous point, which is, do some kind of like common subsequence matching right? And so one idea is to do like a K gram indexing of all of your words. So what was our word for base T ball.
basic
T ball, so we could break it up into its K grams. So just
consecutive letters. And so we have to define. Is it like every 2 letter, every 3 letter? I don't know, for right now we can do 2 grams.
and so what do we get? BA,
a, s.
SEEK,
KTBA, l. Those are the 2 grams.
Those are the twos. I'll do the threes over here.
VASA, SE,
SCK,
e, Kt, this is hard to do. By the way.
Ktb. Tba. thank you.
And then we can go. Take a word like let's say baseball. Right?
So then we have baseball.
And so what are the 2 grams for baseball? BA, a, S.
SE, EB,
BAL ll, okay.
And so now we can do some kind of comparison. So what is the similarity between that word so call that like word, a
and word? B. What's the similarity between word a and word V. In this 2 gram space.
Well, we have to define some way to measure a similarity. Right?
And so does anyone have any suggestions on how we could compare this set of 2 grams, this set of 2 grams.
any ideas.
Let's intersect them.
Okay? So what we'll do is we'll take
the intersection the size of the intersection of it.
Amy.
1, 2, 3,
thank you.
or 5.
But oftentimes we like to also like we want to normalize it by something. And so one trick
is to normalize by the union of them.
So how many? 2 grams are there unique? 2 grams are there? Total? Well, there's 2, 4, 6, 8
repeats 9. Yeah. there's 10 uniques.
I think someone double-checked me.
So what we would say here is.
they're kind of one half similar base. Get ball and to baseball.
Okay.
now, we can do a similar analysis in the 3 gram space. I've not done it, but we can do it. The 3 gram space. Okay?
Or we can go. Take not baseball, but basketball.
and do enumerate its 2 grams.
And then, probably, like some kind of similarity like this, and see what happens. Okay, it turns out this similarity that we just did here is something called
Jacquard similarity.
and we will revisit that probably on Friday, in a different context. right?
Questions about this.
There's a lot of stuff on the slide. Does it make sense? Enumerate? K. Grams define some kind of way to measure similarity, measure it.
use it for lots of words.
and then we have a way to in theory. Say, if someone wrote to baseball. Should it be baseball or basketball, we'll just measure the similarity which one everyone is higher choosing
done.
Okay.
school super cool. I'll give you a different way to do this.
although this can do be done in combination with other methods. So you could also use this in our Wildcard queries. I'll leave that for your readings to figure out what I mean by that. But the other guy is other other approaches called edit distance. Okay? And so
again, I'm not gonna get into details of this. But has anyone, are you guys, do you remember dynamic programming?
You're like, Oh, man.
so we're not gonna do it right now. But I will tell you this dynamic programming is a classic software engineering interview question.
And this is an example where you can do it. I'll just not gonna get into it right now. I don't wanna hurt your brains. What I will say is, if you take something like, what do we say based
T-ball. The idea is, if we only have, let's say. insert
you can define the operations
like, how do we turn baseball into baseball? What are the minimum number of operations. So we have to define some kind of set of operations.
So we might have insert delete. We could have replace
replace is kind of like a delete, an insertion. So it depends on if we define it or not. But we define a set of operations. And so edit distance is basically like the number of operations
to change
one to the other.
one to the other, one to the other.
So like, how do I turn baseball into baseball.
Okay.
delete
delete
at. So we can say, what is the edit distance
between based
to ball and
baseball? It's 3
delete delete. Insert. Similarly, we could say, What is the edit distance between.
did I write? Yeah, wasn't basic based t-ball and
basketball? Well.
I'm leaving.
I gotta add, I gotta add the out. It's all for also also 3, I think. I think, okay.
depending on how you define the operations. All this kind of stuff. Okay? So edit, this is the way you can then measure the distance between words.
And coming back to these earlier points, you could combine this with things like, maybe you know, you can imagine. Like. you know, maybe we only support like baseball related like content.
So you can imagine there might be a waiting like the number of users who are looking for baseball is way higher than it is for basketball. So we might end up waiting that one more based on some human like user generated data. But again, those are kind of combinations that we can add to this.
I'm not gonna go to the dynamic program. And in fact, I'm gonna leave this if you want as a possible extension on the homework. But I'm not gonna require it. Okay, but I'm trying to give you the flavor of like.
how do we deal with spelling errors? Here are some classic approaches. Okay?
And with that we're done with Boolean. We're done with improving our index. The train moves on
cool questions about anything we've covered so far.
I really destroyed this pencil thankfully. Apple sells me probably a $20 detective, right?
Such a dumbbell.
Okay. Any other worries concerns? No, we're cool.
We're better than cool
chill. It's still you ready? Yeah, okay, see you better than that.
So I'll admit, sorry.
So I don't know. Like, I'm gonna stop while I'm ahead and just say, okay, so we're chill. We're good. We're good.
Okay, we're going to talk about 2 statistical properties of text. When you see this, you may think, well, Whoa, it's kind of like a left turn.
I wanna talk about these because they're so important to the design of our search system. And you're gonna see them come back again and again and again, not just in this context. Okay.
the first one is called keeps law.
Each notice where the Apostle he is. So he's like a like James's law. Okay? So heaps log here.
Peaks is basically saying, how big is my vocabulary like my dictionary.
like how many distinct words are there? So I parse a bunch of documents like, Will I ever run out of new terms that I see?
What do you think?
Maybe like, maybe like a visual after like a lot, maybe after, like a lot of documents. So imagine you're building Google right now like a search engine.
The first document you read. Let's say it's tamu.edu. how many you? How many new words will you discover on that first page?
They're almost all right. You're you're probably gonna see the you'll never see a dog again. It's the first time you ever saw it.
So the second page you read, you'll get a lot of new words your points eventually, like once I've looked at a million pages.
Are there really gonna be that many new words that I've never seen before?
It depends on what I'm looking at. What's the context? Right?
So do you think Google has Google saturated? Have they seen all the words there are?
So here let me rephrase this.
Are there any new words being created today.
Guys about here.
Young people slaying, for example, give you another. What is another origin of of new words? Slang.
Tiktok.
Ms. Collins.
like Britain's
all those misspelled. That's the first time that's a real word we've never seen.
We don't. We don't like it. We want to replace it with something else. But it's brand new. Yeah.
What?
What? What do we found as a word? Why are we here?
I don't know.
I think we're all here right now for a purpose. That purpose
right? Every action you made brought you to this country for old men
got enough coins.
every action brought you into moment. maybe problematic.
So
it turns out in practice.
there's always new words. And so people have cooked up this empirical law that matches what that looks like. Okay. In practice, the vocabulary is always gonna keep growing.
If you've indexed a million documents.
if you index a hundred more, you're gonna see new words. If you've indexed a billion documents and you're going to index the next 1 million, you're gonna see some new words. Okay.
it's always going to keep growing. So here this is maybe your vocabulary size.
The skin is like the number of
you know the size of the text, like just the words, not the distinct ones. But like this is a sentence I am speaking. That was 7 words. How many of those were unique, I think all 7.
But as I start repeating myself. The vocabulary size doesn't track within right.
It starts to fall behind.
hey? And be. And Beta are just parameters that you learn empirically. It's not like a set and stone thing, and it's gonna vary by the kind of content if you're indexing like web documents very different from your indexing python code very different from your indexing some Youtube comments, or whatever it's like, all kind of quite different.
So I'll show you an example.
These are 4 news collections. This is that in bigger. So like, I see 10 million words, 20 million words. I've seen 40 million words.
This is the vocabulary size.
I've seen 50,000 unique. I've seen 100,000 unique words. I've seen 250,000 unique, and what you'll notice in all 3 of 4 cases.
If it has that intuition right at the beginning, I'm just. I'm eating up lots of new words like everything I read. I'm getting new stuff. But notice.
it's not right. It would be weird to shut up like this.
But it's sort of Saturday. I'm seeing lots of words. I see the does the ofs, the boobs, the whatever. But as you notice. they have different slopes. They're like they're all still growing
like. In other words, even though I'm looking millions and millions of documents. I'm still adding thousands of new words.
Okay. so
and so you can just go use like excel, or some tool that you could find. What are the that fit this thing? Okay, that's not my concern right now. But imagine you can fit with the curve. Looks like to hear, let's say.
And then I could say, What if we index enough? What if we double the size of the index? We've seen 40 million words? What if we see 80 million words, what would you expect the vocabulary size to be? Well, if you set the curve, you walk it.
you know, wherever it is over here. Okay. so now I turn that around to you and say.
How does this affect us? Why is this interesting to us?
We are designing our search system. Why do I care about this?
That's all experience.
Safe had to be polite.
obviously spaces where we
I gotta store the vocabulary. I've gotta design my system.
I go tell my boss I need how many machines, how much memory, how much this etc., etc. And
let's say we were here, and I told my boss. that's my vocabulary size. This is, I need whatever like resources to cover this, we're good.
A week later, a month later, a year later, go back to your boss and they screwed up.
I need more resources.
Do you not know about use law?
Just.
yeah, of course.
took the stats and talked about it.
So if you were here today, and your boss said, What resources do you need?
Presumably on the back of an envelope, you can say, well, at the rate we're ingesting new documents.
you know. Let's say, this took you a month to do.
By the end of the year. We're gonna be, you know, way up here.
So I need to have the resources ahead of time to support the index that I'm trying to build. But it's a very practical reason why this is important. Cool any other reasons you can think of.
Yeah. After a while you care a lot less than so if you time or care a lot more about retrievers.
So the point is over. Time here, hear less about insertion, more about retrieval. So what do you mean by that? So another good point, which is.
you might think about like the data structures you use over time. It is still growing, but the rate of increase is so much smaller.
So you might have some design choice that you want to make, knowing that the insertions are so few, for example.
alright. Any other comments about heaps. Law. it always keeps growing.
Yeah, I'm just curious about like the words in the collection. I mean, do you know, like entry. Yeah, absolutely so
empirical law, right? I'm not proving to you some property. And so you're right in the sense of like. these are different news like this is a Wall Street journal. This is some Ap. News wire. There is some discussion. There's some decision about
how did you? A part of those documents?
White space, lower casing. All of those will affect how many unique words you see. In fact, I'll have some significance on that. In just a second those design positions definitely affected. They'll impact, you know, the the the sort of the rise and the ultimate slope. But the idea is, you know, you could make sort of more lenient choices. It would grow faster or could be done here. But the point will always continue to grow.
Once you've made those combined choices
pretty much. Yeah. do it?
I mean, as an example, like, I'm not gonna do this now, just go into Google sometime and just type in some like nonsense string.
And typically you'll get a response. It's like, Yeah, there's like, people have published that that random stream before. It's like what?
Because people are just generating lots of new words every day. Okay.
yeah. Oh, yeah. Why do? Why do you think we observe it? Yeah.
Because after so much time, there's only so many words that realistically write.
So you're talking about the saturation right at some point we've seen a billion documents, I mean, come on, they can't be much more.
but yet we do see a little bit more. And so why do we keep seeing a little bit more?
Why do we never saturate? And we talk about all these reasons, the spellings just innovation, creativity.
I mean, have you ever heard of Barbenheimer before a month ago? No. And all of a sudden. That's another interesting thing, which is, you might look at this and think that, like all of these incremental new words are not interesting. Oftentimes they're the most interesting
Barbenheimer brand, new or never seen it. And yet we have to be adaptable and be able to serve millions of people looking for it. Think about the invention of new drugs. They always have insane names.
right? All these kinds of things. So the point is just because you saw where earlier doesn't mean they're more valuable.
Some of this stuff can be noise. But sometimes it can be quite interesting. Okay. you know, why would we care? The second law
adds, like 2 laws of power, keeps law. Zip's law.
Notice where the eposphere is. This, is it apostrophe? S. So Zip is, is was a Harvard linguist.
and he was trying to figure out the relationship between the rank of a word and the number of times it occurs. And so basically, he would say, like, if the most popular word
and what's the most popular word in your text?
Duck, yeah. Duck.
sorry my my PIN is is is having trouble here. Duck. If number 2 was the number 2, most popular word
yo, 3
pay 4 by
we have 4 words. So this is the most popular word. This is the fourth most popular word. The expectation is something like this. If I've seen duck a thousand times.
how many times did I see yo, maybe 900, maybe 800, maybe sort of linearly drops question, mark.
or what like, what's the relationship? Or does it go
a thousand to like a hundred to 10 to one.
or what like? How does this is the rank order? What is the expected like? Number of times those words has occurred, and I'll tell you why this is important in just a second. So Zip was trying to understand this, and empirically trying to look at actual words and documents and figure this out.
So again, in a lot of data sets, we're like, the is the most popular word was any people. Maybe your name is like way down there right way, way, way down. So all of these words are happening a lot.
and so I'll show you what Zip's law is. Then I'll show you in practice what it looks like. And then I'll tell you why we care.
So he basically said, the I most frequent term
occurs proportional to one over. I
so cf, you're like the collection frequency. How many times have we seen that word? Okay? So if you're the first word.
one over 1 1.
If you're the second word, we expect to see you about half. If you're the tenth word. We expected to see you about a tenth as much.
If you're the hundredth word about 1 one hundredth as much. Okay.
kind of the takeaway you'll see from this is that a few words occur a lot.
Most words are quite infrequent.
Have we seen a distribution of words today with counts on them?
Not just a word, but phrases we saw.
This is not exactly. It doesn't exactly fit that exact idea. But notice
have dropped fall from half a million are correct.
So that's 40,036 week, 7,000. 2,000. 1,000. But how do you come down
to 9 to 8?
So we move down to the ranked list.
You see, we're getting this like long tail
of of of misspellings. Okay, this is sort of a zipfyen type distribution. That's what Zip is getting at.
And so from one of the data sets, I think. Yeah, I'll have it in the next slide if you can just take the. There's 2.4 million times 1 million times. This is like 300,000 times 70,000
notice. It's not like perfect.
So this zipping distribution should say, you know, this is like one over 1, one over 2, one over 10.
So you can kind of say, well, you know, half of 2.4 million is like 1.2 million. This is wrong. Yeah, someone that was is not exactly a tenth of the.
But it's in the neighborhood.
And this is not exactly one fiftieth. But it's in the neighborhood. Okay?
So the point of the zipzer distribution I'll show you for the full. This is for Ap. 80,000 Ap. Articles
where they just counted all the words.
And so what you're gonna see is kind of that same idea, 2.4 to one. But notice, by the time you get to the 1, 2, 3, 4, 5, 6, seventh word.
We're only a half a million very cheap with 5 min. No. we're down to a hundred 1,000. After some 2530 words. Okay? So like, if you add it up.
Like all the occurrences of all of these words, right? It's nowhere near the
that's 2.5 million.
Okay.
so a few words occur a lot the of to and in.
if you add these words up, that's 3.5 4 1, 5, 6, 7 is 7 or 8 million occurrences just there.
But how? How long will take the 8 million in time? Down a lot more words if I go even further down the distribution, it's gonna take, you know.
maybe millions of words to equal that. It really drops off. Now
this is Zip's law. It's an again empirical
statistical law. There's not like a theorem that proves this will always happen. But in practice we see this
over and over. Take a different data set, the words will be different. Accounts will be different.
The flavor will be the same. Right?
So oh, there he is! There's George.
handsome, handsome fellow question. why do we observe it? This is what George Zipfs said. Tell me if you believe this.
he said. He had the principle of least effort.
a balance between the speaker's desire for a small vocabulary and the hearers desire for a large one.
So I'm going to use the speaker like I'm talking to you. I don't want to use lots of crazy words, because I'm lazy. I want to use use least effort.
But you, the hearer, want to be surprised and excited, and you want to hear new words. And so it's kind of like we're having this interaction.
and Zip's law falls out of it. You believe this?
Probably not.
This is this guy's terrific. He doesn't know.
The point is, we observe it. Oh, so I'm not. Yeah, I don't. I don't agree with this. So it's relaxed.
You can read more about zis law. There are lots of people trying to then sort of give, like more principled reasons why we see it. Okay. I'm not convinced. I understand why. Okay, all I know is, we see it over and over again.
Okay, now let me turn it around to you. Why do we care about Zip's law
and keep this in mind? We're gonna see it all over the place.
You see it in like when we index documents. we're gonna see the zippy and distribution in the words and see we're gonna see something akin to it in our queries from our users.
A few queries occur a lot, but most queries are quite rare
when we post documents a few words occur a lot. Most words were quite rare.
Why do we care about this?
Yeah, it's gonna affect the design of our system, including the data structure than the algorithms.
Specifically. right now, the you know we're building a search engine
over this Ap. News articles, and we know the is 2.5 million times
limited concrete. We're building a positional index. Remember how visual index works.
I gotta tell you where the word occurs in the documents. Okay. so just the word the is, gonna give me 2.5 million positional entries in my index
for you to know how much it means a lot. But it's only 80,000 relevance.
So I'm just in 2.5 million. Whatever units of you know desk or memory that we need
just to support the one word.
So if you see why in the olden days. But again, this can. Memory were expensive. People are like, when we use stopwords.
Oh, yeah, what's a good software here, though? Oh, yeah, it's gone.
I can save those 2.5 million whatevers for all these other words.
Okay.
let me reframe it like this on the query side, if I told you the queries were zippy.
because that means a few queries. I see a lot.
Most queries are quite rare. How would you design your system for those few queries
that are quite popular that you can see a lot of. yeah, you started programming and destroy the result
cabinet. There's like in Cs, we learn like 3 things.
Really, one of them is caching.
I don't forget the other 2.
If people are, gonna ask the same thing every day.
If you're like me. I
I have to. I ask for the college nation garbage schedule every week. Always forget 2 days of recycling. I forget. I ask that over and over and over again, and it's the same result every time.
If you're like me, you're a Tamu football schedule. Okay, whatever
you know, demo classes something like this. File exam schedule.
There's some things you don't need to be sort of free to meet artists, or you don't need to be like on the fly before
Pre computed results. Okay.
that's a great example. Let me show you in practice. There's if.
by the way. he dying, so long as you 48.
We'll go.
You guys are like, but we're young.
You don't even sense time, right?
Not at all. Not at all.
You're like, Yeah, I'm 21. I'm 20 can't be hurt
and indestructible. I'm gonna live forever.
That's right.
I like this attitude. I like it.
Okay.
let me show you just a quick example. This is from the the readings.
This is, I forget. This is Ap. This is some like news articles. This is some like news articles building the index. I just wanna walk you through
how these laws are going to affect us when we design our system.
So first of all, deceased turn live above my vocabulary size.
This is my vocab. This is my dictionary. like when we talked about like we have our dictionary.
So what they're saying is, I have around 480,000
focus
and unique words, and I throw away numbers. I throw away a little bit 11,000
if I do peace folding. but make everything lower. Case.
I get a free drop. What do they get rid of the 30 most popular software? They're looking down by 30
right. Who's the dictionary? Not the postings?
What about you? Spinning? Spinning is, you know. You turn with the singular turn, everything singular. You drop the I and genes, this kind of stuff
the keeps them on. You go from about half 1 million, depending on the sources to the 400,000, maybe down here.
That's just your dictionary. Now. non-positional boasts.
So just if we're, this is basically saying, like, you know, this is my dictionary.
Non-positional postings is just like.
how many of those do we have?
This is just the 5 vocabulary. How many non-t additional postings do we have?
I see you guys moving. Someone tell me to say, Cat, be quiet.
or being worse.
4 min.
So let's get to that audience. It's 1110. We're done
bye
that.

Okay.
Howdy? Howdy, friends? Oh.
buying new! And I already had the next group. The last time I broke with my weapon, and it came with a handy, dandy little like a little cover.
So yes, it's like this little hat.
So, anyway, welcome back. It's Friday so nice to see all of you.
Some things that are coming up oh.
is showing. But I think before you don't see that deed.
Hmm! It's weird. Let me
it's really weird.
Let me?
Jaka Bernat
Well, it's gonna just live there. I don't wanna deal with it, anyway. Homework one is out already. A couple of things on homework one. Are you gonna tell gate, or does your campus tell gate or anything for football
they're having? They're just like hanging out.
We're gonna record. It's gonna be recorded. So we get to hear what they were talking about a risk. If you wanna play the zoom game, okay, private information will be blasted out on there, anyway. Homework one is out about half of it is, or more than half of it. You can basically get going right now. It's all really stuff, right? Building index and give you some lyrics. I think it's a super fun thing part of it is on ranking, which we're gonna start today
and make sure I'll have it on the readings. You'll have everything you need for that as well. It's due what? Like a week from Wednesday, so you have time. But I would suggest getting started this weekend
on the homework. Quick, quiz. 2 will go out immediately after class. Remember, it should only take like 5 min, Max.
but you have 48 h to do it, and there's no class on Monday, right? So no class, you know the whoop. No, if you want
easy.
if you want to. We could still meet just virtually just hang out at this time. If you're if you're if you're missing that 470. Yeah, totally right? Okay, cool.
Okay. So
oh, yeah. Questions concerns the freak outs any other stuff.
Everyone who
need to go like it's a priority. Get a job. So please go the slides will be made available to stay on top of things, because again on Wednesday. We're gonna cover tf, idf, which is like core on homework.
You can get ahead of me in the readings things like that. But like you're gonna need that information. So if you missed class, make sure you don't let it slide. Okay.
okay. Last time we were talking about the index, the impact on index size of So we're trying to understand some decisions we can make. And how like that, Zip's law connects to it. So this is one thing I just wanna talk about, which is we're walking through. Remember, this is our dictionary. But how many words do we know about
this is how many postings those are like the Doc ids of that word occurs in okay. And so I want you to notice. And then you're non positional posting. So non positional means. I just know that, like the is in a document.
Positional posting means. I know that is in the document. Position 1, 8, 15, did it lots and lots and lots. So if you notice, if you compare the number of non positional positional postings
you go from like a hundred 10 million to like a hundred 97 million. Okay.
but I want you to notice. According to zipz law, we know that a few words occur a lot. So can anyone here identify how we really see that impact of a few words counting for a lot.
where should I focus my attention?
Well, yeah, there's a lot of numbers up there.
So let's do this.
If I get rid of the 30 top, most software's the size of my dictionary, followed by
third
right. I just think Thumb is no longer there
walking across here. If I get rid of
the A in my additional postings, I dropped it by 13 million.
For 30 words.
Walk into here. I go from 180 million words to 120 million. Sorry token, additional tokens, but I lose 50 million
by dropping 30 words.
You see what's going on. So the occurs all these places.
So if I just don't index dot. I go down by like 50 million
positional postings. They're just gone. So keep that in mind. That's what's going on here. You talk about like a few words occur. A lot like that's
that is what we're talking about. Like, that's 30 words.
But 50 million
postings disappear.
Okay, so again, this is the argument historically. Why, again, disk. Expensive memory expensive. You're like I can get rid of 30 words save a ton of space. Great
these days. Of course, our users want all that stuff, and so instead, we're going to eat it and keep all of this right? We're not going to get rid of it.
This is the last bit I wanted to show you. That's that.
Pack it away. We're done. Okay, this is it ranking what up we're doing, ranking? This is now the most exciting.
Sorry, Mike
Zoom, everything's dying right now. Okay, hang on.
So let me lose my brain here, so I need to. Now
come back to this zoom share
screen.
Hey.
we back. Yeah, we're back.
Hey? Look now, the thing is gone. So weird. What's that about
so weird.
so ranking. So our capabilities. So far we can do Boolean retrieval stock. We can do ends, or is it not? You can do phrase theories. You can do proximity theories. You can do wild card periods.
We can do all of these queries over a single index, or we may have multiple indexes.
We have an index for the titles, an index for the lyrics, an index for the dot. Okay?
But in practice we very rarely just rely on this kind of Boolean stuff we want to do ranking. So one thing to make note of we can kind of get like a quasi-ranking even in a Boolean world.
so
we can rewrite our queries and run them against multiple different indexes as part of a strategy. So keep again, we're designing this workflow. So, for example, the query comes in, just meet me at midnight.
Okay, we can take that query. And we may have multiple indexes. And here, what I'm showing you here is like, imagine
I have one index
for my top artists and one for my lower ranked artist. How did we determine who's the top artist?
I don't know. They sold a lot of albums. They're famous, I don't know, but the idea is like, if you're looking for meet me at midnight, we can take this query.
We send it here, and maybe we get back 10 dots. and then we send it here and we get back. Let's say a hundred docs. So you can give to the user.
You can give like those 10 docs first.
So it's kind of like ranking amongst the 10 documents. There is no rank order, but the idea is we're gonna give you hits that come from like the better artists than from the worse artists
make sense.
It turns out at Google. Like all web pages, they have different indexes called a tiered index for different quality web pages.
So what kind of web pages do you think show in the topmost tier for Google's index like an example webpage that would show up in the the highest quality tier.
huge
Youtube.
Wikipedia, right like every result is Wikipedia. Now on Google, okay. what would you say would go in the very like lowest tier
blogs. Some like user generated Crappy blog my blog.
We believe I mean, you can imagine, maybe multiple indexes. It turns out, of course.
the the the highest index is usually the smallest. and you go down I get much bigger and bigger and bigger. At least, I think they used to call the very bottom tier landfill.
I think that's what it is called landfill. So that's just the junk. So again, the idea is, if you have multiple indexes, you kind of run on the first one, and then you sort of drop down if you need more.
Okay? So it doesn't really do range. But it's kind of like it. Okay, we can also do things if you just given the query. And suppose we're hitting a single index. So let's write here, you know, here's our index.
We can rewrite this using some of the strategies we already know.
So, for example, meet me at midnight, we might say, Aha! First whoops
first. We're going to try it, as a phrase. meet me at
midnight. We hit it, and maybe we get 2 docs.
2 dots. Exactly. Have that phrase. Okay.
then, what's we want to get some more documents. How else can we sort of take this raw query and rewrite it and send it to our index using our Boolean tricks?
Yeah. So so, for example, we we could do like meet me.
And yeah. midnight.
So again, it doesn't mean that phrase happens together. But it could be that. You know those words are near each other, and that gives us back some docs. Maybe that gives us back
syndox.
And then at some point, you know, we could just say, Okay, meet, you know. And
me, and at and
midnight. okay.
I mean, that brings me back, you know, a thousand docs. Okay? So again, there's no like right way to do this. But in practice.
when we had these queries, we never, you know, practice. We rarely ever just take a query and run it instead. You may rewrite it, thinking that documents that exactly have the phrase, or, better, the documents that just happen to have some words. Okay, so this or this is called a query rewriting. It's a whole area of research. That's what that is cool.
super cool. Okay.
Notice. However, everything we're doing so far sets. This is a Boolean set
I may have like. here's a sense, but I'm still just giving you back stuff. and you have to figure out.
So maybe we get lucky. And I only find 4 albums, and you can easily look at the 4.
But in practice we're gonna be getting you back hundreds or thousands of albums. You have to do all of the work. We don't wanna do that. I don't wanna do that. No user of any system ever wants to do that. So instead, we're gonna do ranking. You're gonna query, meet me at midnight, and we're gonna give you the order. 1, 2, 3, 4, maybe our interface is defined. We can only show you the top 4.
Maybe we want you to pop one.
Maybe we show you all of them. Okay. But the idea is we're gonna focus your attention on some of them.
And even if there are. let's say. thousands of matches.
we're only needing the top 8, and you can scroll to get the rest. Okay, very natural. I think we're all used to this style of interaction.
So our plan today, and then next week, Wednesday, Friday, we're gonna talk about why, ranking is important. Okay, what are the factors that impact ranking? We're gonna talk about 2 foundational approaches vector, space. And this first one, this is on homework, one
very, very important.
And then, probably after next week, we'll start doing some basic machine. Oh, yeah, we use an Ml to help us do our ranking right?
It's gonna be really awesome.
Okay? So why is ranking important? Why do we care? I ask you
give me some examples where ranking some service you use where ranking is important to use.
Yeah, when I Google.
the coding problem, I wanna get the most relevant results.
Verse and not random. I want
high quality stack overflow.
First.
I just just show me the answer right? And one of the things we're gonna talk about in the Llm. World
is all of this retrieval style is the search engine gives you back documents and asks you to figure it out hopefully, the first one really does it. The Lm style is just gives you the answer.
short circuits, all of this. Okay, so we're gonna try to get there. Great Google coding example. I love it. Give me another example. Yeah. Favorite family member.
okay, so this is offline. I think, favorite
family member. We all have them. Your parents, by the way, if you have siblings, your parents have a favorite. They don't tell you that it is true. It's true, it's not true. We we love you all the same.
But if you think about like, let's say your family member and extended out to some social network, do you? I don't know. Are you on any social networks now, some. I don't know what you guys do anymore. Everyone's off the stuff.
Linkedin. Maybe. Yeah, we'll go crazy on.
But I think that Linkedin, where? What is the what is the ranking component of Linkedin?
Good evening.
But I mean, like, what are they ranking on, Linkedin? It could be your connections.
it could be like, I don't. I don't. They have sort of sort of post people post stuff. So you have like a news feed.
So think about a news feed is you're not doing like an explicit query. So the query is more like, it's me I logged in, and the ranking is here are recent posts that we think you would like.
Okay, so that's an example of why ranking, maybe it's important. So the idea is, you can read the good stuff first, maybe maybe not. Okay.
Let me show you in practice.
Why ranking is so critical. Okay, so this is a sort of foundational work back in the mid 2 thousands. This is what Google used to look like back then, kind of old school. And they basically set up an eye tracking study.
Okay? And they wanted people to go use Google. But in on a desktop.
And they had them. I guess there's some questions using Google. So hey, they'll find out, like, what time is the football game tomorrow.
So go to Google.
and they would eye track them to see where they looked. Okay? And then basically recorded all of these eye movements and truck tried to figure out how they navigated through Google. Okay?
And so if you don't know eye tracking as we're not doing the details, you know, you're wearing this. You have this device here, the ipad and device. And they're looking for like fixations. That's like when you kind of focus on something. You also have extreme rapid movements. You don't realize your eyes are moving a lot. They also can look at people dilation. Okay? And so, for example, if you're on this web page, they may see so much time here watching where your eyes move. Okay?
So what's interesting is what they notice. This is on Google, okay. But where do people look versus? Where do they click. Okay? So if you look at this is how much time they spent reading the little snippet.
So this is this right here, this is time coming down. This is the very first ring, the second ring. So you're kind of seeing down the list.
But really people only see in person, or maybe the fourth and drops off. Where do people click?
That's the that's the dark. They click the first result. This is the same result.
So it's not that ranking is important. But being ranked first, is really important. Okay, so you get some results. You took the first.
Maybe you're using the other like abstracts to give you some context. Okay, what does the second say? Now, it sucks the first one click.
Okay. so
this drives everything. Now, people want the first thing, yeah.
Google is, a company makes money is because they sell ads. Okay? And so the idea. And if you notice, Google's gotten more like, more and more ads over time. Used to. They were just like, I think.
But here you don't even see me ads right?
But when they decide when they had in the old days they were just on the side. Natalie put them up top
right? And yeah, so you have lots of eyeballs, and then they usually denote it with a very tiny ad. Maybe it's like a little grey or sponsored. Exactly. So imagine again, like, maybe your sophisticated user. And you can distinguish this first thing I see. But money, money. Yup! Yup!
This is showing you when your eyeballs saw the result. It's just saying
you saw the first result first, the second result, second, third, fourth, fifth. So the idea is, people where their eyes even begin. We've all been sort trained.
The top left. Read it, go down.
Okay, so this is just sort of saying, like, Yeah, it matters. And again, just to emphasize this like being first is so important. Okay, you're first you get all of the clicks.
This has a huge monetary impact. Kind of what you're getting at. I'll show you like if you go. This is on the the Amazon app and search for Texas flag. And you'll notice, even in Amazon.
sponsored.
sponsored like Greg.
I think.
sponsored. I had this. I had this role to get to the first quote organic results. Okay. But the design of this is basically.
if you are
G 1, 28, you are paying Amazon in this case
to have the featured first spot to get people to click on you. I'm much more sophisticated, and I noticed a little grey. If I scroll down and get to the annually fly breeze which is here, and there's only you'll notice $8, but cheaper
this is, if you go into some app store and you search for music on the left. This is sponsored.
serious text. I'm getting paid to be here. The rest of the theory is organic.
Look, what's number one on the app store? So this is a separate thing we can talk about, which is.
there's lots of
historically, lots of concerns about what goes into these rankings. And like, of course, if you want to feature your own content over others, it's a big sort of self dealing problem. Okay, this is looking for games. So again, magic is an add, you can see, sort of a different color. And at the time it was, whatever's I don't know. Okay, but again, so something like games, you're getting a huge amount of people showing up
people. Click the first one you're gonna get a lot of business.
All of this manipulating or engaging and ranking is a whole industry called SEO search engine optimization. Okay, this is a 120 billion dollar business
every year. Okay, it's growing like crazy.
There are. You can find articles. If you Google for SEO, there's white hat, black hat. There's gray hat. There's all sorts of issues here. There's lots of really shady stuff you can do
and we're gonna talk about this later in the semester about
people basically attacking the search engine to get higher rank and why they want to be higher rank. They want more clicks, more ads, more whatever. Not just that. I want to be higher rank. And I want my competitor to be demoted.
Right? Yeah. So what is the difference? So basically, like, if you're gonna do SEO like
what? Who who decides if you're doing the good stuff or the bad stuff?
Hmm!
Very tricky.
And so we're gonna sometimes, historically, I've done a homework assignment where you guys engage in an SEO contest.
Would you be interested in doing that?
Is it difficult? You make a web page.
Oh.
and you try to be first, and I give you a phrase that's never been indexed before. and we see if you're the first or not.
And so we have to make some guide rules. So, for example, things you should not do is Do Wiki vandalism. People go into Wikipedia immediately, and they make a Wikipedia page about that phrase, and then the editors have to, then go roll it back over and over and over again.
So to me that is something you should not do. It's unethical. And so we make that as a rule, do not do the key vandalism. Okay? But in the past people have made a yelp. There was a Chinese restaurant on Yelp whose address was my office. Yeah. And people I forget. There was some dish that was really good. I forget in my office. So that's that's that's funny.
Then they had my phone number, and people kept calling and ordering, and I was like. No, no, no, that didn't happen. That didn't happen. Anyway, ranking is super important. SEO is a big deal. We're gonna revisit this later in the semester. Okay.
What I wanna do now is just sort of, I hope I convinced you breaking is very important to megabillion dollar industry. Everyone cares not just the search engine, not just the Amazon. Search the app store search, but all of the people creating the stuff. If you have a restaurant, if you sell flowers.
you want to be ranked highly. If you sell flags, you want to be ranked highly. So this is big competition amongst all these people sort of mediating with these big companies. Very complex, very big market, very, very lots of goofy stuff going on.
What I want to do is freeze that for a moment and talk about the factors that we think impact ranking. Okay? So again, the query comes in via midnight. This is on tabs algorithms, the album store. We're mailing people, these lps.
we're gonna do some ranked list of albums. Okay?
So in this specific context for cavs algorithms.
By the way, did you know? So
that was a Czech pizza idea. I did not know that. Thank you.
Self. That's good, full disclosure. I like that. What are some factors that we should consider when we rank things.
Meet me at midnight. What factors should we consider when we rank our albums? I need at least 6 factors. Let's go
say to
okay, popular art, art artist, popularity.
And how do we measure artists? Popularity? It could be previous fails.
etc., etc., etc. Okay. Great. So more popular artists should be favored over less popular artists. Give me, hold these one else give me another way.
The popularity of the particular tracks. so we could also do album popularity. We can do track popularity, etc., certain different ways to measure popularity. I acknowledge that, but those are all great factors. Give me some more
really safe. You want fresh or old release date.
Fresh fresh is best. Okay?
More best mentions that we're yeah. Some some sort of text
text or content match.
It says, Meet me at midnight.
Okay? And then we have to talk about right now. We've learned how to do so far is like exact word match.
But if I said, Meet me at 1159 PM.
That's semantically similar to midnight, so we might want to match on that as well. Okay. So later in the semester, we'll learn how to kind of loosen those exact matches. Okay? Or if he said, meet me at noon.
Well, noon as a time of day. It's also 12. It's not midnight, but it's sort of somehow related, and so you can imagine we may want to relax that exact match to support that as well. Give me 2 more
yeah price. And what do you want? Cheap depends, depend? Oh, it depends sheep or expensive. But yeah, certainly price is an important factor.
And so part of that, what's good about this like freshness price some of these factors. They may be personalized, and they may be for specific kinds of items.
Right? So if I'm looking for Frank Sinatra albums.
I would say release date may not be the most important factor, because they're all old.
but if I do look for Taylor Swift, more likely I'm looking for the newest album.
So it may depend on the user's intent. Same thing with like price, cheap or expensive. So there's there's stories that, like.
historically, if you were using a Mac. On! I think it's like on Amazon.
You might see a a higher price than if you were using like a windows or a Linux machine.
So the idea was, they're doing some sort of price discrimination, they say, oh, if you have a Mac, you have more money, we can charge you a higher price.
So these kinds of tricks are employed. Give me one more factor. Yeah, availability. Love it. Yeah, I better be able to get it. So don't show me stuff that you don't have love it.
This we just hooked up a bunch of factors. We can do more. Okay, but let me consider another scenario.
Let's look at If you're doing Linkedin job search.
give me quickly 5 factors we care about. I'm looking for software engineering jobs. What are the factors they can consider when they show me their list of jobs.
I guess I would say, like how many like
or previously hired. So, for example, like, you'd probably want something like Microsoft or Amazon, some sort of higher success rate, something like this. Okay, great. I mean, another factor. Okay.
location. And maybe you want to say, near
nearer to me could be a factor. There's lots of ways location can play in. Give me another factor.
Yeah. So match match. Kind of my my expertise.
My seniority.
Yeah, sometimes you search for something. It's like, you would be, CEO, we're just getting started here. Yeah, okay, well.
like, ideally, you want like money. Yeah. show me the money. Yeah.
This is also sort of our freshness. Yeah.
They posted show me jobs that are new. Don't show me a job from a year ago, I mean, that's not very helpful. Okay, love it. Let's do one more scenario. What's a
what else do you guys search for? That's interesting.
Who's this lead?
Okay? Well, so what? What do you use? Google? Yelp. So we'll do, Google Yelp kind of food.
Okay, give me some factors.
And so and give me your your comical theory. What are you looking for? Different kinds? You could be saying, I want thai food, or you could say, I want cheap eats, or I want food for a day, or I want you know what I mean, like, thai. Okay, thai food
go any factors people haven't heard from. Yeah, good distance from where you are.
Yeah. Closeness.
Cause I do this a lot. And I'm like, Yeah, I want some Fajitas. No, I don't want to go to Papacidos in Houston, though I love it. But it's too far. Yeah, man, like, is it good or not? Rating is super valuable over here? Yeah.
yeah. Don't send me some places closed. Dummy? Right? Like, do better. Yeah.
Oh, okay. So more kind of like more options. There's delivery is one of them. But it could be kid, friendly, pet, friendly, etc., etc., right here.
Okay, price range. Is it one of those, or is it
one of those? Right? And you're like, come on, do me better. Yeah.
And scary
relevance to my my, this, maybe this search.
and then we'll also say kind of like in sort of to my profile intent.
hey? Every time I've ever used a service. I've gone to the cheapest place, so when I search for thai food, don't give me the 4 star Thai food place. Give me the cheap.
So it's not just this query, it's kind of knowing the context, beautiful. This is a good one, because you guys are like we do this all the time. So give me one more factor here.
Aye.
we already have open
about about this one.
Have I been there before?
I think Google does a thing now, or one of the services it's like we're in a new city. It'll give you, it'll say like, Oh, you might like this because you've been to places in college station like it.
so trying to give me some like vibes based like, Oh, you like this kind of place. So here's some things you might like. So something like, Have I been there before? Is it the kind of thing I like? Okay, this is cool.
So we're gonna do another one. What I want you to get at here is we just cooked up
1015 factors fast, right in practice. These search engines use not just 10 or 15. They use lots and lots. So earlier this year in January.
the index, anyone know the index.
the index is most. I was going to do my bad Russian accent. It's number one search engine in Russia.
the index. Okay? And someone someone, a former employee leads Yandex source code.
This is like Google of Russia. Okay.
they leaked it. People started going through it and found
1,900 factors. They consider
not 5, not 6, not 7, 1,900 factors. Okay? And so there's this nice URL. I leap for you here, and you can find all of those factors.
and they include things like page rank. We'll come back to that later on. But, like the age of the page, the freshness, how relevant it is! The text, relevancy to the query, the content agent, freshness.
end-user behavior signals. This is like, do people click into it or not? Post reliability is like, if you try to load it? Is it fast. Does it load also, some like special like special code says, if this is Wikipedia, you get bonus points, no matter what, just for being Wikipedia.
But anyway, there's 1,900 factors. So I want you to think about now is you're trying to write a ranker. Okay?
So we're gonna have to take 1,900 factors. Or, as we said before.
1, 2, 3, 4, 5, even 10 factors.
or each factor, we, we measured 5 different ways, 50 factors.
And so now we're gonna write a function
that takes in 1,900 factors, 20 factors and gives us a score and allows us to do ranking. Okay? So again, where are we? Where are we going now?
We're gonna figure out right now, we're gonna simplify our thing.
Okay.
we're gonna ignore those 900 factors and how we combine them. We're gonna start with the 2 foundational ir methods.
Vector space VM, 25, 2, 2 methods.
Okay, they consider just text-match. They ignore everything else. Okay.
we're gonna learn how to combine those by hand to write functions that take
vector space plus bm, 25 plus freshness plus whatever, plus the 1,900 factors. We're doing it by hand.
And then we're going to learn how to use machine learning to help us write those functions, instead of doing it by hand, will let the machine learn how to write the function.
Okay, this is kind of what we're doing right now
to rewind. We're gonna build a scoring function. You give me query.
You give me documents. You give me score.
Okay.
So, for example, score of what is it? You know. Meet me at
midnight. The document is
Taylor Swift Album. I forget the name of it
equals point 9 score of
meet me at midnight.
Calves. self-produced
mixtape.
low score.
So
query document the output of score query, document output a score.
If we run this scoring function over every document.
what is a document? It's an album. What is a document? It's a restaurant. What is a document? It's a linkedin.
a job. or indeed, or whatever you guys use.
We're gonna run that query over every document, get scores. and then we just sort them by the scores. So this will bring higher limits
school. So we got to cook up a scoring function. Wemo.
before we do this.
let me come back here. Erase this for a second
score. Query, document.
So what do we have here? Let's do the
CAD's algorithms. We have popularity release, date, text price availability. What I'm saying is, we're gonna write a function that says something like this.
It says.
what is the price of the document?
Some function picks them. The document is an album. and maybe we have some parameter point one times that plus. Then we have some freshness spore
which takes in a document. and it gets some function. It it applies, some some parameter point 2 to it. and then we do some kind of text match score between the query and the document.
and that's point 3 of that.
And then we take point 2 of the popularity of the document. Then we take point one of the popularity of the
popularity of the artist
of that document, plus. So I say, we're gonna write a function. This is what I mean.
We're gonna write a function that takes in all those little features that we cooked up.
and it somehow combines them. And at the end of the day, you know, it gives me a point 5 3. It gives me some number.
Okay.
and you put in a different document. And it gives me a different number. Okay.
so that's what we're gonna do. We're gonna try to write these big functions.
Okay? And so the index is doing is they're writing a function that has 1,900 of these, 1, 2, 3, 4, 5,
1,900 smashes them all together output score. Okay? And so what I'm saying, what we're gonna do now is we're gonna simplify our life.
Okay. And we're gonna ignore all of this for now we're only gonna focus for now
on some sort of notion of text-match
which that's all we're going to focus on for the next week and on your homework. Okay?
Then we'll revisit, and we'll say, Well, how do we add all the other stuff?
Okay.
that's the plan. School. Take a breath. Let's take a breath
on the quick quiz that goes out today.
Ask you for your best estimate of the
of the score difference in the game tomorrow. Okay, the line is like 38. Okay? So he's expected. He is in my 38.
So one of the questions is, you're gonna put in your guess. So 2 things I want to do. So one you can say, well, I'm gonna put in something stupid like we're gonna buy a thousand.
I think I put a limit on. You can go to a thousand. But to encourage you to actually post a good score. If you
everybody nails the score is gonna get some kind of price
separately. I'm looking in the aggregate. I want to see if I look at the whole class's guesses. Oh, we didn't.
I don't know what the price is, the price could be a high 5. The prize you want is
this depends on how bold you are. Sometimes, I say, what's surprise? You're like automatic a drop. The class get 8. You go in small, small ball. I just want a little extra credit. Yeah, maybe it's a point or 2.
One extra point on behalf. So here's the problem I
we've now trans class into like a side hustle gambling concern. This is very problematic. I think
so, slipped my mind.
If you hadn't told me so, I would be, the guy would say, my class is full of like the most amazing gamblers. And so now, like, I'm gonna open up my bank account and bet next week on your guesses.
like they were all wrong. How did that happen?
Okay. okay, let's go look at some, some some people here.
So we're gonna talk about quickly in the last 10 min. This will help you on your homework
vector, space model. This is Jerry Salton. Quote, unquote father of Ir. Okay.
he was a professor. Cornell did a lot of foundational work in this area.
Here is a paper from 75. There were works in the 60 s. And earlier, for example, here, laying down some of the foundational ideas of this vector, space model, okay, you can trace some of the ideas even earlier. But I'm not going into the whole history of this stuff. But I want you to let you know that like this is stuff that was happening so long ago
and yet is still absolutely relevant and used as a component of all these modern systems. We built. Okay, even though it's some old thing.
Okay, yeah, this is in the Cornell back in the Seventys doing this work. Yeah, building on some of his own work and other people's work even before.
So this is a vector space model. This is like foundational model. It's the kind of thing when you have a job, and it's 10 years from now, and we're friends on Facebook. But I deleted my Facebook long time ago. But, like somehow, you get in touch with me.
I'm gonna ask you like, Hey, remember the vector, space model, you're gonna go cab. Honestly, I do.
And we're gonna talk about this right here. Okay? And to be honest, like, if I do do that call to you, hey, Kev, what's up? I'm CEO of my startup, and we just, you know. Blah, blah, I'm gonna say, yeah, what about the vector, space model, you know, like, don't remember it.
I'm gonna hunt you down. I'm gonna come, find you.
You're talking in front of your whole board. You're raising like millions of dollars. I'm saying you don't remember it, and we're going to go back, and we're going to talk about this topic
right here. Okay.
this is it. This is the big idea.
I took this figure from one of those original papers in the second, this is in the 70 S.
We're gonna treat documents and queries as vectors
in a high dimensional space
member vectors.
right?
What are the axes
that are good. So when we talk about like, so what I'm saying here is, I say, a document.
Wait a minute.
Right now. We're only doing the words, okay? So they said a document.
it's just gonna be a, vector so this is like Doc, one
and a query is a, vector and so we read, a vector is something like this, 1, 2, 0, one.
0, 0, 3, 4.
The query might be something like 1 1 0 0.
So if you just look at. These are 2 vectors. right? Those those are vectors
for those of you who've done any of this more like sophisticated language models and machine learning, what does this look like to you?
These vectors? Do you use these in your sophisticated?
So later on, we're going to talk about like, how do we write a query that looks something like this?
Does this look familiar to anybody
later on? These are going to be embeddings.
if you've heard about like dense, embedding, dense vectors. If you don't know what I'm talking about right now, don't worry. But the idea is this thing we were doing way back in the sixties and seventies
we're still doing today, only we call it embeddings. This is all Ml, and you know how to do this stuff, and you get paid a lot of money.
Okay.
so don't worry about that for right. Whoa. when you read it about it. don't worry about it right now.
So we have vectors. I said, document one is a vector. 1, 2, 0 1 0 0. So I'm asking, What are the axes? What is what is that? What is that? What does that one mean? There? What does that 0 mean? There.
let's go back and look at the figure.
How many dimensions are in this vector, space.
how many dimensions are in the same
one.
2, 3. So a 3 dimensional vector, space is saying.
you know, document one has 3 dimensions. So I'm just gonna write it down. Let's say 2, 1, 3
is making up those numbers up. In fact, let me write different numbers that don't confuse you.
it might be 0 point 1
0 point 5 0 point 3.
So when I say, what are the axes? So that's an axes. That's an axis. That's an axis. There's 3 of them. It's a 3 dimensional vector space.
Now, I'm asking you. what what is T. T. One. What is t 2. It's usually we think, XYZ. Now we're doing TT. We're in T's
factors. We're only doing text, though.
Right now.
what factory do we know from our text that starts with the team text.
This is the insight, this is it. These are terms.
these are terms. So watch this. What are the axes?
Terms. When I say terms. I mean terms in my
dictionary meaning. how big is my dictionary?
Remember our dictionary everywhere we've ever encountered. So how long is my, vector
it's the length of my dictionary, the length of my vocabulary. So let's say, we have a hundred K
terms in my vocab.
So what I'm saying is.
we have a hundred 1,000 dimensional vectors. Okay, but let's simplify it for just a moment. In the last 3 min. This is critical. This will help you on your homework. Okay.
you know this is very sparse. Let's assume our vocab as forwards only. so you can imagine the query is,
Hi! There.
and the document might be.
Hi, hi!
Go!
Maggie's document. 2 might be. go go aggies and maybe document 3 is something like
Hi, hi. 5.
So assuming we only have 4 words. When I say these are vectors.
you can say this is like high
whoops.
It's like saying this. It's like saying aggies.
I go
there.
So if I want to make these into vectors.
for now we can just count them up. So Hi! There has no aggies. 1 0 1.
Hi, there! You buy that. Let's do. Document one. What does it look like?
Yeah.
And each occurs once, high, high, go.
no, there, go go aggies
and High High High High.
So when we say we're going to do vector space model.
we're going to take all of our queries and documents, treat them vectors in a high-dimensional space.
How many dimensions do we have in this space? 4.
This is a 4 d. Space.
because there are 4
columns or 4 axes. In practice we have not a four-dimensional space we have, let's say, 100,000 dimensional space.
meaning this same query would be 0 1 0 1
0 0 0 0 0 0. It's like, I've seen these youtubers, they just they count.
I'm gonna say, 0 times now. And maybe people will watch my video, 0, 0, 0
100,000 times.
This document 1, 2, 1 0 and then another 9. The 9,000 zeros. Okay.
this is the vector space model. This does not do ranking yet. So the next thing we're gonna do when we come back on Wednesday.
we're gonna learn how to rate these documents. According to this query.
you know how like this.
you know, this is.
these are vectors, but we're gonna measure and then measure the angle. You know, that's called
very cool.
that's called the cosines. This measures the angle is that.
see, you guys, next week.