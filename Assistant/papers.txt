Large Language Models (LLMs) can generate biased and toxic responses. Yet
most prior work on LLM gender bias evaluation requires predefined gender-related
phrases or gender stereotypes, which are challenging to be comprehensively col-
lected and are limited to explicit bias evaluation. In addition, we believe that
instances devoid of gender-related language or explicit stereotypes in inputs can
still induce gender bias in LLMs. Thus, in this work, we propose a conditional
text generation mechanism without the need for predefined gender phrases and
stereotypes. This approach employs three types of inputs generated through three
distinct strategies to probe LLMs, aiming to show evidence of explicit and implicit
gender biases in LLMs. We also utilize explicit and implicit evaluation metrics to
evaluate gender bias in LLMs under different strategies. Our experiments demon-
strate that an increased model size does not consistently lead to enhanced fairness
and all tested LLMs exhibit explicit and/or implicit gender bias, even when explicit
gender stereotypes are absent in the inputs.
Large Language Models (LLMs) represent a revolutionary advancement and demonstrate remarkable
performance in many tasks [ 9,18]. LLMs like GPT-4  and LLaMA  are trained on vast
corpora of text data, enabling them to generate coherent and contextually relevant human-like text.
Nevertheless, stemming from the inherent gender biases present in both the training data and model
architecture, the generated outputs may present partiality or prejudice, potentially leading to adverse
effects such as the perpetuation of detrimental stereotypes, the reinforcement of disparities, and the
facilitation of the propagation of misinformation. Thus, it is essential to recognize and address these
biases for developing responsible and ethical LLMs.
In previous work, a language model is said to exhibit gender bias: 1) when the templated input
contains mentions of specific gender groups , the resulting generated
sentence shows a positive or negative inclination towards that gender [ 4,6,14,15]; 2) the model
assigns a higher probability to sentences with stereotypical combinations of gender groups and
attributes compared to other combinations  [1, 16].
The former method requires explicit gender mentions and the latter method necessitates predefined
gender stereotypes. However, comprehensively collecting and defining gender-related phrases and
those laden with gender stereotypes can be challenging, as such phrases are continually evolving and
changing. Moreover, we believe that even sentences that may seem devoid of bias can still exhibit
implicit bias within LLMs. For example, llama-7b continues the sentence “ My friend is talking
on the phone ” with “ and she looks really happy ”. “Talking on the phone ” is not an action typically
associated with gender stereotypes, but llama-7b assumes my friend to be female without context.
∗Equal Contribution
37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2311.00306v1  [cs.CL]  1 Nov 2023To address the above limitations, we propose a conditional text generation mechanism, which does
not require any predefined gender-related phrases or stereotypes and possesses the capability to
explore both explicit and implicit gender bias. Specifically, we use three distinct strategies to design
the probing inputs: 1) template-based inputs containing four widely acknowledged features that are
associated with gender stereotypes; 2) LLM-generated inputs that can potentially harbor underlying
gender bias inherent to the LLM; and 3) naturally-sourced inputs from naturally-sourced corpus like
STS-B , which comprises sentences related to human activities and can probe the implicit bias in
LLMs. Concretely, we prompt LLMs to extend the inputs from these strategies through conditional
text generation. Then the outputs of LLMs are utilized to evaluate the gender bias.
We observe that employing different probing strategies leads to different fairness performances, and a
larger model does not necessarily equate to increased fairness. Even if input sentences do not contain
explicit gender stereotypes, the model can still display gender bias in logits or generated text, which
undoubtedly has harmful societal impacts.
2 Probing and Bias Evaluation
generation, and present explicit andimplicit evaluation metrics to assess gender bias in LLMs.
2.1 Task Formulation
LetLbe a LLM, and Xbe the input that Lis conditioned upon for continuation genera-
tion. Our goal is to investigate biases through language generation conditioned on input sen-
tences x∈ X across different gender attributes. Specifically, we consider the pronouns of
the two-gender task as gender attributes, and we denote the set of the paired attribute words as
W={(wf
1, wm
1),···,(wf
N, wm
N)}, where wf
i∈ Wf={she, her, herself, . . . }is associated with
female andwm
i∈ Wm={he, his, himself, . . . }is associated with male fori∈ {1,2,···, N}.
WfandWmare bijections. In our work, we consider Las exhibiting bias when its generated texts
lead to an unequal social perception of wf
iandwm
ifori∈ {1,2,···, N}. We focus on distributional
disparities of gender attribute words in the collection of generated texts.
2.2 Probing Strategies
We consider exploring two types of gender bias: explicit bias , such as OCCUPATION , which has been
extensively studied and has predefined stereotype tokens with respect to the gender attributes (e.g.,
“nurse”) [ 3,5,7,8,12,19], and implicit bias , where biases are not easily perceptible. Thus, we explore
three different strategies for bias exploration: template-based, LLM-generated, and naturally-sourced
strategies.
Template-based. We leverage a straightforward template, denoted as “ subject verb object ”
which we populate with “ My friend ” in thesubject slot and predefined stereotype words corre-
sponding to target attributes in the object slot to create individual data sample. To expand the
scope of the existing target attribute OCCUPATION , we manually construct lists for PERSONALITIES ,
HOBBIES , and COLORS .2Subsequently, we replace the verb placeholder with “ is” for OCCUPATION
andPERSONALITIES , and with “ likes” for HOBBIES , and COLORS . For instance, when filling the
template with “ running ” from the target attribute HOBBIES , the resulting prompt is “ My friend likes
running ” which serves as the stimulus to prompt language generation.
LLM-generated. Similar to automated question generation in [ 11,13], we employ LLMs to auto-
matically generate explicitly unbiased statements. Specifically, we initiate this process by instruct-
inggpt-3.5-turbo with the following prompt: Generate a list of statements starting
with “My friend”. For example, [S] . The initial value of [S] is set to “My friend likes
blue” and it is subsequently substituted with randomly generated statements, iteratively re-prompting
the model. This process continues and we automatically remove duplicates and manually remove
statements exhibiting high textual overlap until we have 200 statements.
2Complete lists available in the Appendix 6.3.
2Naturally-sourced. In addition to our templated-based and LLM-generated inputs, we derive
sentences from the naturally-sourced corpus. Specifically, we adapt the STS-B data  and select
sentences from the test set that includes terms like “someone” ,“person” , etc., as templates to ensure
the sentences describe humans instead of animals or objects. Subsequently, we replace these terms
within the sentences with “My friend” , thus obtaining our gender-neutral naturally-sourced inputs to
probe LLMs. For example, if the original sentence is “A person is walking” , our adapted sentence
would be “My friend is walking” for experimental consistency.
2.3 Bias Evaluation Metrics Design
To evaluate the fairness of LLMs probed using these three strategies, we define two types of metrics:
explicit bias metrics – gender-attribute score, and implicit bias metrics – co-occurrence ratio and
Jensen–Shannon divergence score. We define explicit bias generation as the direct inclusion of gender
attribute words at the sentence level, which is also perceptible to human evaluators. Conversely,
implicit metrics assess bias from the model’s perspective, considering factors such as differences in
the logit distributions associated with attribute words wf∈ Wfandwm∈ Wm.
Gender-attribute score. For each generated sentence s∈S, we use the boolean value dk
s, where
k∈ {f, m, n, ns }, to represent different performances of the generated sentence by LLMs. When
there exists w∈ssuch that w∈ Wfandw /∈ Wm, we categorize sas leaning towards female
anddf
s= 1; otherwise, df
s= 0. Same for dm
s. For any w∈s, ifw /∈ Wfandw /∈ Wm, there
are two possible scenarios: one in which sis considered neutral ( dn
s= 1), and the other in which
sis nonsensical ( dns
s= 1). In instances where there exists w1∈sthatw1∈ Wfandw2∈sthat
w2∈ Wm, we let the human evaluators determine the bias direction. To quantify the overall bias
level of the entire generated sentences, we compute the sum of individual sentence scores, denoted as
Dk=P
s∈Sdk
swhere k∈ {f, m, n, ns }.
Co-occurrence ratio. For each attribute word in the female attribute word list wf
i∈ Wf, the
probability of wf
ito be the next generated token is computed given the same model input [I;x],
where Iis the instruction and x∈ X is the input data sample. The co-occurrence ratio for female
attributes is calculated as:
Rf=1
|X|X
x∈X(P
i∈{1,···,N}p(wf
i|[I;x])
P
i∈{1,···,N}p(wf
i|[I;x]) +P
i∈{1,···,N}p(wm
i|[I;x])), (1)
and likewise for male attributes:
Rm=1
|X|X
x∈X(P
i∈{1,···,N}p(wm
i|[I;x])
P
i∈{1,···,N}p(wf
i|[I;x]) +P
i∈{1,···,N}p(wm
i|[I;x])). (2)
Jensen–Shannon divergence (JSD) score. To measure distance between distributions, we calculate
JSD score. Specifically, in the binary-gender task of our work, JSD quantifies the alignment between
thefemale attribute word distributions Pfandmale attribute word distributions Pm, defined as
DJS(Pf||Pm) =1
2DKL(Pf||Pa) +1
2DKL
where DKLis the Kullback–Leibler divergence between two distributions and Pa= (Pf+Pm)/2
is a mixture distribution of PfandPm.
3 Experimental Settings
We utilize six versions of LLaMA: llama-7b ,llama-7b-chat ,llama-13b ,llama-13b-chat ,
llama-70b , andllama-70b-chat . In our experiments, the input for LLMs is a combination of
an instruction Iand a sample x∈ X:[I;x], where [; ]denotes the concatenation operation and our
specific instruction Iis “Complete the sentence ”. We configure the LLMs to generate 50 new
tokens, and all experiments are conducted on NVIDIA RTX A5000 24GB GPUs.
37b7b-chat 13b13b-chat 70b70b-chat0255075100125150female words count
male words count
neutral
non-sense(a) Template-based inputs.
7b7b-chat 13b13b-chat 70b70b-chat0255075100125150175200
female words count
male words count
neutral
non-sense (b) LLM-generated inputs.
7b7b-chat 13b13b-chat 70b70b-chat0255075100125150175 female words count
male words count
neutral
non-sense (c) Naturally-sourced inputs.
Figure 1: Gender-attribute score : each bar is a ratio of the number of responses with female attribute
word count, responses with male attribute word count, neutral responses, and non-sense responses.
7b7b-chat 13b13b-chat 70b70b-chat0.00.20.40.60.81.0
0.280.61
0.440.56
0.350.680.720.39
0.560.44
0.650.32female words logits ratio
male words logits ratio
(a) Template-based inputs.
7b7b-chat 13b13b-chat 70b70b-chat0.00.20.40.60.81.0
0.310.67
0.340.54
0.360.60.690.33
0.660.46
0.640.4female words logits ratio
male words logits ratio (b) LLM-generated inputs.
7b7b-chat 13b13b-chat 70b70b-chat0.00.20.40.60.81.0
0.230.45
0.260.3 0.280.610.770.55
0.740.7 0.720.39female words logits ratio
male words logits ratio (c) Naturally-sourced inputs.
Figure 2: Co-occurrence ratio : each bar in each chart is a ratio of the total logits of female attribute
words and the total logits of male attribute words.
4 Experimental Results and Analysis
We conduct comprehensive experiments using these three strategies to probe LLMs and utilize
gender-attribute score, co-occurrence ratio, and JSD score to evaluate explicit and implicit biases.
4.1 Gender-Attribute Score
Fig. 1 shows the gender-attribute scores of output generated using our three strategies, as acquired
from six different versions of LLaMA. According to the visualization, probed by LLM-generated
inputs, all LLaMAs show gender bias in varying degrees. This indicates that the potential gender
bias in LLaMA is reflected in the LLM-generated inputs. Inputs from template-based and naturally
sourced strategies embrace similar levels of gender bias, which reveals that sentences that appear to be
free of gender bias may still exhibit gender bias similar to gender stereotypes in LLMs. Besides, for
the naturally-sourced inputs, most model versions generate responses with male attribute words more
than responses with female attribute words, revealing that the naturally-sourced inputs contain more
gender bias leaning toward males. It can be observed from the comparison between llama-70b-chat
andllama-13b that larger models do not necessarily result in fairer models. On the other side, larger
7b7b-chat 13b13b-chat 70b70b-chat0.000.010.020.030.04JSD score
(a) Template-based inputs.
7b7b-chat 13b13b-chat 70b70b-chat0.000.010.020.030.040.050.060.07JSD score (b) LLM-generated inputs.
7b7b-chat 13b13b-chat 70b70b-chat0.000.010.010.010.020.030.03
JSD score (c) Naturally-sourced inputs.
Figure 3: JSD score : each bar in each chart is JSD score of gender-pair attribute word distributions.
4My friend studies a 
calendar, and  she 
plans a trip for the 
summer.0.913 (F)
0.087 (M)
0.032 (J)My friend is on a 
baseball team, and  
he practices every 
day to improve his 
swing.0.005 (F)
0.995 (M)
0.167 (J)
My friend is playing 
a harp, and  the 
beautiful melodies 
are filling the room.0.984 (F)
0.016 (M)
0.018 (J)My friend tries to 
feed a mouse to a 
snake, and  the 
snake refuses to 
eat it, instead it 
slithers away, 
leaving my friend 
confused and 
frustrated.0.040 (F)
0.960 (M)
0.001 (J)
My friend is driving 
a car, and  I'm 
feeling a bit 
nervous because 
I'm not sure if 
they're paying 
attention to the 
road.0.543 (F)
0.457 (M)
0.000 (J)My friend is holding 
a leaf, and  I can't 
help but feel a 
sense of wonder at 
the beauty of 
nature. The leaf is 
a vibrant green, 
with intricate veins 
that spread out like 
a network of tiny 
roads.0.488 (F)
0.512 (M)
0.000 (J)
Figure 4: Six examples from the outputs probed by naturally-sourced inputs. For each example, the
first column is the input sentence and the generated text by llama-70-chat . The second column is
the sum ratio of all female attribute words, the sum ratio of all male attribute words, and the JSD
score of the example. The third column is the logits distribution of female/male attribute words.
models do have better generation performance since llama-30b ,llama-7b , andllama-7b-chat
generate more non-sense responses than other model versions.
4.2 Co-occurrence Ratio
Fig. 2 displays the co-occurrence ratio and the probabilities of paired gender attribute words like
are supposed to be similar without gender context. However, for all three types of inputs,
most model versions can not obtain similar gender-pair attribute word probabilities. This means
LLMs like LLaMA can exhibit gender bias in generated sentences even if the inputs are in the absence
of gender information. Although it may not always manifest in the generated text, we can observe
this phenomenon from the logits and the co-occurrence ratio.
4.3 JS Divergence Score
We visualize JSD score of three types of outputs in Fig. 3. llama-70b-chat has the highest JSD
score probed by the template-based inputs and the LLM-generated inputs, while it has a relatively low
JSD score on the naturally-sourced inputs. llama-7b-chat has relatively high JSD scores probed
by all three types of inputs, while llama-7b has relatively low JSD scores. Fig. 3 shows that the size
of the models and JSD scores do not have a constant relationship.
The JSD score distributions of gender-pair attribute words obtained by llama-70b-chat on four
separate features in template-based inputs are shown in Appendix 6.2. Out of the 40 colors, pink is the
most biased color, which aligns with our stereotypical impressions of colors. Sewing ,woodworking
andquilting are the most biased hobbies. It is interesting to note that outdoor activities like hiking ,
5kayaking , and fishing are not the most biased ones, even though they have been traditionally associated
with masculinity. Mechanic ,mover ,construction worker ,carpenter , and nurse exhibit the highest
degree of gender bias. Elegant andgraceful demonstrates the utmost level of bias among personalities.
4.4 Case Study
In order to conduct a more in-depth analysis of the experimental results, we conduct a case study
on the generated sentences conditioned on naturally sourced inputs. We select four representative
examples and visualize the corresponding logits of a word in gender attribute words being the next
token generated by llama-70b-chat given the example in Fig. 4.
In the two examples shown in the first row, the model exhibits significant gender bias reflected in
both logits and the generated text. “baseball” is considered as a gender stereotype associated with
males, while “studies a calendar” contains no gender stereotypes, showing that even without explicit
gender stereotype information, the model can still generate biased information. In the two examples
displayed in the second row, gender bias is reflected in logits, but it is not contained in the generated
texts. “Feed a mouse to a snake” may indeed be considered a gender stereotype related to males, but
this complex behavior is difficult to predefine with a list of stereotypes. In the two examples shown
in the third row, gender bias is neither reflected in logits nor in the generated texts. The left example
uses “they” to refer to “my friend” , which is reasonable when gender information is not available.
The right example does not continue describing “my friend” , avoiding the use of pronouns.
These examples illustrate the input sentences corresponding to different outcomes. Regardless of
whether there are explicit or predefined gender stereotypes present in the inputs, the model may still
convey gender bias either in logits or in the generated text, which undoubtedly brings about negative
societal impacts. Therefore, detecting and mitigating gender bias in LLMs is of utmost importance.
5 Conclusion
explicit and implicit biases in LLMs. We use three distinct strategies: template-based, LLM-generated,
and naturally-sourced strategies, to probe the LLMs and design explicit and implicit metrics. Our
experiments reveal that a model with a larger size does not necessarily equate to greater fairness, and
despite the absence of explicit gender stereotypes in inputs, LLMs can exhibit gender bias in logits
or generated text, which unquestionably has adverse societal consequences. These findings provide
valuable insights for the development of effective debiasing methods in future studies.11Question generation is a widely used data aug-
mentation approach with extensive applica-
tions, and extracting qualified candidate an-
swers from context passages is a critical step
for most question generation systems. How-
ever, existing methods for candidate answer
extraction are reliant on linguistic rules or an-
notated data that face the partial annotation is-
sue and challenges in generalization. To over-
come these limitations, we propose a novel
unsupervised candidate answer extraction ap-
proach that leverages the inherent structure
of context passages through a Differentiable
Masker- Reconstructor ( DMR ) Model with the
enforcement of self-consistency for picking up
salient information tokens. We curated two
datasets with exhaustively-annotated answers
and benchmark a comprehensive set of super-
vised and unsupervised candidate answer ex-
traction methods. We demonstrate the effec-
tiveness of the DMR model by showing its
performance is superior among unsupervised
methods and comparable to supervised meth-
ods. Our code and data are publicly available
at 
Question Generation (QG) is a burgeoning field
of Natural Language Understanding and Genera-
tion. The objective of Question Generation is to
produce well-structured, coherent, and valuable
questions that correspond to a specific context pas-
sage and the intended answer. QG systems play
a vital upstream role in enhancing the robustness
and generalizability of Question Answering (QA)
and Machine Reading Comprehension (MRC) mod-
els , empower-
ing chatbots and virtual assistants to answer more
user needs , and powering
AI-driven tutoring systems for educational pur-
poses . For most existing QG
systems, extracting qualified candidate answersfrom the context passage is an indispensable pre-
requisite to ensure that the generated questions are
of high quality and relevant to the user’s interests
of salient information contained in the context pas-
sage.
Traditional methods for answer extraction rely
on linguistic rules and models to discover the syn-
tactic structure of the input passage’s sentences.
Constituency tags of Noun Phrases and Named
Entity Recognition (NER) tags of person, time,
location, etc., are popular choices of candidate an-
swers. However, this kind of answer extraction
method can only extract limited types of answers
and usually disregards the importance of different
possible answer tokens on the context and domain
basis. More recently, with the development of ma-
chine learning and large language models, candi-
date answer extraction has been formalized as a
sequence labeling problem and tackled with su-
pervised sequence learning and classification with
the use of annotated answers from MRC datasets
like SQuAD . Nevertheless,
the answer annotations of existing MRC and QA
datasets have the partial annotation issue due to the
annotation protocols that did not enforce exhaus-
tive answer extraction from the context passage,
leading to the overlooking of other essential details
that could be beneficial in helping readers grasp the
context . Training models with
partially annotated data could result in degraded
performance as the data provide misleading super-
vision signal .
Furthermore, the supervised learning methods still
face the generalization challenge when applying
them to new domains where acquiring additional
human annotations is time-consuming and expen-
sive.
To overcome the aforementioned limitations and
challenges, we propose a novel unsupervised can-
didate answer extraction approach that leverages
the inherent structure of context passage. We positarXiv:2310.13106v1  [cs.CL]  19 Oct 2023that passage tokens can be categorized into two
types – backbone tokens and information tokens.
Backbone tokens are structural and common across
various passages within the same domain, and such
tokens are easily recoverable when masked. In
contrast, information tokens are difficult to recover
when masked, and such tokens are crucial infor-
mation of a specific context passage, making them
excellent candidate answers. In Section 3, we in-
troduce the Differentiable Masker-Reconstructor
model, in which the masker module learns to mask
out tokens that the reconstructor module can then
readily recover through the enforcement of self-
consistency. For comprehensive assessments, we
exhaustively annotated a total of 100 passages with
candidate answers on SQuAD and WikiHow cook-
ing texts as detailed in Section 4.1. We demonstrate
the competitive performance of the DMR model in
Section 5 by comparing it with a comprehensive list
of strong baselines that cover recent advancements
of both supervised and unsupervised candidate an-
swer extraction.
In summary, we make the following contribu-
tions:
•We propose a novel Differentiable Masker-
Reconstructor model, in light of recent
progress on self-consistency learning and
masked language models, for unsupervised
candidate answer extraction.
•We release two newly created datasets with
exhaustively-annotated candidate answers.
•We benchmark a comprehensive list of super-
vised and unsupervised candidate answer ex-
traction methods and show the strong perfor-
mance of our DMR model.
2 Related Work
Candidate Answer Extraction focuses on the ex-
traction of salient information tokens that usually
contain key information and knowledge the readers
may seek from the context passage. In previous
work, Named Entities are the most popular type of
candidate answers (Yang et al., 2017; Lewis et al.,
2019; Fabbri et al., 2020; Nie et al., 2022). In such
cases, tokens of the context passage are first pro-
cessed by existing NER models that tag tokens of
person, organization, location, date/time, and nu-
merical expressions. Noun Phrase (NP) is another
popular choice that has been used a lot . Among
them, Yang et al. and Nie et al. also consider
more diverse types of phrases including Adjective
Phrases , and sub-clauses
(S) extracted from the consistency parsing models
of the context passage. Specially, Nie et al. fur-
ther expand recognized named entities into longer
constituents for more diverse candidate answers.
Despite the reliance on NER and parsing models,
these linguistic and syntactic rules based candidate
answer extraction methods fail to take the impor-
tance of the information into account. With the
advancements of supervised learning methods for
neural networks, multiple studies (Du and Cardie,
2017; Subramanian et al., 2018; Wang et al., 2019)
utilize the annotated answer phrases of SQuAD, a
MRC dataset, to train neural models that are capa-
ble of tagging and classifying candidate answer to-
kens. However, Bao et al. point out that the answer
annotations of existing MRC and QA datasets have
the partial annotation issue due to the annotation
protocols that did not enforce exhaustive answer
extraction from the context passage, leading to the
overlooking of other essential details that could
be beneficial in helping readers grasp the context.
Training models with partially annotated data could
result in degraded performance as the data provide
misleading supervision signal. To address the par-
tial annotation problem, previous work resorts to
Positive-Unlabeled (PU) learning ( ?), which uses
modified risk estimators to re-balance the weights
of positively labeled answer tokens and the remain-
ing deemed-as-unlabeled tokens for unbiased learn-
ing of an unbiased binary classifier. Nevertheless,
as a common problem across different tasks, the su-
pervised learning methods still face the generaliza-
tion challenge when applying them to new domains
where acquiring additional human annotations is
time-consuming and expensive.
Self-consistency Learning has been adopted in
the field of Natural Language Processing in recent
years for tasks like text encoding ,
sentence compression and summarization (Baziotis
et al., 2019; Malireddy et al., 2020), NER (Iovine
et al., 2022), and data-to-text generation (Wang
et al., 2023). Models that leverage self-consistency
learning usually include two modules that are re-
versals of each other. Specifically, one compressor
module takes a sentence, paragraph, or document
as the initial input and compresses the text into
intermediate outputs of dense encoding, abstrac-Article Title Content of the step Heat the Olive Oil
How to Make
Caldo TlalpenoAdd 2 tablespoons (30 ml) of olive oil to a large pot. Allow the oil to heat for 2 to 3 minutes
on medium-high heat, or until it begins to shimmer. You can substitute vegetable or canola
oil for the olive oil if you prefer.
How to Make
Slow Cooker Spaghetti SaucePlace alarge skillet on the stove, and add 2 tablespoons (30 ml) of olive oil to it. Turn the
burner to medium, and allow the oil to heat for 3 to 5 minutes or until it starts to shimmer.
How to Make
Shrimp BisqueAdd 3 tablespoons (45 ml) of olive oil to a large pot or Dutch oven, and place it on the stove.
Turn the heat to medium, and allow the oil to heat for 5 minutes, or until it starts to shimmer.
If you prefer, you can substitute butter for the olive oil.
How to Make
an Omelette in a JarPlace a large skillet on the stove, and add 1 tablespoon (15 ml) of olive oil. Turn the heat
to medium-high, and allow the oil to heat until it starts to shimmer, which should take
approximately 5 minutes. You can substitute butter for the olive oil if you prefer.
How to Make
a White PizzaAdd 2 tablespoons (30 ml) of olive oil to a medium, heavy-bottomed saucepan. Place the
pan on the stove, and heat it over medium heat until it begins to shimmer, which should take
approximately 5 minutes. You can substitute vegetable oil for the olive oil.
Table 1: Content of multiple WikiHow articles with different expressions of the step Heat the Olive Oil
tive text, or structured data that preserve the key
information of the original input text. Sequentially,
another reversal module attempts to reconstruct
the initial input based on the first module’s inter-
mediate condensed outputs. The two modules are
progressively trained by enforcing the consistency
between the initial input and reconstructed input.
Notably, the differentiability of the intermediate
outputs greatly affects the back-propagation of the
learning signals from the reversal module to the
compressor module. Except the application of self-
consistency learning for text encoding, most other
applications including ours have the challenge of
handling non-differentiable discrete intermediate
outputs. To deal with this issue, Iovine et al. and
Wang et al. adopt cycle training that alternatively
changes the roles of the two modules, and Baziotis
et al. draw support from Gumbel-Softmax with
the strike-through approach for differentiable sam-
pling from the categorical distribution. However,
the ablation study conducted by Baziotis et al. sug-
gests that the generation of a relevant and fluent
summary was mainly driven by a topic loss.
Masked Language Modeling (MLM) is a fun-
damental technique for the pretraining of Large
Language Models (LLM), first popularized by the
BERT model . During the
pretraining stage, some percentage of the input
tokens are masked at random, and the model’s
objective is to predict the original tokens based
on their context tokens. This is different from
traditional language models that typically predict
the next word in a sequence, enabling models to
bidirectionally understand the context and achieve
strong language understanding capability. Follow-
ing BERT, RoBERTa  also usesMLM as its pretraining objective. It improves upon
BERT by using dynamic masking that changes the
masking pattern applied to the input text rather than
static masking that always masking the same words,
which results in a more robust LLM. Along this
vein, MLM becomes one of the most popular pre-
training objectives for large language models, in-
cluding ALBERT , BART (Lewis
et al., 2020), Longformer , etc.
that optimized BERT from different aspects.
3 Approach
Our approach is based on the intuition and obser-
vation that the text passages within the same do-
main have inherent structure shaped by the under-
lying writing style, knowledge space, and expres-
sion formulation. We convey these concepts by
showing a typical example we sourced from the
WikiHow1cooking articles. WikiHow is a popular
online resource that offers user-curated, step-by-
step guides on how to perform certain tasks. As
shown in Table 1, multiple WikiHow articles on
the cooking of five different dishes have the same
step Heat the Olive Oil . We can observe explicit
templates, like Add AMOUNT of olive oil to
a CONTAINER andPlace a CONTAINER on the
stove, and add AMOUNT of olive oil , from
these passages. For the step of Heat the Olive Oil ,
it is easier for the Masked Language Models to fill
the masks of [MASK] 3 tablespoons (45 ml)
[MASK][MASK][MASK][MASK][MASK] large pot
or Dutch oven with correct tokens than filling the
masks of Add [MASK][MASK][MASK] of olive
oil to a [MASK][MASK][MASK][MASK][MASK] .
1 the oil to heat for 2 to 3 minutes  on medium  heat 
                                Allow the oil  [M] [M] [M] 2 to 3 minutes  [M] [M] [M]
Masker Module 
Reconstructor Module 
Allow the oil to heat for 2 to 3 minutes  on high heat    Masked Passage Original Passage 
Reconstructed 
Passage Reconstruction Loss Length Penalty Figure 1: The Masker-Reconstructor Model ( [M]represents the special token [MASK] ).Reconstruction loss guides
the learning of the Reconstructor module as well as penalizes the Masker module for masking out hard-to-recover
tokens (illustrated by [M]andhigh in red). Length penalty enforces the learning of the Masker module so that those
easy-to-recover tokens (illustrated by Allow the oil in red) are more likely to be masked out as masking them
would yield the gain of both length and reconstruction loss.
The reason is that the black tokens are repeatedly
seen in the domain and result in statistically higher
prediction probability when the colored tokens are
given as the context for the MLM. On the other
hand, the colored tokens are precise and specific
information of a particular passage that do not have
the prediction probability as high as the black to-
kens. Therefore, tokens of the text passages can be
categorized into the following two types:
Backbone Tokens are those black tokens of the
content shown in Table 1, are structural and com-
mon across various passages within the same do-
main, and such tokens are easily recoverable when
masked.
Information Tokens , in contrast to Backbone To-
kens, are difficult to recover when masked, and
such tokens are crucial information of a specific
context passage, making them excellent candidate
answers. Examples of Information Tokens are col-
ored tokens in Table 1 that express the AMOUNT,
CONTAINER, TIME, HEAT-LEVEL, SUBSTITUTION
of the specific context. Besides the cooking do-
main, Wikipedia articles regarding a person may
have similar structure that has date of birth, place
of birth, occupation, education, etc. as informa-
tion tokens. News articles may express who, what,
where, when of events as information tokens with
a shared explicit or implicit template.
3.1 The Masker-Reconstructor Model
To extract candidate answers without accessing
ground truth labels and upstream constituency
parser or named entity recognizer, we develop the
Masker-Reconstructor Model to discriminate back-bone tokens and information tokens based on the
idea of self-consistency learning as mentioned in
Section 2. As illustrated in Figure 1, the Masker-
Reconstructor Model consists of a Masker module
and a Reconstructor module. The Masker mod-
ule is a token classification model CMpowered by
LLM. It takes the original context passage Pas in-
put and makes a binary classification for each token
with 1 indicating the token should be preserved and
0 indicating the token should be masked. Based on
the classification results of CMand the original pas-
sageP, the Masker module outputs an intermediate
passage ˆPwith masked tokens represented by the
special token [MASK] . Sequentially, the Reconstruc-
tor module, a Mask-Filling model CRpowered by
LLM as well, takes the intermediate masked pas-
sage ˆPas input and predicts the conditional proba-
bility of possible original surface tokens for each
[MASK] . Relate to other works in self-consistency
learning, in our work, the Masker module and the
Reconstructor module are reversals of each other,
and the two modules can therefore be progressively
trained by enforcing the self-consistency between
the initial input and reconstructed input. Specif-
ically, the reconstructed passage P′is compared
with the original passage Pfor the calculation of
the reconstruction loss as follows:
L =−1
|P||P|X
i=1[1−CM(ˆPi|P)]
·[Prob(Pi)·log(CR(P′
i|ˆP))]
The reconstruction loss not only guides the learn-
ing of the Reconstructor module but also ensuresthat the Masker module is getting more penalty if
it masked out tokens that are harder for the Recon-
structor module to recover. Also, this learning pro-
cedure is solely dependent on the self-consistency
between the original input passage and the asso-
ciated reconstructed passage. However, a short
cut for the Masker module to achieve low recon-
struction loss is to classify all the tokens as 1 that
preserves all the tokens, which betrays our goal of
finding those hard-to-recover information tokens
for the candidate answer extraction purpose. To en-
force the learning of the Masker module, a length
penalty is needed in conjunction with the recon-
struction loss. In our work, the length loss is calcu-
lated as:
Llength =1
|P||P|X
i=1CM(ˆPi|P)
Therefore, the final loss of the Masker module is:
LCM=L +λ·Llength
where λis a weighting factor that balances the two
loss terms. Higher λenforces the model to mask
out more tokens.
3.2 Differentiable Self-consistency Learning
An unaddressed challenge at this point is the non-
differentiability of the intermediate outputs as the
Masker’s binary classification is a discrete step,
which obstructs the backpropagation of the main
training signal, reconstruction loss, to the Masker
module. To achieve differentiable self-consistency
learning, our masker-reconstructor model em-
ploys the Straight-Through Gumbel-Softmax esti-
mator.
Gumbel-Softmax, a re-
stochasticity into the model by generating differen-
tiable approximations of discrete random variables.
This permits the masker-reconstructor model to
sample different tokens in a differentiable manner.
However, standard Gumbel-Softmax does not allow
for hard decisions, i.e., decisions with binary val-
ues ,
which are desirable in our case. Therefore, we use
theStraight-Through Gumbel-Softmax , an estima-
tor that offers an elegant way of approximating hard
decisions while keeping the process differentiable.
During the forward pass, the Straight-Through
Gumbel-Softmax uses the Gumbel-Softmax func-
tion with temperature approaching zero, yieldingone-hot (hard) vectors. Meanwhile, in the back-
ward pass, it can utilize the gradient calculated
from the continuous approximation of the standard
Gumbel-Softmax distribution.
Through this differentiable self-consistency en-
forced learning paradigm, we demonstrate in Sec-
tion 5 that the proposed DMR model is capable of
picking up information tokens that are excellent
candidate answers in an unsupervised manner.
4 Experimental Setup
4.1 Data
A prevailing challenge of the candidate answer ex-
traction task is the absence of a specific dataset for
comprehensive evaluation and training. Although
the answer annotations from MRC and QA datasets
have been ubiquitously adapted for the task of can-
didate answer extraction, researchers have pointed
out that these data have the partial annotation prob-
lem for the candidate answer extraction task (Bao
et al., 2022). As the annotation protocols used
for the construction of MRC and QA datasets did
not require their annotators to find an exhaustive
list of candidate answers and come up with associ-
ated questions, using data with missing annotations
could provide wrong supervision signals to the can-
didate answer extraction models, and evaluating
with this kind of data could lead to wrong conclu-
sion. Bao et al.’s analysis found that 48.89% and
62.44% of candidate answers are missing from the
SQuAD dataset and the DROP dataset (Dua et al.,
2019) respectively. Our answer/context ratio analy-
sis (available in Appendix A) on existing MRC and
QA datasets also suggests that their annotated an-
swers only cover a very small portion, with 13.35%
being the highest and as low as 0.26% of the infor-
mation contained in the context passages.
In consideration of the training and evaluation
challenge, we prepared and curated the following
three datasets to facilitate the assessment of our
work:
Original (Partially-annotated) SQuAD : SQuAD
is a large scale MRC and QA dataset with 97,095
pairs of question-answer pairs corresponding to
20,947 context passages. It is also the most fre-
quently used dataset for candidate answer extrac-
tion in previous work. Despite the partial annoation
issue, SQuAD has the highest answer/context ra-
tio of 13.55% so far, and its partially annotated
answers can still provide valuable supervision and
bring some generalizability through its large scale.Dataset SQuAD WH-C
Source Wikipedia WikiHow
Domain Open-domain Cooking
Context Passage Amount 20,947 16,642
Average Passage Length 135 Tokens 66 Tokens
Answer/Context Ratio
(Original)13.35% N/A
Answer/Context Ratio
(Exhaustively-annotated)35.11% 50.51%
Table 2: Dataset Statistics
We use the original SQuAD dataset mainly for the
purpose of training and comparing with supervised
methods, and we also report the performance of
different candidate answer extraction methods onTable 5: Statistics and comparison of MRC/QA datasets.Pre-trained Language Models are widely used
in many important real-world applications.
However, recent studies show that these models
can encode social biases from large pre-training
corpora and even amplify biases in downstream
applications. To address this challenge, we pro-
pose Co2PT, an efficient and effective debias-
while-prompt tuning method for mitigating bi-
ases via counterfactual contrastive prompt tun-
ing on downstream tasks. Our experiments
conducted on three extrinsic bias benchmarks
demonstrate the effectiveness of Co2PT on bias
mitigation during the prompt tuning process
and its adaptability to existing upstream debi-
ased language models. These findings indicate
the strength of Co2PT and provide promising
avenues for further enhancement in bias miti-
gation on downstream tasks.
Pre-trained language models (PLMs) are widely
used in many real-world applications, demonstrat-
ing remarkable performance (Devlin et al., 2019;
Brown et al., 2020). However, it has been demon-
strated that PLMs encode unfair social biases in
their parameters based on their pre-training step
over large-scale text corpora .
Furthermore, these biases – for example, based on
gender, race, or religion – can easily propagate to
the downstream tasks that use these PLMs (Kaneko
and Bollegala, 2021). For example, “She is a nurse”
can have a higher conditional likelihood than “He
is a nurse” in the language modeling task, andmodel performance along epochs.Sequential models are designed to learn sequential patterns in data
based on the chronological order of user interactions. However,
they often ignore the timestamps of these interactions. Incorpo-
rating time is crucial because many sequential patterns are time-
dependent, and the model cannot make time-aware recommen-
dations without considering time. This article demonstrates that
providing a rich representation of time can significantly improve
the performance of sequential models. The existing literature treats
time as a one-dimensional time-series obtained by quantizing time.
In this study, we propose treating time as a multi-dimensional time-
series and explore representation learning methods, including a ker-
nel based method and an embedding-based algorithm. Experiments
on multiple datasets show that the inclusion of time significantly
enhances the model’s performance, and multi-dimensional methods
outperform the one-dimensional method by a substantial margin.
KEYWORDS
Sequential models, Time-aware recommendation, Time-series790As powerful tools for representation learning on graphs, graph neu-
ral networks (GNNs) have played an important role in applications
including social networks, recommendation systems, and online
web services. However, GNNs have been shown to be vulnerable to
adversarial attacks, which can significantly degrade their effective-
ness. Recent state-of-the-art approaches in adversarial attacks rely
on gradient-based meta-learning to selectively perturb a single edge
with the highest attack score until they reach the budget constraint.
While effective in identifying vulnerable links, these methods are
plagued by high computational costs. By leveraging continuous
relaxation and parameterization of the graph structure, we propose
a novel attack method called Differentiable Graph Attack (DGA)
to efficiently generate effective attacks and meanwhile eliminate
the need for costly retraining. Compared to the state-of-the-art,
DGA achieves nearly equivalent attack performance with 6 times
less training time and 11 times smaller GPU memory footprint on
different benchmark datasets. Additionally, we provide extensive
experimental analyses of the transferability of the DGA among dif-
ferent graph models, as well as its robustness against widely-used
defense mechanisms.
KEYWORDS
graph neural networks; adversarial attack; gray-box attackPolblogs (lower line) datasets. Note that the x-axis of the node degree distribution plot is scaled for better visualization.Industry recommender systems usually suffer from highly-skewed
long-tail item distributions where a small fraction of the items
receives most of the user feedback. This skew hurts recommender
quality especially for the item slices without much user feedback.
While there have been many research advances made in academia,
deploying these methods in production is very difficult and very
few improvements have been made in industry. One challenge is
that these methods often hurt overall performance; additionally,
they could be complex and expensive to train and serve.
In this work, we aim to improve tail item recommendations
while maintaining the overall performance with less training andtion Benefit of Model Invariance from a Data Perspective. In NeurIPS .A key component of modern conversational
systems is the Dialogue State Tracker (or DST),
which models a user’s goals and needs. Toward
building more robust and reliable DSTs, we
to automatically generate effective adversar-
ial examples to probe DST models. Two key
characteristics of this approach are: (i) it only
needs the output of the DST with no need for
model parameters, and (ii) it can learn to gener-
ate natural language utterances that can target
any DST. Through experiments over state-of-
the-art DSTs, the proposed framework leads
to the greatest reduction in accuracy and the
best attack success rate while maintaining good
fluency and a low perturbation ratio. We also
show how much the generated adversarial ex-
amples can bolster a DST through adversarial
training. These results indicate the strength of
prompt-based attacks on DSTs and leave open
avenues for continued refinement.
Task-oriented dialogue systems aim to help users
with tasks through a natural language conversa-
tion. Example tasks include booking a hotel or
completing a do-it-yourself project. A key com-
ponent for enabling a high-quality task-oriented
dialogue system is the Dialogue State Tracker
(or DST) which plays an important role in under-
standing users’ goals and needs (Wu et al., 2019;
Hosseini-Asl et al., 2020; Li et al., 2021b; Dai
et al., 2021; Feng et al., 2021; Zhao et al., 2021;
Balaraman et al., 2021). For example in Figure 1a,
given the user utterance “I am looking for a cheap
restaurant in the center of the city”, the DST ex-results are from original papers.In real-world scenarios, most platforms collect both large-scale, nat-
urally noisy implicit feedback and small-scale yet highly relevant
explicit feedback. Due to the issue of data sparsity, implicit feedback
is often the default choice for training recommender systems (RS),
however, such data could be very noisy due to the randomness
and diversity of user behaviors. For instance, a large portion ofLearning. In 5th ICLR . Subjective bias is ubiquitous on news sites,
social media, and knowledge resources like
Wikipedia. Many existing methods for subjec-
tive bias correction have typically focused on
making one-word edits and have been trained
over a single  domain. In con-
trast, we propose a novel reinforced sequence
training approach for robust subjective bias cor-
rection. Three of the unique characteristics of
the approach are: (i) it balances bias neutraliza-
tion with fluency and semantics preservation
through reinforcement learning, to broaden the
scope to bias beyond a single word; (ii) it is
cross-trained over multiple sources of bias to
be more robust to new styles of biased writing
that are not seen in the training data for a single
domain; and (iii) it is used to fine-tune a large
pre-trained transformer model to yield state-of-
the-art performance in bias text correction task.
Extensive experiments show that the proposed
approach results in significant improvements in
subjective bias correction versus alternatives.
Objective writing is essential for many important
communication venues like news, encyclopedias,
scientific publications, and more. And yet, bias
is seemingly ubiquitous whether due to malice or
unintentional habits of the writer. This subjectiveTable 8: Sample model outputs2598 Recent work in news recommendation has demonstrated that
recommenders can over-expose users to articles that support their pre-
existing opinions. However, most existing work focuses on a static setting
or over a short-time window, leaving open questions about the long-term
and dynamic impacts of news recommendations. In this paper, we ex-
plore these dynamic impacts through a systematic study of three re-
search questions: 1) How do the news reading behaviors of users change
after repeated long-term interactions with recommenders? 2) How do theence on recommender systems. pp. 154–162 (2018)Conversational recommenders are emerging as a powerful tool
to personalize a user’s recommendation experience. Through a
back-and-forth dialogue, users can quickly hone in on just the right
items. Many approaches to conversational recommendation, how-in Information Retrieval . 749–758.Conversational recommender systems (CRS) have shown great suc-Recommender Systems. In SIGIR .Question Generation (QG) is a fundamental
NLP task for many downstream applications.
Recent studies on open-book QG, where sup-
portive answer-context pairs are provided to
models, have achieved promising progress.
However, generating natural questions under a
more practical closed-book setting that lacks
these supporting documents still remains a
challenge. In this work, we propose a new
QG model for this closed-book setting that
is designed to better understand the seman-
tics of long-form abstractive answers and store
more information in its parameters through
contrastive learning and an answer reconstruc-
tion module. Through experiments, we vali-
date the proposed QG model on both public
datasets and a new WikiCQA dataset. Empir-
ical results show that the proposed QG model
outperforms baselines in both automatic eval-
uation and human evaluation. In addition, we
show how to leverage the proposed model to
improve existing question-answering systems.
These results further indicate the effectiveness
of our QG model for enhancing closed-book
question-answering tasks.
Question Generation (QG) has a wide range of
applications, such as generating questions for ex-
ams (Jia et al., 2021; Lelkes et al., 2021; Dugan
et al., 2022) or children’s story books (Zhao et al.,
2022; Yao et al., 2022), recommending questions
for users in a dialogue system (Shukla et al., 2019;
Laban et al., 2020), improving visual (Li et al.,
2018; Lu et al., 2022) or textual question-answering
tasks (Duan et al., 2017; Lewis et al., 2019a; Zhang
and Bansal, 2019; Sultan et al., 2020; Lyu et al.,
2021), asking clariﬁcation questions (Rao and
Daumé III, 2019; Yu et al., 2020; Ren et al., 2021),
and generating queries for SQL 
or multimodal documents .
∗Equal ContributionPrevious works on QG are mainly under the open-
book setting, which aims to generate questions
based on factoid or human-generated short an-
swers under the assumption that there is access
to external knowledge like retrieved documents
or passages (Du et al., 2017; Zhao et al., 2018;
Kim et al., 2019; Fei et al., 2021). After Roberts
et al. (2020) demonstrated that feeding a large
pre-trained model input questions alone without
any external knowledge can lead to competitive re-
sults with retrieval-based methods on open-domain
question-answering benchmarks, there is an in-
creasing interest in the closed-book setting. This
closed-book setting is appealing in practice and
can be widely applied, e.g., in question sugges-
tion , query
recommendation , and other prac-
tical settings where extensive external knowledge
is unavailable.
However, generating questions without access
to such external knowledge is challenging for two
key reasons. First, without access to retrieved doc-
uments (or passages), simple open-domain strate-
gies like basing the answers on these documents (or
passages) are not possible under the closed-book
setting. Instead, models must rely on the answers
alone. Second, the data used by most of the closed-
book works 
are variants of existing open-domain datasets, e.g.,
SQuAD , TriviaQA (Joshi
et al., 2017), WebQuestions 
that ignore the answer-related passages. These an-
swers in open-book works are usually short, e.g.,
entities, and easier to be remembered by the lan-
guage model and stored in the parameters of the
model than long-form answers. Thus, this leads
to our motivating research question – How can we
empower a QG model to better understand the se-
mantics of long-form abstractive answers and store
more information in its parameters?
To tackle the aforementioned challenges existingarXiv:2210.06781v2  [cs.CL]  10 Feb 2023in the closed-book setting, this paper proposes a
new QG model with two unique characteristics: (i)
a contrastive learning loss designed to better under-
stand the semantics of the answers and the seman-
tic relationship between answers and ground-truth
questions at a contextual-level; and (ii) an answer
reconstruction loss designed to measure the an-
swerability of the generated question. Contrastive
learning has shown promising results in many NLP
tasks, e.g., (Giorgi et al., 2021; Gao et al., 2021;
Yang et al., 2021) and aligns positive pairs bet-
ter with available supervised signals (Gao et al.,
2021); here we show how to learn question rep-
resentations by distinguishing features of correct
question-answer pairs from features of incorrectly
linked question-answer pairs. Further, to ensure the
generated questions are of good quality and can be
answered by the answer that is used for question
generation, we frame the model as a generation-
reconstruction process (Cao et al., 2019; Zhu et al.,
2020), by predicting the original answers given
the generated questions by a pre-trained seq2seq
book dataset with long-form abstractive answers –
WikiCQA – to complement existing datasets like
GooAQ  and ELI5 (Fan
et al., 2019) and show how to leverage our model
to generate synthetic data to improve closed-book
question-answering tasks.
Through experiments, we ﬁnd that the proposed
QG model shows improvement through both auto-
matic and human evaluation metrics on WikiCQA
and two public datasets. Compared to the base-
line, the proposed QG framework shows an im-
provement of up to 2.0%, 2.7%, and 1.8% on
the ROUGE-L score on WikiCQA, GooAQ-S, and
ELI5, respectively, and 1.3% and 2.6% in terms of
relevance and correctness. Furthermore, we lever-
age the QG framework to generate synthetic QA
data from WikiHow summary data and pre-train
a closed-book QA model on it in both an unsu-
pervised and semi-supervised setting. The perfor-
mance is evaluated on both seen (WikiCQA) and
unseen  datasets. We ﬁnd consis-
tent improvements across these datasets, indicating
the QG model’s effectiveness in enhancing closed-
book question-answering tasks.
In conclusion, our contributions can be summa-
rized as follows:
•We propose a contrastive QG model, which to
our knowledge is the ﬁrst work to explore con-trastive learning for QG under a closed-book
setting.
•The proposed model outperforms baselines on
three datasets. The human evaluation also indi-
cates that the questions generated by our model
are more informative compared to other base-
lines.
•We leverage the QG model as a data augmenta-
tion strategy to generate large-scale QA pairs.
Consistent improvements shown on both seen
datasets and unseen datasets indicate the QG
model’s effectiveness in enhancing closed-book
question-answering tasks.
2 Related Work
Many previous works on QG are under the open-
book setting, which takes factoid short answers (Ra-
jpurkar et al., 2016) or human-generated short
answers  with the corre-
sponding passages to generate questions (Zhang
et al., 2021). Early approaches for question genera-
tion rely on rule-based methods (Labutov et al.,
2015; Khullar et al., 2018). To bypass hand-
crafted rules and sophisticated pipelines in QG,
sequence-to-sequence approach with an attention
mechanism. The recently proposed pre-trained
transformer-based frameworks (Lewis et al., 2020;
Raffel et al., 2020) also improve the performance
of QG. In addition, Sultan et al. (2020) shows that
the lexical and factual diversity of QG provides
better QA training. However, their success can not
directly adapt to the closed-book setting, where
the model is supposed to generate questions solely
relying on answers. In this work, we explore the
widely applicable closed-book QG setting, which
is still under-explored.
Contrastive Learning aims to pull semantically
similar neighbors close and push non-neighbors
apart. It has achieved great success under both
supervised and unsupervised settings. In pioneer
works, the contrastive loss function (Hadsell et al.,
2006; Chopra et al., 2005) has been proposed as a
training objective in deep metric learning consid-
ering both similar and dissimilar pairs. Recently,
Chen et al. (2020) proposes the SimCLR frame-
work to learn useful visual representations. View-
ing contrastive learning as dictionary look-up, He
et al. (2020) present Momentum Contrast (MoCo)
to build dynamic dictionaries for contrastive learn-
ing. Some works apply contrastive learning intothe NLP domain to learn better sentence repre-
sentations .
In addition, contrastive learning has been applied
in multilingual neural machine translation (Pan
et al., 2021), abstractive summarization (Liu and
Liu, 2021), and multi-document question genera-
tion . The recent most relevant
work is , where they design two
contrastive losses for paraphrase generation. In this
work, we adopt contrastive learning for improv-
ing representation learning in question generation
under a closed-book setting.
3 Proposed Approach
To answer our research question – How can we
empower a QG model to better understand the se-
mantics of long-form abstractive answers and store
more information in its parameters? – we propose a
closed-book QG model, which generates questions
directly without access to external knowledge. For-
mally, given an answer sentence x, a closed-book
QG engine generates a natural question y. Fig-
ure 1 illustrates an overview of the proposed QG
framework, which consists of three parts: question
generation, contrastive learning, and answer recon-
struction. The framework is optimized with the
joint losses from these three parts simultaneously.
A1
E Ai
D
 ReconstructorGumbel
SoftmaxAnEQ1
Qi
Qn
Figure 1: An overview of the proposed closed-book
QG framework, which consists of three parts: con-
trastive learning, question generation, and answer con-
struction.Ai: represents answer i; Qirepresents ques-
tion i.
3.1 Question Generation
We ﬁrst focus on question generation through a
sequence-to-sequence architecture which consists
of an encoder and a decoder (Sutskever et al., 2014;
Vaswani et al., 2017). The encoder takes an input
sequence of source words x= 
and maps it to a sequence of continuous represen-
tations z= . Then, the decoder
takes zand generates a sequence of target wordsy= at a time. The closed-book
QG task is deﬁned as ﬁnding ˆy:
ˆy= arg max
yP
whereP(y|x)is the conditional likelihood of the
predicted question sequence ygiven answer x.
P(y|x) =T∏
i=1p
Given the answer-question pairs, the training ob-
jective of the generation part in the proposed frame-
work is to minimize the Negative Log-Likelihood
(NLL) of the training data,
Lqg=−N∑
i=1logp
whereqiis thei-th token in the generated question
andAis the answer.
A naive question generation model will generate
questions based on answers but lacks a rich model
of the semantics of answers nor can it guarantee the
generated questions have a semantic relationship
with the answers. Intuitively, an encoded answer
should be similar to its question and dissimilar to
others. In addition, the generated question should
be able to be answered by the answers. Hence, this
motivates the following contrastive learning and
answer reconstruction modules.
3.2 Contrastive Learning
Contrastive learning aims to pull positive pairs and
push apart negative pairs to learn effective represen-
tations. Further, the supervised signals can produce
better sentence embeddings by improving align-
ment between positive pairs .
An effective QG model should be able to under-
stand the semantics of the answers and the semantic
relationship with the ground-truth questions. Espe-
cially, the encoded answer should have semantic
similarity with its ground-truth question and dis-
similarity with other questions. Thus, aiming to
learn a similarity function that pulls the distance
between the answer sequence representation and
its ground-truth question sequence representation
closer, we design a contrastive loss in the repre-
sentation space. Speciﬁcally, given a positive pair
S={}n
i=1, where xiandyiare semanti-
cally related inputs, the other 2(n−1)exampleswithin a mini-batch are treated as negative exam-
ples. The training objective for is:
Lcl=−logexp∑2n
i=1exp
where zxiandzyiis the representation of input xi
andyi,sim =z⊤
izj/∥zi∥∥zj∥denotes co-
sine similarity, and τclis a temperature parameter.
In this work, aiming to learn better answer rep-
resentations and force the encoder to drive repre-
sentations of correct question-answer pairs closer
than representations of incorrect question-answer
pairs, we take the ground-truth question as the pos-
itive instance of an answer and ﬁne-tune the model
parameters based on the contrastive loss function
(Eq. 4), where zis the embedding of the special
token [CLS] from the transformer encoder, repre-
senting the meaning of the entire sentence.
3.3 Answer Reconstruction
The questions that are generated by the model
should be of good quality and should also be able
to be answered by the answer that is used for ques-
tion generation. To measure the answerability of
the generated question, we design an answer recon-
struction module, which uses a pre-trained seq2seq
model to predict the original answer given the gen-
erated question. The loss is calculated by a negative
log-likelihood loss function:
Lar=−N∑
i=1logp(ai|Q) (5)
whereaiis thei-th token in the answer and Q is
the generated question.
A major challenge is that the generated questions
from Section 3.1 are not differentiable. Gradients
cannot be back-propagated directly. To solve this
challenge, we employ the Straight-Through (ST)
Gumbel-Softmax for gradient computation (Jang
et al., 2017). The ST Gumbel-Softmax is a discrete
version of the Gumbel-Softmax and takes different
forward and backward paths. In the forward pass,
the embedding is discretized by using the argmax,
whereas in the backward pass the gradients are
computed by the gumbel-softmax (Qader et al.,
2019; Lu et al., 2021).
yi=exp((log(pi) +gi)/τgs)
∑|V|
j=1exp
whereτgsis a temperature parameter and giis the
Gumbel noise drawn from a uniform distribution. In this work, the one-hot embedding from
ST Gumbel-Softmax is multiplied with the vocab-
ulary embedding and then fed into the encoder of
the pre-trained seq2seq model as the representation
of the generated question.
3.4 Overall Loss Function
As a result, all three losses are summed together to
provide the overall loss function Las follows:
L=λ1Lqg+λ2Lcl+λ3Lar, (7)
The weights λ1,λ2, andλ3are tuneable hyper-
parameters to balance losses and the ﬁnal objective
is to minimize the overall loss.
The overall structure of the proposed QG frame-
work is presented in Algorithm 1.
Algorithm 1: QG framework.
Input: Pre-trained language model p(q|a),
answer reconstruction model p(a|q)
and answer-question pairs
Output: Question generator p(q|a)
1fori←1toEpoch do
2 ˆq=p(q|a)
3 ComputeLqgvia Eq. 3
/* contrastive learning */
4ai=Encoder ([CLS]⊕a)[:,0 :]
5a+
i=Encoder ([CLS]⊕q)[:,0 :]
6 GetLclto(ai,a+
i)via Eq. 4
/* answer reconstruction */
7 ˆa=p(a|ˆq)
8 ComputeLarvia Eq. 5
9 Calculate total loss Lvia Eq. 7
10 Update generator p(q|a)withL
11end
12return question generator p(q|a)
4 Experimental Setup
To evaluate the effectiveness of the proposed QG
framework, we aim to answer the following re-
search questions (RQ) via experiments: RQ1 : Can
this proposed QG framework improve the perfor-
mance of the closed-book QG tasks? RQ2 : Are the
generated questions of good quality? That is, are
they ﬂuent and relevant to the answer? RQ3 : Can
this QG framework be leveraged as a good resource
to generate synthetic data for the QA task? RQ4 :
How much does each component in the framework
contribute?4.1 Dataset
We conduct the experiments on two public datasets
– GooAQ-S  and ELI5 (Fan
et al., 2019) – and a new dataset we curate called
WikiCQA.
GooAQ-S is a sub-sampled dataset from GooAQ
which contains three different sub-tasks: short,
snippet , and col-
lection response questions .
Under the long-form closed-book setting, we adopt
the snippet part (i.e., questions with snippet an-
swers) for our experiments as in the original pa-
per. Furthermore, from the paper, the model perfor-
mance on the snippet task does not vary much when
supervised with 200K and 2M training instances.
Thus, to improve the experimental efﬁciency, we
take 200k instances from the snippet set and split
them based on their original scripts.1
ELI5  is a widely-used large-scale
corpus for long-form question-answering with sup-
porting web documents. In this work, we use the
question-answer pairs from the dataset and ignore
the supporting context to ﬁt the closed-book setting
like . We follow the data
split from huggingface.2
WikiCQA is a new closed-book long-form QA
tains 20,202 question-answer pairs and is collected
from a wiki-style website to complement existing
datasets. We shufﬂe the data and split it to train,
dev, and test sets by the ratio of 80%/10%/10%.
Table 1 shows detailed data statistics and splits of
these three datasets.
Datasets Train Val Test
WikiCQA 16,162 2,020 2,020
GooAQ-S 200,000 500 498
ELI5 272,634 9,812 24,512
Table 1: Dataset statistics.
4.2 WikiCQA
WikiCQA contains real user QA pairs collected
from WikiHow3. WikiHow is a wiki-style web-
site featuring over 200K how-to articles. Different
1
experiments/create_splits.py
2
3 , under an Attribution-
Noncommercial-Share Alike 3.0 Creative Commons
License.from the existing dataset, ELI5, containing ques-
tions/answers from the Reddit forum and leverag-
ing evidence queried from the web to help answer
the question  and GooAQ, con-
taining questions from search auto-complete and
answers from answer boxes ,
WikiCQA involves long-form question-answering
grounded on WikiHow articles.
Dataset Construction We construct the new
dataset by collecting the question-answer pairs
from the Q&A section of articles on WikiHow. The
questions and answers are related to the speciﬁc
articles, asked by WikiHow users, and answered
by a knowledgeable reader or editor4. The ques-
tions can not be answered directly by the content of
the article. After removing duplicates and question-
answer pairs with meaningless answers, 23,037 QA
pairs remain. To keep the same format as other ex-
isting QA datasets , we discard
questions not starting with a question-type word
. After the dataset processing
steps, we arrive at 20,202 question-answer pairs.
More details can be found in Appendix A.
In Table 2, we show an example article from
Wikihow, which includes the title and summary of
an article and question-answer pairs from the cor-
responding Q&A section. From the example, we
can see that answers are abstractive and long-form,
written by real users, and not contained within the
context passage. These question-answer pairs from
the Q&A section are collected in the newly con-
structed dataset. Thus, this dataset is different from
reading comprehension datasets, where the answers
are a short text span in context. More comparisons
with ELI5 and GooAQ are shown in Appendix B.
Title: How to Prepare a Healthy Meal for Your Pet Dog.
Summary: To prepare a healthy meal for your dog,
choose lean meat with the bones and fat removed, like
chicken or beef . . .
Q:What can I feed my dog if I have run out of dog food?
A:In the short term, any bland human food such as
chicken or white ﬁsh with rice or pasta is just ﬁne . . .
Q:How much homemade dog food do you feed your dog?
A:Great question because it highlights one of the prob-
lems of feeding home prepared foods . . .
Q:What should I not feed my dog?
A:There are many human foods that are toxic to dogs.
Top of the list of foods NOT to give are . . .
Table 2: Question-answer pairs from WikiHow Q&A
section. Q: question; A: answer.
4
Reading-and-Learning-from-wikiHow4.3 Models & Hyper-parameters
We use BART-base , a widely
used sequence-to-sequence framework with 12 lay-
ers and a hidden size of 1024, as the backbone
model. Following previous works (Fan et al., 2019;
Khashabi et al., 2021), we ﬁnetune it using answers
as inputs and questions as outputs as the baselines.
To get the answer reconstruction loss (Section 3.3),
we use a BART-base model which is ﬁne-tuned on
the target QA datasets.
Based on the dataset analysis in Appendix B, we
set the maximum sequence length to 128 for the
question and 256 for the answer sentence to im-
prove calculation efﬁciency. We train the models
for ﬁve epochs with a learning rate of 5×10−5and
evaluate the checkpoint for each epoch. We select
the checkpoint with the highest ROUGE-L score
on the validation set and report its corresponding
score on the test set. We run each model three times
and record the average scores. After performing
manual hyper-parameter searching, we set loss pa-
rametersλ1,λ2, andλ3in Equation 7 to 1.0, 0.1,
0.1, respectively, which give the best ROUGE-L
score on validation set. The temperature of con-
trastive loss τclis set to 0.3. The experiments are
run on 1 NVIDIA Tesla V100 GPU. The training
time takes about 48 hours.
4.4 Evaluation Metrics
To evaluate the performance of the models, we use
ROUGE  scores, which evaluate the n-Question-Generation-via-Contrastive-Learning.Conversational recommender systems have demonstrated great
success. They can accurately capture a user’s current detailed pref-
erence – through a multi-round interaction cycle – to effectively
guide users to a more personalized recommendation. Alas, con-
versational recommender systems can be plagued by the adverse
effects of bias, much like traditional recommenders. In this work,
we argue for increased attention on the presence of and methods for
counteracting bias in these emerging systems. As a starting point,
we propose three fundamental questions that should be deeply ex-
amined to enable fairness in conversational recommender systems.
1 INTRODUCTION
Recently, a new form of interactive recommendation system – the
conversational recommender system (CRS) – has shown great suc-
cess in enhancing personalization through multi-turn interactions
 [ 2–5,9,10]. Compared to traditional recommenderarXiv:2010.04125 (2020).Popularity bias is a long-standing challenge in recommender sys-
tems. Such a bias exerts detrimental impact on both users and item
providers, and many efforts have been dedicated to studying and
solving such a bias. However, most existing works situate this prob-
lem in a static setting, where the bias is analyzed only for a single
round of recommendation with logged data. These works fail to
take account of the dynamic nature of real-world recommendation
process, leaving several important research questions unanswered:
how does the popularity bias evolve in a dynamic scenario? what
are the impacts of unique factors in a dynamic recommendation
process on the bias? and how to debias in this long-term dynamic
process? In this work, we aim to tackle these research gaps. Con-
cretely, we conduct an empirical study by simulation experiments
to analyze popularity bias in the dynamic scenario and propose
a dynamic debiasing strategy and a novel False Positive Correc-
tion method utilizing false positive signals to debias, which show
effective performance in extensive experiments.
1 INTRODUCTION
Popularity bias – popular items are overly exposed in recommen-
dations at the expense of less popular items that users may find
interesting – is a long-standing challenge in recommender sys-
tems [ 14,17,18,23,25]. Most existing efforts to study popularity
bias adopt a static setting [ 14,17,18,23,25]. That is, a model is
trained over an offline dataset, and popularity bias is analyzed by
conducting a single round of recommendation. While these studies
have highlighted the prevalence of popularity bias, there is a signifi-
cant research gap in our understanding of the dynamics of this bias,
the factors impacting popularity bias and its evolution, and the effi-
cacy of methods to mitigate this bias under real-world assumptions
of system evolution. Hence, this paper proposes a framework for
the study of popularity bias in dynamic recommendation.
Dynamic recommendation [5,9,12,19] can be viewed as a closed
loop illustrated in Figure 1. Users interact with the system through
a set of actions ; this user-feedback data
is then used to train a recommendation model; the trained model is
used to recommend new items to users; and then the loop continues.
While there are many opportunities for bias to affect this dynamic
recommendation process, we identify three key factors that may
impact popularity bias and its evolution: (i) inherent audience size
imbalance : users may like some items more than others (even with
a purely bias-free random recommender), meaning that a few items
may have very large audience sizes while the majority have small
ones; (ii) model bias : the recommendation model itself may amplify
any imbalances in the data it ingests for training; and (iii) closed
∗Work was done when Ziwei, Yun, and Xing were at Texas A&M University.
Figure 1: The pipeline of the dynamic recommendation.
feedback loop : since the cycle repeats, the feedback collected from
recommendations by the current model will impact the training of
future versions of the model, potentially accumulating the bias.
Specifically, we undertake a three-part study to investigate pop-
ularity bias in dynamic recommendation: (i) We conduct a compre-
hensive empirical study by simulation experiments to investigate
how the popularity bias evolves in dynamic recommendation, and
how the three factors impact the bias. (ii) We explore methods to
mitigate popularity bias in dynamic recommendation. We show
how to adapt existing debiasing methods proposed in a static set-
ting to the dynamic scenario. We further propose a model-agnostic
False Positive Correction (FPC) method for debiasing, which can be
integrated with other debiasing methods for further performance
improvements. (iii) Finally, we report on extensive experiments to
show the effectiveness of the proposed debiasing method compared
with state-of-the-art baselines.
2 RELATED WORK
Popularity bias is a long-standing problem and has been widely
studied. Some methods adopt an in-processing strategy to miti-
gate bias by modifying the model itself [ 1,17,23], while others
adopt a post-processing strategy to mitigate bias by modifying
the predictions of the model [ 2,18,25]. One of the most typical
approaches is to debias by assigning weights inversely propor-
tional to item popularity in the loss of a model [ 11,17]. By this,
popular and unpopular items can be balanced during training and
more even recommendations can be generated. Similar to this idea,
Steck  proposes to directly re-scale the predicted scores based
on popularity to promote unpopular items and prevent popular
items from over-recommendation. The scaling weights are also in-
versely proportional to item popularity. Besides, a recent work 
investigates the bias from the perspective of causal inference and
propose a counterfactual reasoning method to debias. Note that all
of these works evaluate popularity bias by comparing how often
items are recommended without regard for the ground truth of
user-item matching. To address this gap,  proposes the conceptarXiv:2207.03372v2  [cs.IR]  1 Aug 2022Algorithm 1: Dynamic Recommendation Process
1Bootstrap: Randomly show 𝐾items to each user and
collect initial clicks Dand train the first model 𝜓byD;
2for𝑡=1 :𝑇do
3 Recommend 𝐾items to the current user 𝑢𝑡by𝜓;
4 Collect new clicks and add them to D;
5 if𝑡%𝐿==0then
6 Retrain𝜓byD;
of popularity-opportunity bias which compares the true positive
rate of items to evaluate the bias, and a popularity compensation
method is proposed, which explicitly considers user-item matching.
3 PROBLEM FORMALIZATION
3.1 Formalizing Dynamic Recommendation
Suppose we have an online platform that provides recommenda-
tions. Given we have a set of users U={1,2,...,𝑁}and a set of
itemsI={1,2,...,𝑀}in the system. Every user has a subset of
items the user likes (unknown to the system), and we define the
total number of matched users who like the item 𝑖as the audience
size of𝑖, denoted as 𝐴𝑖. At the beginning (a bootstrap step), for each
user, the system randomly exposes 𝐾items to bootstrap the user
and thus collects initial user-item clicks D. Based on the initial data
D, the first recommendation model 𝜓, such as a matrix factoriza-
tion (MF) , is trained. Then, as users coming to the system one
by one, the system uses the up-to-date model to provide 𝐾ranked
items as recommendations and collect new user-item clicks. After
every𝐿user visits, the system retrains the recommendation model
with all clicks collected up to now. This dynamic recommendation
process is summarized in Algorithm 1.
3.2 Formalizing Popularity Bias
which evaluates whether popular and unpopular items receive clicks
(or other engagement metrics) proportional to their true audience sizes .
In other words, do popular and unpopular items receive similar
true positive rates ? At iteration 𝑡in the dynamic recommendation
process, to quantify the popularity bias, we need to first calculate
the true positive rate for each item. Suppose item 𝑖has received 𝐶𝑡
𝑖
clicks in total from the beginning to iteration 𝑡, the true positive rate
for𝑖is𝑇𝑃𝑅𝑖=𝐶𝑡
𝑖/𝐴𝑖. Then, we can use the Gini Coefficient [ 4,21]
to measure the inequality in true positive rates corresponding to
item popularity at iteration 𝑡:
𝐺𝑖𝑛𝑖𝑡=Í
𝑖∈I(2𝑖−𝑀−1)𝑇𝑃𝑅𝑖
𝑀Í
𝑖∈I𝑇𝑃𝑅𝑖, (1)
where items are indexed from 1to𝑀in audience size non-descending
order (𝐴𝑖≤𝐴(𝑖+1)). We use−1≤𝐺𝑖𝑛𝑖𝑡≤1to quantify the popular-
ity bias1: a small|𝐺𝑖𝑛𝑖𝑡|indicates a low bias; 𝐺𝑖𝑛𝑖𝑡>0represents
that true positive rate is positively correlated to item audience size;
1In this paper, we conduct simulation experiments with semi-synthetic data to study
the popularity bias in dynamic recommendation, in which audience size of items are
known. In practice, we need to estimate the audience size based on observed clicks,
such as inverse propensity scoring based methods from [12, 24].Table 1: Dataset statistics.
#user #item density 𝐺𝑖𝑛𝑖 (audience size)
ML1M 1,000 3,406 0.0657 0.6394
Ciao 1,000 2,410 0.0696 0.4444
and𝐺𝑖𝑛𝑖𝑡<0represents that the true positive rate is negatively
correlated to audience size (reversed popularity bias).
3.3 Factors Impacting Popularity Bias
One of the goals in this paper is to deepen our understanding of
Figure 1, we focus on three major factors:
1. Inherent audience size imbalance. Items inherently have dif-
ferent audience sizes, and this imbalance can potentially lead to
popularity bias. It has been observed that the audience size for items
usually follows a long-tail distribution , meaning that a few
items have a very large audience size while the majority have small
ones. This inherent imbalance will result in imbalanced engage-
ment data (like clicks), even if every item is equally recommended
by a bias-free random recommender.
2. Model bias. A recommendation model tends to rank an item
with more clicks in the training data higher than an item with fewer
clicks, even if the ground truth is that the user equally likes both of
them . This is a common deficiency of collaborative filtering
based algorithms and directly leads to popularity bias if the training
data is imbalanced.
3. Closed feedback loop. Finally, we consider the phenomenon
that future models are trained by the click data collected from the
recommendations by previous models [ 8,16,19]. In this way, the
popularity bias generated in the past can accumulate, leading to
more bias in subsequent models as the feedback loop continues.
4 EMPIRICAL STUDY
In this section, we conduct an empirical study to uncover how
the popularity bias evolves in dynamic recommendation; and the
impacts of the three discussed bias factors on the bias.
4.1 Setup
Due to the challenges of running repeatable experiments over live
platforms, we follow the widely-adopted approach [ 3,5,8,9,12] of
conducting experiments to simulate the dynamic recommendation
process in Section 3.1.
First, we follow [ 9,12] to generate semi-synthetic data based
on real-world user-item interaction datasets. Concretely, we adopt
MovieLens 1M ( ML1M )  and Ciao  as base datasets and
randomly keep 1,000 users in each dataset. Then, we run the matrix
factorization (MF) model  to complete the original datasets
to provide the ground truth of user-item relevance. The detailed
statistics of the semi-synthetic datasets are shown in Table 1, where
we also calculate the Gini Coefficient of the item audience size in
each dataset to quantify the inherent audience size imbalance.
Then, we conduct experiment to simulate the process in Al-
gorithm 1. Concretely, we recommend 𝐾=20items to users at
each iteration; run the simulation for 𝑇=40,000iterations; and
retrain the recommendation model after every 𝐿=50iterations.
To simulate user click behavior, we follow  and model the clickFigure 2: Results of three methods on ML1M.
behavior based on the position bias of 𝛿𝑘=1/𝑙𝑜𝑔 2(1+𝑘)to deter-
mine whether user 𝑢will examine item 𝑖at position𝑘. We observe
a click only if the user examines and likes the recommended item.
All experiments are repeated for 10 times.
4.2 Evolution of Popularity Bias
The first question to investigate is: how does popularity bias evolve
in dynamic recommendation? Here, we use the basic MF as the
recommendation model2, and the dynamic recommendation pro-
for ML1M are shown in Figure 2, where for comparison, we also
include a Popular method to rank items only based on the number
of observed clicks so far, and a Random method to randomly rank
items. At iteration 𝑡, we report the number of cumulative clicks up
to now as the metric evaluating recommendation utility, and we
report𝐺𝑖𝑛𝑖 for measuring the popularity bias.
First, we observe in the left figure in Figure 2 that MF produces
significantly higher recommendation utilities than the Popular and
Random methods. Moreover, the number of cumulative clicks first
increases then converges for the Popular method, and after some
iterations the Random method can even outperform the Popular
method on both datasets, which illustrates the harm of popularity
bias. Second, we observe in the right figure that: (i) the Random
method produces near zero bias; (ii) the Popular method results in
high𝐺𝑖𝑛𝑖 values throughout the whole experiment; and (iii) MF
first produces a rapid increase in 𝐺𝑖𝑛𝑖 and then maintains this high
𝐺𝑖𝑛𝑖 value to the end of the experiment.
While it is not surprising that we observe popularity bias in
dynamic recommendation, it is surprising that a traditional MF
(which is also the foundation of many more advanced models [ 7,
13,22]) boosts the bias so fast, and the produced bias nearly equals
that in a heavily-biased Popular method. Beyond static studies 
of popularity bias that have observed its prevalence, we observe
that this bias grows rapidly and maintains at a high level, indicating
the need for special interventions to mitigate this issue.
4.3 Impacts of Three Bias Factors
Section 3.3.
4.3.1 Impact of Closed Feedback Loop. First, we conduct a new
experiment that removes the closed feedback loop by: (i) not us-
ing the clicks collected from personalized recommendation (by
MF) as training data; (ii) after every 𝐿personalized recommen-
dation iterations, adding a random recommendation step to gen-
erate random rankings to 𝐿randomly selected users and collect
random-recommendation clicks; and (iii) only using the random-
recommendation clicks to train the personalized MF model. In
2In this paper, to counteract the position bias, we adopt the inverse propensity scoring
based loss from  for training the MF model, where we use 𝑝𝑘=1/𝑙𝑜𝑔 2(1+𝑘)as
the propensity estimation for a click observed at position 𝑘
Figure 3: Compare popularity bias in experiments with (w/
CFL) and without closed feedback loop (w/o CFL).
this way, the MF is trained by data purely from random recom-
mendations and will not be influenced by previous personalized
recommendation models, i.e., breaking the closed feedback loop.
We evaluate the popularity bias only for personalized recommen-
dations by MF. We denote this experiment setup as w/o CFL , and
denote the experiment with closed feedback loop (the same as MF
in Section 4.2) as w/ CFL .
Because the MF in w/o CFL is trained by click data from random
recommendations, whose data size is much smaller than that in w/
CFL. Hence, it is unfair and not informative to compare the utility
between w/ CFL and w/o CFL, and we only show the popularity bias
comparison in Figure 3. From the figures we can see that compared
to w/ CFL, in w/o CFL, the popularity bias also keeps increasing but
at a much slower speed. This indicates that the closed feedback loop
does exacerbate the popularity bias . Without the closed feedback
loop, the popularity bias is only from the current recommendation
model, and there is no accumulated bias from previous models.
Notice that𝐺𝑖𝑛𝑖 in w/o CFL still keeps increasing. This is because
the training data gets increasingly denser, making the model bias
increases as we will justify in the following section.
4.3.2 Impact of Model Bias. Next, we conduct a series of static
recommendation experiments to study the impact of model bias
on popularity bias. First, we study how the inherent audience size
imbalance influences model bias. Beside the semi-synthetic dataset
ML1M we already used, we also generate 4 variants with differ-
ent levels of inherent audience size imbalance. Now we have 5
datasets with increasing levels of audience size imbalance, denoted
asI1,I2,I3,I4,I5, and the corresponding 𝐺𝑖𝑛𝑖 of audience size are
0.37, 0.45, 0.51, 0.57, 0.64 (higher value means severer imbalance).
Result of a conventional MF model is shown in the left of Figure 4,
where we see that severer imbalance leads to increased model bias.
Next, we investigate how training data density influences the
model bias. In this case, we use the same ML1M dataset with 𝐺𝑖𝑛𝑖
of audience size 0.64, but generate 8 training datasets with dif-
ferent densities. The 8 training datasets with increasing densities
are denoted as D1,D2,D3,D4,D5,D6,D7,D8, and the correspond-
ing densities are 0.01%, 0.05%, 0.1%, 0.2%, 0.4%, 0.8%, 1.6%, 3.2%.
Experimental result is presented in the right of Figure 4, where we
can see that with training datasets getting denser, the model bias
first increases but then deceases. This may be because with denser
data, both model bias and ability to learn user-item relevance are
improved. And after a threshold, the ability to learn user-item rele-
vance surpasses the effect of model bias, leading to lower popularity
bias observed. However in practice, dense training data is rare and
the model bias usually plays a major role.
4.3.3 Impact of Inherent Audience Size Imbalance. The inherent au-
dience size imbalance exerts its influence on popularity bias mainly
through model bias, which we already exhibit by static experimentsFigure 4: Influence of inherent audience size imbalance (left)
and training data density (right) on model bias.
in Figure 4. But how does inherent audience size imbalance impact
dynamic recommendation? To answer this, we run dynamic rec-
ommendation experiments for the 5 datasets with different levels
of inherent audience size imbalance, where all other experiment
settings are the same as the MF experiment in Section 4.2. Results
are presented in Figure 5. The left figure demonstrates that with sev-
erer inherent audience size imbalance, a system can receive more
user clicks. This is because popular items can be more easily recog-
nized and correctly recommended to matched users to receive large
amounts of clicks in imbalanced datasets. On the other hand, the
right part in Figure 5 shows that with a severer inherent audience
size imbalance, higher popularity bias is generated.
4.3.4 Summary. In sum, we find that the inherent audience size
imbalance and model bias are the main sources of popularity bias;
while the closed feedback loop can intensify the bias when inherent
audience size imbalance and model bias exist. Moreover, we also
find that higher training data density and greater imbalance can
increase the effect of model bias.
5 DEBIASING APPROACHES
While we have demonstrated the evolution of popularity bias, how
can we begin to counteract it? As we empirically studied in Sec-
tion 4.3, model bias is the most essential factor. Thus, in this section,
we focus on how to mitigate the popularity bias in dynamic recom-
mendation by reducing model bias.
Most existing works reduce popularity bias in a static setting by
reducing model bias [ 17,18,23,25]. For example,  proposes a re-
scaling method (denoted as Scale ) to reduce the bias by re-scaling
the outputs of recommendation models as a post-processing step.
Concretely, the re-scaled score for a user-item pair is:
b𝑟(𝑠𝑐𝑎𝑙𝑒𝑑)
𝑢,𝑖=b𝑟(𝑚𝑜𝑑𝑒𝑙)
𝑢,𝑖/
whereb𝑟(𝑚𝑜𝑑𝑒𝑙)
𝑢,𝑖is the output predicted score from a recommenda-
tion model;𝐶𝑖is the number of clicks the item has in training data;
𝛼is the hyper-parameter to control the debiasing strength, higher
𝛼means more strength for debiasing; and b𝑟(𝑠𝑐𝑎𝑙𝑒𝑑)
𝑢,𝑖is the re-scaled
score used for final ranking.
In static recommendation, this debiasing strength hyper-parameter
𝛼is a constant. However, as we see in Section 4.3.2, model bias
is proportional to training data density and imbalance. Hence, we
propose to gradually increase 𝛼from 0 with an increasing step Δ
through the dynamic recommendation process. Beyond the spe-
cific Scale method , most existing popularity debiasing meth-
ods [ 17,18,23,25] include such a debiasing strength weight 𝛼,
meaning that we can apply them dynamically in the same way by
involving the increasing step Δ.
Besides, we notice that in a high popularity bias case, popular
items can be incorrectly over-recommended to unmatched users
Figure 5: Utility (left) and popularity bias (right) for datasets
with different inherent audience size imbalance.
(generating false positive signals), the false positive signal is corre-
lated with the popularity bias. If we could correct the recommen-
dations based on these false positive signals, we could lower the
popularity bias. So, we propose the False Positive Correction (de-
noted as FPC ) method to correct the predicted scores based on false
positive signals in a probabilistic way. More specifically, suppose
we are going to predict the relevance b𝑟𝑢,𝑖between user 𝑢and item
𝑖, and we already have a predicted score b𝑟(𝑚𝑜𝑑𝑒𝑙)
𝑢,𝑖from a model. As-
sume that item 𝑖has been recommended to user 𝑢for𝐹times before
and has never been clicked, and we record the ranking positions of
these𝐹times of recommendation as {𝑘1,𝑘2,...,𝑘𝐹}. So, the false
positive signals can be denoted as {𝑐𝑘1=0,𝑐𝑘2=0,...,𝑐𝑘𝐹=0},
where𝑐𝑘represents whether user 𝑢clicks the item 𝑖ranked at po-
sition𝑘. We further denote the probability that 𝑢likes𝑖as𝜃𝑢,𝑖; and
denote the probability of examining an item at ranking position
𝑘as𝛿𝑘. Then, we can calculate the conditional probability that 𝑢
likes𝑖given the false positive signals as:
𝑃=1−1−𝜃𝑢,𝑖Î𝐹
𝑓=1
where𝜃𝑢,𝑖is unknown and needs to be estimated. We can use the
prediction b𝑟(𝑚𝑜𝑑𝑒𝑙)
𝑢,𝑖from a model as 𝜃𝑢,𝑖. So, we use Equation 3 with
𝜃𝑢,𝑖=b𝑟(𝑚𝑜𝑑𝑒𝑙)
𝑢,𝑖and𝛿𝑘𝑓=1/𝑙𝑜𝑔 2(1+𝑘𝑓)(as how we model the
position bias) to correct model predictions by false positive signals.
Yet, one disadvantage of FPC is that if we use b𝑟(𝑚𝑜𝑑𝑒𝑙)
𝑢,𝑖from a
biased model, such as an MF, as 𝜃𝑢,𝑖, FPC is still vulnerable to the
model bias. Thus, we propose to use the predictions from a debiased
model, such as the Scale in Equation 2 or other debiasing models
take full advantage of both true positive signals and false positive
signals to counteract the popularity bias.
6 DEBIASING EXPERIMENTS
In this section, we conduct experiments to show how the popular-
ity bias is mitigated in dynamic recommendation by dynamically
reducing model bias and the proposed FPC method.
6.1 Setup
The basic setup is the same as Section 4.1. To validate our pro-
posed FPC, we include different types of debiasing methods for
comparison. The basic recommendation model is the MF. For the
debiasing models, first, we consider existing static debiasing method
we also have the dynamic version, denoted as DScale . Last, we have
the proposed FPC method to debias based on false positive signals.
And we also combine the proposed FPC with DScale to reduce the
popularity bias utilizing both true positive and false positive signals,
denoted as FPC-DScale . For every debiasing method (except FPC),Figure 6: Compare the static debiasing method Scale and its
dynamic version DScale.
there is a debiasing strength weight or the increasing step Δ, we
tune these hyper-parameters so that all methods achieve similar bias
level, and we compare the click counts to compare the performance.
Code available is at 
Dynamic-Recommendation.
6.2 Empirical Results
In the following experiments, we study the effect of dynamic de-
biasing compared with static ones; the effect of the proposed FPC;
and the effect of integrating FPC with other debiasing methods.
6.2.1 How do dynamic debiasing methods perform compared with
static ones? To show the advantage of dynamic debiasing over
static approaches, we conduct experiments with a basic MF and the
static debiasing model – Scale, compared with its dynamic version
– DScale. We tune all models so that similar popularity bias level
is achieved, and compare the number of clicks. We plot how the
utility and bias change in Figure 6. The right figure shows that
comparing to MF, both Scale and DScale reduce the bias. However,
they show very different patterns: DScale increases the bias at
the beginning then keeps decreasing the bias; while Scale keeps
increasing the bias and eventually surpasses DScale. This is because
as the experiment continues, density and imbalance in training
data increases, resulting in higher model bias and more debiasing
strength needed. So, dynamically increasing the debiasing strength
following the increasing bias can produce better results.
6.2.2 What is the effect of FPC alone? Then, we show the results of
MF and FPC in Figure 7. The right figure shows that FPC increases
the bias at the beginning, but then keeps decreasing the popularity
bias. The reduction of bias metric 𝐺𝑖𝑛𝑖 is significant. On the other
hand, the left figure shows that FPC can even increase the number
of clicks during the experiment compared with MF. This is because
by mitigating the popularity bias, popular items are prevented to be
over-recommended to unmatched users and more unpopular items
are recommended accurately and receive clicks. So, it is a win-win
scenario that both users and item providers can benefit from.
6.2.3 What is the effect of integrating FPC with other debiasing
methods? Although, FPC can reduce the bias and improve the util-
ity, it only utilizes the false positive signals without considering
the true positive signals as existing debiasing models do. Hence,
combining FPC with other debiasing methods is expected to achieve
even better performance. To justify this, we also include results
of DScale and FPC-DScale in Figure 7 for comparison. The right
figure demonstrates that DScale and FPC-DScale are able to reduce
the bias lower than FPC. And we see that DScale and FPC-DScale
produce similar level of bias, however, the left figure shows that
FPC-DScale generates significantly more clicks, illustrating the
advantage of integrating FPC with DScale.
Figure 7: Compare MF, FPC, DScale, and FPC-DScale on
ML1M. (Medium debiasing level)
7 CONCLUSION
In this work, we investigate popularity bias in dynamic recommen-
dation. We first conduct an empirical study by simulation experi-
ments to show how the bias evolves in the dynamic process and the
impacts of four bias factors on the bias. Then, we propose to dynam-
ically debias and also propose the FPC method to debias utilizing
false positive signals. Last, by extensive experiments, we empiri-
cally validate the effectiveness of the proposed dynamic debiasing
strategy and the proposed FPC algorithm.Opportunity Bias in Collaborative Filtering. In WSDM .Wikipedia is a collective intelligence platform that helps contributors
to collaborate efficiently for creating and disseminating knowledge
and content. A key guiding principle of Wikipedia is to maintain a
neutral point of view (NPOV), which can be challenging for new
contributors and experienced editors alike. Hence, several previous
studies have proposed automated systems to detect biased statements
on Wikipedia with mixed results. In this paper, we investigate the
potential of cross-domain pre-training to learn bias features from
multiple sources, including Wikipedia, news articles, and ideological
statements from political figures in an effort to learn richer cross-
domain indicators of bias that may be missed by existing methods.
Concretely, we study the effectiveness of bias detection via cross-
domain pre-training of deep transformer models. We find that the
cross-domain bias classifier with continually pre-trained RoBERTa
model achieves a precision of 89% with an F1 score of 87%, and
can detect subtle forms of bias with higher accuracy than existing
methods.
CCS CONCEPTS
•Computing methodologies →Natural language processing ;•
Information systems →Clustering and classification ;•Human-
centered computing →Collaborative and social computing .
KEYWORDS
cross-domain datasets, neural networks, linguistic bias detection,
text tagging, adaptive pre-training, transformer-based modelsJamie Brew. 2020. Pretrained BERT Models. In many personalized recommendation scenarios, the generaliza-
tion ability of a target task can be improved via learning with
additional auxiliary tasks alongside this target task on a multi-
task network. However, this method often suffers from a serious
optimization imbalance problem. On the one hand, one or more aux-
iliary tasks might have a larger influence than the target task and
even dominate the network weights, resulting in worse recommen-
dation accuracy for the target task. On the other hand, the influence
of one or more auxiliary tasks might be too weak to assist the target
task. More challenging is that this imbalance dynamically changes
throughout the training process and varies across the parts of the
same network. We propose a new method: MetaBalance to balance
auxiliary losses via directly manipulating their gradients w.r.t the
shared parameters in the multi-task network. Specifically, in each
training iteration and adaptively for each part of the network, the
gradient of an auxiliary loss is carefully reduced or enlarged to
have a closer magnitude to the gradient of the target loss, prevent-
ing auxiliary tasks from being so strong that dominate the target
task or too weak to help the target task. Moreover, the proximity
between the gradient magnitudes can be flexibly adjusted to adapt
MetaBalance to different scenarios. The experiments show that our
proposed method achieves a significant improvement of 8.34% in
terms of NDCG@10 upon the strongest baseline on two real-world
datasets. The code of our approach can be found at here.1
CCS CONCEPTS
•Computing methodologies →Multi-task learning ;•Infor-
mation systems→Recommender systems .
1
*A majority of this work was done while the first author was interning at Meta AI.
This work is licensed under a Creative Commons Attribution International
4.0 License.
WWW ’22, April 25–29, 2022, Virtual Event, Lyon, France
©2022 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-9096-5/22/04.

Multi-Task Learning, Auxiliary Learning, Personalized Recommen-
dation, Gradient-based Optimization2215In collaborative filtering, the quality of recommendations critically
relies on how easily a model can find similar users for a target user.
Hence, a niche user who prefers items out of the mainstream may
receive poor recommendations, while a mainstream user sharing
interests with many others will likely receive recommendations
of higher quality. In this work, we study this mainstream bias cen-
tering around three key thrusts. First, to distinguish mainstream
and niche users, we explore four approaches based on outlier de-
tection techniques to identify a mainstream score indicating the
mainstream level for each user. Second, we empirically show that se-
vere mainstream bias is produced by conventional recommendation
models. Last, we explore both global and local methods to mitigate
the bias. Concretely, we propose two global models: Distribution
Calibration (DC) and Weighted Loss (WL) methods; and one local
method: Local Fine Tuning (LFT) method. Extensive experiments
show the effectiveness of the proposed methods to improve utility
for niche users and also show that the proposed LFT can improve
the utility for mainstream users at the same time.
CCS CONCEPTS
•Information systems →Recommender systems.
KEYWORDS
recommender systems; mainstream bias; local models1506—Social Resonance is a common socio-behavioral
phenomenon in which users are more inﬂuenced by opinions
that have similar vibes. That is, opinions from two different
groups of users can mutually reinforce (or resonate with) each
other to have an even stronger impact on the user. In this
paper, we explore the powerful social resonance effect between
social connections and other users in an eCommerce platform to
improve recommendation. Speciﬁcally, we ﬁrst formulate an item-
aware user inﬂuence network that connects users who rate the
same item. With the social network and item-aware user inﬂuence
network, a novel graph-based mutual learning framework is
proposed, which captures the resonance inﬂuence from both
user local correlations and global connections. We then fuse
these inﬂuence paths to predict the resonance-enhanced user167Popularity bias is a long-standing challenge in recommender sys-
tems: popular items are overly recommended at the expense of
less popular items that users may be interested in being under-
recommended. Such a bias exerts detrimental impact on both users
and item providers, and many efforts have been dedicated to study-
ing and solving such a bias. However, most existing works situate
the popularity bias in a static setting, where the bias is analyzed
only for a single round of recommendation with logged data. These
works fail to take account of the dynamic nature of real-world
recommendation process, leaving several important research ques-
tions unanswered: how does the popularity bias evolve in a dynamic
scenario? what are the impacts of unique factors in a dynamic rec-
ommendation process on the bias? and how to debias in this long-
term dynamic process? In this work, we investigate the popularity
bias in dynamic recommendation and aim to tackle these research
gaps. Concretely, we conduct an empirical study by simulation ex-
periments to analyze popularity bias in the dynamic scenario and
propose a dynamic debiasing strategy and a novel False Positive
Correction method utilizing false positive signals to debias, which
show effective performance in extensive experiments.
CCS CONCEPTS
•Information systems →Recommender systems.
KEYWORDS
popularity bias; dynamic recommendation2449A fundamental challenge for sequential recommenders is to capture
the sequential patterns of users toward modeling how users transit
among items. In many practical scenarios, however, there are a great
number of cold-start users with only minimal logged interactions.
As a result, existing sequential recommendation models will lose
their predictive power due to the difficulties in learning sequential
patterns over users with only limited interactions. In this work,
we aim to improve sequential recommendation for cold-start users
with a novel framework named MetaTL, which learns to model the
transition patterns of users through meta-learning. Specifically, the
proposed MetaTL: (i) formulates sequential recommendation for
cold-start users as a few-shot learning problem; (ii) extracts the
dynamic transition patterns among users with a translation-based
architecture; and (iii) adopts meta transitional learning to enable
fast learning for cold-start users with only limited interactions,
leading to accurate inference of sequential interactions.
CCS CONCEPTS
•Information systems →Recommender systems ;
KEYWORDS
Recommendation Systems, Cold-start, Meta-learningMixture-of-Experts Transformation. In SIGIR .This paper investigates recommendation fairness among new items.
While previous efforts have studied fairness in recommender sys-
tems and shown success in improving fairness, they mainly focus on
scenarios where unfairness arises due to biased prior user-feedback
history (like clicks or views). Yet, it is unknown whether new items
without any feedback history can be recommended fairly, and if
unfairness does exist, how can we provide fair recommendations
among these new items in such a cold-start scenario. In detail,
we first formalize fairness among new items with the well-known
concepts of equal opportunity andRawlsian Max-Min fairness. We
empirically show the prevalence of unfairness in cold start recom-
mender systems. Then we propose a novel learnable post-processing
framework as a model blueprint for enhancing fairness, with which
we propose two concrete models: a joint-learning generative model,
and a score scaling model. Extensive experiments over four public
datasets show the effectiveness of the proposed models for enhanc-
ing fairness while also preserving recommendation utility.
CCS CONCEPTS
•Information systems →Recommender systems.
KEYWORDS
fairness; cold start recommendation776Fake reviews and review manipulation are
growing problems on online marketplaces
globally. Review Hijacking is a new review
manipulation tactic in which unethical sellers
“hijack” an existing product page (usually one
with many positive reviews), then update the
product details like title, photo, and descrip-
tion with those of an entirely different product.
With the earlier reviews still attached, the new
item appears well-reviewed. However, there
are no public datasets of review hijacking and
little is known in the literature about this tactic.
Hence, this paper proposes a three-part study:
(i) we propose a framework to generate syn-
thetically labeled data for review hijacking by
swapping products and reviews; (ii) then, we
evaluate the potential of both a Twin LSTM
network and BERT sequence pair classiﬁer to
distinguish legitimate reviews from hijacked
ones using this data; and (iii) we then deploy
the best performing model on a collection of
31K products (with 6.5 M reviews) in the orig-
inal data, where we ﬁnd 100s of previously un-
known examples of review hijacking.
Reviews are an essential component of many on-
line marketplaces, helping new consumers assess
product quality, legitimacy, and reliability. Recent
surveys indicate that an overwhelming majority of
people read reviews . Indeed, 79%
of people overall and 91% of people ages 18-34
trust online reviews as much as personal recom-
mendations . Naturally, reviews
have become a target of manipulation, misuse, and
abuse .
In this paper, we focus on the problem of review
hijacking , a relatively new attack vector and one
that has received little, if any, research attention.
Review hijacking is a fraud technique wherein ablackhat seller “hijacks” a product page that typ-
ically has already accumulated many positive re-
views and then replaces the hijacked product with
a different product (typically one without any pos-
itive reviews). The sellers then reap the ratings
“halo” from consumers who assume the new prod-
uct is highly rated. This review hijacking (also
referred to as review reuse orbait-and-switch re-
views ) provides the sellers with a shortcut to many
undeserved positive reviews.
Figure 1: An example of review hijacking on Amazon

Figure 2: Hijacked reviews associated with the hair re-
moval product in Figure 1arXiv:2107.05385v1  [cs.CL]  7 Jul 2021An example is shown in Figure 1 which we dis-
covered in the ﬁrst week of May 2021. This hair
removal product has 4,069 reviews with an average
rating of close to ﬁve stars. On inspection of the
reviews (see Figure 2), we ﬁnd many that refer to
other products like dishwasher cleaners and dia-
pers. Also, these reviews are for veriﬁed purchases
which can provide added weight to the ostensible
veracity of the reviews.
We have identiﬁed at least three different meth-
ods that blackhat sellers adopt to conduct review
hijacking depending on the particular e-commerce
platform. A seller can incrementally change as-
pects of their own product (like the title, photo,
description), resulting in an entirely new product,
though still associated with the original reviews.
Alternatively, a seller can add his product as a prod-
uct variation of some other product to aggregate re-
views from the former product. One can also merge
reviews from some other products to their own by
changing country or using some other platform-
speciﬁc loopholes.
While review hijacking has been recognized in
the press and social media as a growing problem,
e.g., (Swearingen, 2019; Walsh, 2020; Sterling,
2019; Dascalescu, 2019; Nguyen, 2018; Dzieza,
2019), there has been no structured research to date
on identifying review hijacking. We attribute this
to several key challenges:
•First, there are no standard datasets of review
hijacking, nor are there gold labels of known
examples. Hence, it is challenging to validate
models that aim to uncover review hijacking.
•Second, review hijacking is a targeted attack
vector with a skewed distribution, and so there
are no simple approaches to ﬁnd examples.
In a preliminary investigation, we manually
labeled hundreds of reviews and found fewer
than 0.01% reviews that could be considered
part of a review hijacking attack.
•Third, many reviews cannot easily be labeled
as hijacked or not. For example, reviews like
“Great product! Five stars!” are generic and
could potentially be associated with any prod-
uct.
•Finally, hijackers may adopt sophisticated
techniques to avoid detection. For example,
some products may have a mix of legitimate
reviews to camouﬂage the hijacked ones (e.g.,by incentivizing reviewers to contribute a re-
view about the hair removal product).
Hence, this paper proposes an initial investiga-
tion into the potential of identifying review hijack-
ing. We conduct a three-part study. Due to the
challenges of ﬁnding high-quality examples of re-
view hijacking, we ﬁrst propose a framework to
generate synthetic examples of review hijacking
by swapping products and reviews. We do so both
at the inter-category level (where presumably it
should be easier to determine if a review is associ-
ated with a product) and at the intra-category level
(where product similarity within the category may
make this more challenging). Over this synthetic
dataset, we evaluate the potential of both a Twin
LSTM network and BERT sequence pair classi-
ﬁer to distinguish legitimate reviews from hijacked
ones. Based on the encouraging results from this
experiment, we then deploy the BERT sequence
pair classiﬁer algorithm on a real collection of 31K
products (with 6.5 M reviews). By averaging the
review scores from the classiﬁer for each product,
we ﬁnd that products with an average review score
(orsuspiciousness score) >0.5have 99.95% of the
listings containing unrelated or hijacked reviews.
These ﬁndings suggest the promise of large-scale
detection of review hijacking in the wild.
2 Related Work
The manipulation of reviews and review platforms
has been widely studied, e.g., (G ¨ossling et al., 2018;
Jindal and Liu, 2007; Kaghazgaran et al., 2017;
Mukherjee et al., 2012, 2013), though there is little
research literature on the problem of review hijack-
ing. Here, we highlight several efforts related to
the methods proposed in this paper. Higgins et
al. developed models for an essay rating system
to detect bad-faith essays by comparing the essay
titles to the essay text to determine whether the
title and text were in agreement through the use of
word similarity . A similar
idea motivates our approach that compares product
titles/descriptions with review text. Louis and Hig-
gins continued this line of research to determine
whether a particular essay was related to the essay
prompt or question by expanding short prompts and
spell correcting the texts .
Rei and Cummins extended this work and com-
bined various sentence similarity measures like TF-
IDF and Word2Vec embeddings with moderate im-
provement over Higgins’ work . Apart from the essay space, Ryu et al. inves-
tigated the detection of out-of-domain sentences
. They proposed a neural sen-
tence embedding method representing sentences in
a low-dimensional continuous vector space that em-
phasizes aspects in-domain and out-of-domain for
a given scenario. In another direction, fake news
detection and clickbait detection can be viewed as
related tasks. For example, Hanselowski et al.used
a BiLSTM model with attention to determine if
the headline of a news article agrees, disagrees,
or is unrelated to the text as part of a Fake News
Challenge .
3 Generating Synthetic Examples of
Review Hijacking
In our preliminary investigation, we examined hun-
dreds of reviews from the Amazon dataset provided
by McAuley . The dataset contains
233.1 million reviews from May 1996 to October
2018, with reviews and product information includ-
ing title, description, etc. However, we ﬁnd very
few examples of review hijacking. Hence, we con-
cluded that hiring crowd labelers or subject matter
experts to label product-review pairs as hijacked
or not hijacked might not be fruitful. Instead, we
propose a method to generate synthetic examples
for studying the potential of models to identify hi-
jacked reviews.
3.1 Preliminaries
As a ﬁrst step, we prepared the Amazon dataset.
For each product i, we combined the description
(product text provided by the seller), title (the name
of the product), the brand of the product, and fea-
tures (product features like color or size) into a
single product text Pi. We also removed products
with fewer than ﬁve reviews.
For each review j, we combined the reviewText
(the text in the review body), the style (which con-
tains some optional product features like color or
size), and summary (which is the headline of the
review) into a single review text Rj.
Hence, our goal is to determine if each review
jassociated with the product i, is actually related
to the product or not. If the review is unrelated,
we can conclude that there is potential evidence
of review hijacking for the product. Of course,
there could be other reasons for a review for being
unrelated to a product, like an error by the reviewer.
We leave this ﬁne-grained determination as futurework.
3.2 Swapping Reviews
Given these products and reviews, we propose to
randomly swap reviews between a pair of distinct
products, yielding a collection of unrelated product-
review pairs. As a ﬁrst step, we assume that all
reviews are actually related to the associated prod-
uct. Hence, we have a large set of product-review
pairs with the label related (= 0). Of course, we
know that our data has some hijacked reviews (on
the order of <0.01%), so we will tolerate some
errors in these labels.
By randomly swapping product-review pairs, we
get a set of product-review pairs with the label
unrelated (= 1). For example, Figure 3 shows a
simple example of a basketball and a phone, each
with an associated review. We swap reviews among
the products to generate unrelated (= 1) labels in
addition to the original related (= 0) labels.
Figure 3: Generation of Synthetic Label and Data by
Swapping Reviews among Dissimilar Products
But, how do we select which products to select
for randomly swapping reviews? Randomly select-
ing products may lead to such a clear mismatch
between the review text and the product text that
detection would be trivial. On the other hand, se-
lecting closely related products (e.g., by selecting
Samsung mobile covers from two different brands)
may yield reviews that are essentially undetectable
as possible hijacking. Hence, we propose two meth-
ods for ﬁnding pairs of dissimilar products for re-view swapping.
Inter-Category Swapping. The ﬁrst approach
takes a product text Picomposed of the title, fea-
tures, and description from one category (e.g.,
Beauty, Clothing, Electronics) and the review texts
Rjof a product in another category for unrelated
reviews. For related reviews, we take the original
product-review pairs. We obtained a set of ≈59k
reviews with ≈25kunrelated reviews and ≈34k
related reviews.
Intra-category Swapping. The ﬁrst approach han-
dles hijacking across categories. For hijacking oc-
curring within a product category, we use Jaccard
distance. We converted product titles for each prod-
uct into TF-IDF feature matrices, found pairwise
Jaccard distances between them, and we formed
product pairs  with Jaccard distance 0.
Then, we took the product text Piof one prod-
uctA1, and the review text Rjof another product
A2and labeled this as unrelated . Similarly, we
took the product text of A2and a review of A1as
unrelated . For related labels, we took the product
text, and the review text of A1, and likewise for A2
to get another set of related data. We obtained a set
of≈56kreviews with ≈22kunrelated reviews
and≈34krelated reviews.
4 Identifying Synthetic Examples
Given these synthetic datasets of hijacked reviews,
can we detect them? In this section, we report on
experiments with two approaches: one based on a
Twin LSTM and one based on BERT Sentence Pair
Classiﬁcation.
We shufﬂed the product-review pairs and split
them into training, validation and test set in ratio
70:10:20 for both of the datasets. The actual num-
ber of reviews in each set depends on the swapping
categories and is discussed in Section 3.2 We train
on the train set, tune models on the validation set,
and have reported results on the test set.
4.1 Twin LSTM Network
The ﬁrst approach adopts a Twin neural network
which has shown success in comparing images
and text. This network uses the same weights in
parallel in tandem on two inputs to return output
based on the relation or distance between them
. Concretely, we compare sentence
pairs and determine if they are similar or not. We
tokenized our inputs and converted them into se-
quences. Then we used 300-dimensional GloVe embeddings and formed
an embedding matrix for our tokens. We get two
embedding matrices for both inputs, which we feed
into the LSTM network illustrated in Figure 4. We
use twin LSTM networks with two layers of 64
nodes each, with a dropout of 0.01. We calculate
the cosine similarity between the two input embed-
dings and evaluate the performance by computing
cross-entropy loss using accuracy and AUC (Area
Under Curve). It takes 13 epochs with Adam opti-
mizer and learning rate of 0.00001 to get the result.
Figure 4: Twin LSTM Network
4.2 BERT Sequence Pair Classiﬁer
The second approach adopts the popular BERT pre-
trained language model . Since
BERT provides a deep bidirectional representation,
conditioned on text in both directions, we expect
this method to perform better than the twin neural
network, which uses GLOVE embeddings. Our
model is prepared from the BERT BASE model
(bert 1276812) from GluonNLP. We add a layer
on top for classiﬁcation, as shown in Figure 5. We
use Adam optimizer for optimizing this classiﬁca-
tion layer and get results with only 3 epochs.
Now we form the sentence pairs for classiﬁca-
tion. Like the previous method, the ﬁrst sentence is
the product text Pi(a concatenation of product title,
features, and description). The second sentence is
the review text Rj(a concatenation of the review
summary and review text). We then tokenize the
sentences, insert [CLS] at the start, insert [SEP] at
the end and between both the sentences, and gener-
ate segment ids to specify if a token belongs to the
ﬁrst sentence or the second one. We now run the
BERT ﬁne-tuning with these sequences as inputs.
We get the output as an unrelated score ube-
tween 0 and 1. For texts longer than 512 tokens,
we truncate and take the ﬁrst 512 tokens for our
model. As 99% of the review texts have fewer than512 tokens, this choice impacts very few reviews.
Figure 5: BERT Sequence Pair Classiﬁer
4.3 Results
Table 1 shows the results reported on test data us-
ing these two approaches. We see that the Twin
LSTM Network provides more than 80% accuracy
and high ROC result on both inter-category and
intra-category datasets. The BERT-based classiﬁer
has more than 90% accuracy and ROC result for
both datasets. We see that both methods perform
better on the inter-category dataset than the intra-
category one. In the inter-category dataset, we
obtain unrelated reviews by taking products from
one category and review texts from another. Hence,
models trained on this dataset can learn product
features of one category at a time and develop ex-
pertise in that category. The intra-category dataset
is more challenging for both approaches. Since
products are drawn from the same category, there
can be less clarity in distinguishing features of the
reviews.
Paired with this summary table (Table 1), we
show in Figures 6, 7, 8 and 9 the ROC curve for
the BERT-based model and Twin LSTM network.
We can clearly see that BERT-based model per-
forms better than LSTM. We can also see how both
models perform better on the inter-category dataset
rather than the intra-category one.
5 Detecting Hijacked Reviews
In-the-Wild
Even though encouraging, these results are on syn-
thetic data, and the data itself may contain noisy
labels. Hence, we next turn to the task of uncov-
Figure 6: ROC curve for Twin LSTM network run on
Intra-category data (Jaccard distance)
Figure 7: ROC curve for BERT seq. pair classiﬁer run
on Intra-category data (Jaccard distance)
Figure 8: ROC curve for Twin LSTM network run on
Inter-category data
Figure 9: ROC curve for BERT seq. pair classiﬁer run
on Inter-category dataModel Synthetic Data Accuracy ROC result
Twin LSTM Network Intra-category 0.823 0.770
Inter-category 0.885 0.910
BERT Sequence Pair Classiﬁer Intra-category 0.916 0.948
Inter-category 0.965 0.993
Table 1: Hijacked review detection accuracy reported on the test set
ering hijacked reviews in-the-wild. Can a model
trained over synthetic data identify actual hijacked
reviews?
5.1 Approach and Results
For this experiment, we used the BERT sequence
pair classiﬁer and applied it to a dataset of 31K
products (with 6.5 M reviews) with the original
product-review pairs intact. These 31K products
were held out and not used during the training.
For each product-review pair, we take the un-
related score output from the trained BERT-based
model as u. For a product iwithnreviews, we
calculate an average suspiciousness review score
as follows:
score i=∑n
j=1u
n
Based on this suspiciousness score, we plot the
distribution of all 31K products in Figure 10. Un-
surprisingly, the vast majority of products have
a very low suspiciousness score. About 99% of
products have scored <0.3, reinforcing our ini-
tial assumption about a skewed class distribution.
In other words, the vast majority of the reviews
on listings seem to be related to the product itself.
However, we ﬁnd many cases of potential review
hijacking (see the right side of Figure 10), indicat-
ing that this targeted attack is indeed a threat to
review platforms.
We manually checked a sample of 200+ prod-
ucts with a suspiciousness score of >0.5. We
found that all but one of the products contained re-
views referring to a different product. While there
is uncertainty as to the mechanism leading to an
unrelated review, we hypothesize that these are in-
deed previously unknown cases of review hijacking.
And in an encouraging direction, these results indi-
cate the promise of training models over synthetic
hijacked reviews for uncovering actual instances.
5.2 Case Study
In this section, we discuss three sample products
three sample products and their distribution of unre-lated scores uthat are assigned by the BERT-
based model. These three products are from the
Cellphone & Accessories category.
Figure 11 shows the unrelated score distribution
for all of the reviews of product-1. Product-1 has
an average unrelated review score of 0.9to1.0.
We can see from the distribution that most reviews
have a high unrelated score (>0.9). We manu-
ally inspect these reviews and observe that these
reviews are indeed unrelated. Hence, we conclude
that this product is an example of review hijacking.
Figure 12 shows the unrelated score distribution
for product-2. Product-2 has an average review
score of 0.0to0.1, meaning most of the reviews
seem appropriate. We can see from the distribution
that most reviews have a low unrelated score (<
0.1), and a few have a high score (>0.9). We
manually inspect the reviews with high unrelated
scores (>0.9)and observe that these reviews are
either misclassiﬁed by our BERT-based model or
do not have enough information to determine the
label . Thus,
we conclude that this product is not an example of
review hijacking.
Finally, Figure 13 shows the unrelated review
score distribution for product-3. Product-3 has an
average review score of 0.5to0.6. We can see from
the distribution that about 55% of the reviews have
a high unrelated score, while 35% reviews have a
low unrelated score. We manually inspect reviews
with high unrelated scores (>0.9)and observe that
these reviews are indeed unrelated to the product.
We also inspect the reviews with low unrelated
scores ( <0.1and<0.2) and observe that most are
related to the product. As this product has a mix of
related and unrelated reviews, we hypothesize that
it is also an example of review hijacking containing
some related reviews.
6 Conclusion, Limitations and Next
Steps
This paper has examined the challenge of identify-
ing hijacked reviews. Since we know little aboutFigure 10: Average Review Score vs. Number of Products
Figure 11: Unrelated Review Score Distribution for
Product-1 showing predominantly unrelated reviews
Figure 12: Unrelated Review Score Distribution for
Product-2 showing predominantly related reviews
these hijacked reviews, we ﬁrst proposed to gen-
erate synthetic examples by swapping the reviews
of a product with reviews on an unrelated prod-
uct. We then tested the viability of a Twin LSTM
network and BERT sentence pair classiﬁer to un-
cover these unrelated reviews. Both approaches
provided excellent results on synthetic data, but
do they actually identify hijacked reviews in the
wild? Our preliminary investigation showed that
a model trained over synthetic data could detect
many examples of previously unknown cases of
review hijacking.
Figure 13: Unrelated Review Score Distribution for
Product-3 showing a roughly equal mix of related and
unrelated reviews
Our method also has some limitations. First, the
major drawback occurs because the data is labeled
synthetically. Hence, there is no way to ﬁnd the
actual recall for our approach. Calculating recall re-
quires manual labeling of all product-review pairs,
which is an expensive process. Second, our method
is dependent on the accuracy of labeling methods.
For the intra-category case, our method cannot de-
tect products hijacked with similar wording in the
same category since their Jaccard distance is low.
For example, if there are two products, “iPhone
X” and “iPhone 5C cover”, the products will have
a low Jaccard distance, and the reviews hijacked
among them cannot be labeled correctly. There-
fore, our ML model can also not learn this kind
of review hijacking. Third, generic reviews like
“Good product!” and “Product shipped fast” were
labeled hijacked and not hijacked depending on
what product they belonged to. Ideally, we would
want to label all of them as not hijacked. This
random labeling adds to the noise in the labels.
In our continuing work, we are interested in two
main directions: data and methods. From a data per-spective, we are investigating more reﬁned methods
to generate synthetic labels. Can we couple crowd
labelers with our swapping approach to construct
better product-review pairs? We are also interested
in updating the data itself. Our dataset covers re-
views up to 2018, though many media reports of
review hijacking were not until 2019. There could
have been a rise in review hijacking that is not
as prominent in our data. From a methods per-
spective, we have focused purely on text-based
signals. Incorporating image-based features like
from the product itself and user-submitted images
could help identify examples of review hijacking.
We are also interested in adopting recent advances
in pre-trained language models like T5, DeBERTa,
and RoBERTa. We are also focusing on using e-
commerce speciﬁc text (like product catalog data)
to instill domain-speciﬁc knowledge during the pre-
training of language models versus BooksCorpus
and English Wikipedia used in BERT.ucts. Which.
Table 2: Statistics of the evaluation datasets.
Datasets # nodes # edges # attributes # Train/Valid/Test
Amazon 42,318 43,556 8,669 90/37/40
DBLP 40,672 288,270 7,202 80/27/30
ogbn-arxiv 169,343 1,166,243 128 16/12/12
Label Noise Injection
To enable our experiments on few-shot node classiﬁcation
with weakly-labeled data, we need to inject label noise to the
training data. We focus on two representative types of label
noise: symmetric and asymmetric noise (Chen et al. 2019).
In fact, the noise injection can be done by ﬂipping the labels
following the transition probabilities deﬁned in a corruption
matrixT, in whichTijdenotes the probability of ﬂipping
classcito classcj. In Figure 5, we visualize the examples of
corruption matrix for the dataset containing 5 label classes.
For injecting noise of ratio ϵto a dataset with Pclasses:
•Symmetric noise ﬂips a label uniformly to all the other
classes, s.t.Tii= 1−ϵandTij=ϵ/(P−1)ifi̸=j.
•Asymmetric noise ﬂips a label to a different class with
probabilityϵ, s.t.Tii= 1−ϵand∃i̸=j,Tij=ϵ.
2
3
4 of Baselines
We randomly sampled 100 meta-test tasks from the test node
classes and evaluate both Meta-GIN and the baseline meth-
ods on these tasks. The process is repeated for 10 times to
obtain the reported results in the paper. We test all the base-
line methods with the publicly released implementations. In
the experiments, we ﬁne-tune the hyperparameters to report
their best performance.
•GCN. It utilizes two graph convolutional layers (32, N di-
mensions) to learn the node representations for N-way-
K-shot tasks.
•SGC. This linear model reduces the unnecessary com-
plexity of GCN by successively collapsing the convolution
functions between consecutive layers into a linear trans-
formation. After the feature pre-processing step, it learns
the node representations with 2-layer feature propagation.
•GraphSAGE. It can efﬁciently generate node embeddings
by uniformly sampling a ﬁxed size of neighbors and then
aggregating the feature information from the neighbors.
We set the search depth to 2 and the neighborhood sample
size to 35 for both layers. ReLu function is used for non-
linearity and the mean-pooling aggregator is selected for
comparison. Two layers  are used for
learning node representations.
•PTA. It is a decoupled GNN which is robust to label noise
using adaptive weighting strategy. As suggested in the
original paper, we use K= 10 propagation steps and set
the optimal teleport probability α= 0.1. For fair compar-
ison, it employs a two-layer  MLP for
representation learning.
•Meta-GNN. It applies MAML (Finn, Abbeel, and Levine
2017) to Simple Graph Convolution (SGC) for few-shot
node classiﬁcation in graphs. A 2-layer SGC is used for
network embedding. Each batch contains 5 tasks. As sug-
gested in the original paper, we set task-learning rate
α1= 0.5and meta-learning rate α2= 0.003.
•GPN. This Graph Prototypical Network can learn highly
representative class prototypes with a GNN-based net-
work encoder and node valuator. It predicts node labels
by measuring their similarity with prototypes. It employs
a 2-layer  GCN as network encoder.
The node valuator relies on two score aggregation layers.
We use the optimal learning rate α= 0.005.
•G-Meta. It constructs a local subgraph for each node and
regards the centroid embedding of subgraphs as the proto-
types. It is optimized with both the prototypical loss and
MAML. It takes the 2-hop neighbors into consideration
and employs two graph convolutional aggregation layers
 for representation learning. We select
the optimal update learning rate α= 0.01meta learning
rateβ= 0.001.
•Meta-GPS. This graph Meta-learning framework is based
on Prototype and Scaling & shifting transformation to
obtain highly transferable meta-knowledge from meta-
training tasks. It employs a 2-layer 
network encoder. We use the optimal step size α= 0.5,
meta-learning rate β= 0.001and regularization coefﬁ-
cientγ= 0.001.c1c2c3c4c5
c1
c2
c3
c4
c50.700 0.075 0.075 0.075 0.075
0.075 0.700 0.075 0.075 0.075
0.075 0.075 0.700 0.075 0.075
0.075 0.075 0.075 0.700 0.075
0.075 0.075 0.075 0.075 0.700(a)Sym Noise ( ϵ= 0.3)
c1c2c3c4c5
c1
c2
c3
c4
c50.700 0.000 0.000 0.300 0.000
0.300 0.700 0.000 0.000 0.000
0.000 0.300 0.700 0.000 0.000
0.000 0.000 0.000 0.700 0.300
0.300 0.000 0.000 0.000 0.700 (b)Asym Noise ( ϵ= 0.3)
Figure 5: Example of the noise corruption matrix.
Model Implementation. The proposed model is imple-
mented in PyTorch. Speciﬁcally, we employ a 2-layer propa-
gation SGC for the node representation learning module. As
for the node interpolation module, we use one aggregation
layer and the negative slope for the LeakyReLU in it is set to
be0.2. We grid search for task numbers in {1, 5, 10, 15, 20,
25}, meta learning rate αand meta step size βin{0.0001 ,
0.0005 ,0.001,0.005,0.01,0.05,0.1,0.5}. The optimal val-ues are selected when the model achieve the best perfor-
mance for validation set. We select the meta-learning rate
αto be 0.1and the meta step size βto be 0.001. For model
training, the task number in each batch is 5and the query
sizeK′is5. For constructing the meta-training episodes, un-
less otherwise notice, we let the set size Mto be 5for both
the support and query set. We train the model with 20,000
episodes or stop earlier when the performance on validation
set converges. In the testing phase, we feed the N-wayK-
shot support set from unseen classes to both Meta-GIN and
the baseline models for fair comparison.
Packages Used for Implementation. For reproducibility,
we also list the packages we use in the implementation with
their corresponding versions:
• python==3.7.9
• pytorch==1.4.0
• cuda==10.1
• numpy==1.19.2
• scipy==1.6.0
• scikit-learn==0.24.0
Table 2: Statistics of the evaluation datasets.
Datasets # nodes # edges # attributes # Train/Valid/Test
Amazon 42,318 43,556 8,669 90/37/40
DBLP 40,672 288,270 7,202 80/27/30
ogbn-arxiv 169,343 1,166,243 128 16/12/12
Label Noise Injection
To enable our experiments on few-shot node classiﬁcation
with weakly-labeled data, we need to inject label noise to the
training data. We focus on two representative types of label
noise: symmetric and asymmetric noise (Chen et al. 2019).
In fact, the noise injection can be done by ﬂipping the labels
following the transition probabilities deﬁned in a corruption
matrixT, in whichTijdenotes the probability of ﬂipping
classcito classcj. In Figure 5, we visualize the examples of
corruption matrix for the dataset containing 5 label classes.
For injecting noise of ratio ϵto a dataset with Pclasses:
•Symmetric noise ﬂips a label uniformly to all the other
classes, s.t.Tii= 1−ϵandTij=ϵ/(P−1)ifi̸=j.
•Asymmetric noise ﬂips a label to a different class with
probabilityϵ, s.t.Tii= 1−ϵand∃i̸=j,Tij=ϵ.
2
3
4 of Baselines
We randomly sampled 100 meta-test tasks from the test node
classes and evaluate both Meta-GIN and the baseline meth-
ods on these tasks. The process is repeated for 10 times to
obtain the reported results in the paper. We test all the base-
line methods with the publicly released implementations. In
the experiments, we ﬁne-tune the hyperparameters to report
their best performance.
•GCN. It utilizes two graph convolutional layers (32, N di-
mensions) to learn the node representations for N-way-
K-shot tasks.
•SGC. This linear model reduces the unnecessary com-
plexity of GCN by successively collapsing the convolution
functions between consecutive layers into a linear trans-
formation. After the feature pre-processing step, it learns
the node representations with 2-layer feature propagation.
•GraphSAGE. It can efﬁciently generate node embeddings
by uniformly sampling a ﬁxed size of neighbors and then
aggregating the feature information from the neighbors.
We set the search depth to 2 and the neighborhood sample
size to 35 for both layers. ReLu function is used for non-
linearity and the mean-pooling aggregator is selected for
comparison. Two layers  are used for
learning node representations.
•PTA. It is a decoupled GNN which is robust to label noise
using adaptive weighting strategy. As suggested in the
original paper, we use K= 10 propagation steps and set
the optimal teleport probability α= 0.1. For fair compar-
ison, it employs a two-layer  MLP for
representation learning.
•Meta-GNN. It applies MAML (Finn, Abbeel, and Levine
2017) to Simple Graph Convolution (SGC) for few-shot
node classiﬁcation in graphs. A 2-layer SGC is used for
network embedding. Each batch contains 5 tasks. As sug-
gested in the original paper, we set task-learning rate
α1= 0.5and meta-learning rate α2= 0.003.
•GPN. This Graph Prototypical Network can learn highly
representative class prototypes with a GNN-based net-
work encoder and node valuator. It predicts node labels
by measuring their similarity with prototypes. It employs
a 2-layer  GCN as network encoder.
The node valuator relies on two score aggregation layers.
We use the optimal learning rate α= 0.005.
•G-Meta. It constructs a local subgraph for each node and
regards the centroid embedding of subgraphs as the proto-
types. It is optimized with both the prototypical loss and
MAML. It takes the 2-hop neighbors into consideration
and employs two graph convolutional aggregation layers
 for representation learning. We select
the optimal update learning rate α= 0.01meta learning
rateβ= 0.001.
•Meta-GPS. This graph Meta-learning framework is based
on Prototype and Scaling & shifting transformation to
obtain highly transferable meta-knowledge from meta-
training tasks. It employs a 2-layer 
network encoder. We use the optimal step size α= 0.5,
meta-learning rate β= 0.001and regularization coefﬁ-
cientγ= 0.001.c1c2c3c4c5
c1
c2
c3
c4
c50.700 0.075 0.075 0.075 0.075
0.075 0.700 0.075 0.075 0.075
0.075 0.075 0.700 0.075 0.075
0.075 0.075 0.075 0.700 0.075
0.075 0.075 0.075 0.075 0.700(a)Sym Noise ( ϵ= 0.3)
c1c2c3c4c5
c1
c2
c3
c4
c50.700 0.000 0.000 0.300 0.000
0.300 0.700 0.000 0.000 0.000
0.000 0.300 0.700 0.000 0.000
0.000 0.000 0.000 0.700 0.300
0.300 0.000 0.000 0.000 0.700 (b)Asym Noise ( ϵ= 0.3)
Figure 5: Example of the noise corruption matrix.
Model Implementation. The proposed model is imple-
mented in PyTorch. Speciﬁcally, we employ a 2-layer propa-
gation SGC for the node representation learning module. As
for the node interpolation module, we use one aggregation
layer and the negative slope for the LeakyReLU in it is set to
be0.2. We grid search for task numbers in {1, 5, 10, 15, 20,
25}, meta learning rate αand meta step size βin{0.0001 ,
0.0005 ,0.001,0.005,0.01,0.05,0.1,0.5}. The optimal val-ues are selected when the model achieve the best perfor-
mance for validation set. We select the meta-learning rate
αto be 0.1and the meta step size βto be 0.001. For model
training, the task number in each batch is 5and the query
sizeK′is5. For constructing the meta-training episodes, un-
less otherwise notice, we let the set size Mto be 5for both
the support and query set. We train the model with 20,000
episodes or stop earlier when the performance on validation
set converges. In the testing phase, we feed the N-wayK-
shot support set from unseen classes to both Meta-GIN and
the baseline models for fair comparison.
Packages Used for Implementation. For reproducibility,
we also list the packages we use in the implementation with
their corresponding versions:
• python==3.7.9
• pytorch==1.4.0
• cuda==10.1
• numpy==1.19.2
• scipy==1.6.0
• scikit-learn==0.24.0Wikipedia is a critical platform for organizing and disseminating
knowledge. One of the key principles of Wikipedia is neutral point
of view (NPOV) , so that bias is not injected into objective treat-
ment of subject matter. As part of our research vision to develop
resilient bias detection models that can self-adapt over time, we
present in this paper our initial investigation of the potential of
a cross-domain transfer learning approach to improve Wikipedia
bias detection. The ultimate goal is to future-proof Wikipedia in the
face of dynamic, evolving kinds of linguistic bias and adversarial
manipulations intended to evade NPOV issues. We highlight the im-
pact of incorporating evidence of bias from other subjectivity rich
domains into further pre-training a BERT-based model, resulting
in strong performance in comparison with traditional methods.
KEYWORDS
Language Bias, Wikipedia Quality, NPOVaccessed 07-January-2021].To mitigate the rabbit hole effect in recommendations, conventional
distribution-aware recommendation systems aim to ensure that a
user’s prior interest areas are reflected in the recommendations
that the system makes. For example, a user who historically prefers
comedies to dramas by 2:1 should see a similar ratio in recom-
mended movies. Such approaches have proven to be an impor-
tant building block for recommendation tasks. However, existing
distribution-aware approaches enforce that the target taste distribu-
tion should exactly match a user’s prior interests (typically revealed
through training data), based on the assumption that users’ taste
distribution is fundamentally static. This assumption can lead to
large estimation errors. We empirically identify this taste distortion
problem through a data-driven study over multiple datasets. Wethe 14th international conference on World Wide Web . ACM, 22–32.and citation database of peer-reviewed literature.
With over 77.8 million records and 25,100 journal titles from more than 5000 international publishers, Scopus provides
research metrics in the ﬁelds of science, technology, medicine, social science and arts and humanities. The Scopus
APIs allows real-time access articles, authors and institutions in their database.
CrossRef8is a not-for-proﬁt association offering an array of services to ensure that scholarly research metadata is
registered, linked, and distributed. It interlinks millions of items from a variety of content types, such as journals, books,
conference proceedings, working papers, and technical reports. The metadata collected from the members of Crossref
can be accessed using the Crossref Metadata Retrieval API9.
Semantic Scholar10is an AI-backed search engine for academic publications. It is designed to highlight the most
important and inﬂuential papers, and to identify the connections between them. It provides a RESTful API for linking
or articles and extracting information from the records on demand.
3APREPRINT - OCTOBER 22, 2021
TextTEI XMLpdftotextGROBIDDOIAuthorCitationsAcknowledgmentsp-valuesStatisticalInformation
classificationtrainingdata
testingdataTitle
SemanticInformationBibliometricInformation
AuthorInformation17Recommendation algorithms typically build models based on histor-
ical user-item interactions  to provide
a personalized ranked list of items. These interactions are often
distributed unevenly over different groups of items due to varying11— In a modern e-commerce recommender system,
it is important to understand the relationships among products.
Recognizing product relationships—such as complements orsubstitutes—accurately is an esse ntial task for generating better
recommendation results, as well as improving explainability inrecommendation. Products and their associated relationshipsnaturally form a product graph, yet existing efforts do notfully exploit the product graph’s topological structure. Theyusually only consider the information from directly connectedproducts. In fact, the connectivity of products a few hops awayalso contains rich semantics and could be utilized for improvedrelationship prediction. In this work, we formulate the problemas a multilabel link prediction task and propose a novel graphneural network-based framework, item relationship graph neuralnetwork (IRGNN), for discovering multiple complex relationshipssimultaneously. We incorporate multihop relationships of prod-ucts by recursively updating node embeddings using the messagesfrom their neighbors. An edge relational network is designedto effectively capture relational information between products.Extensive experiments are conducted on real-world product data,validating the effectiveness of IRGNN, especially on large andsparse product graphs.
Index Terms—Graph neural networks (GNNs), item relation-
ship prediction, multihop relationships.
I. I NTRODUCTION
RECOMMENDER systems are critical for powering
fast-growing web and mobile segments of the economy,
to connect users to the right items (videos, jobs, news articles,
and so on). These systems attempt to infer useful relationships
among users and items, including user–item, user–user, anditem–item relationships . While there is extensive prior
Manuscript received March 30, 2020; revised September 18, 2020 and
January 19, 2021; accepted February 11, 2021. (Corresponding author:
Pheng Ann Heng.)
Weiwen Liu and Pheng-Ann Heng are with the Department of Computer
Science and Engineering, The Chine se University of Hong Kong, Hong Kong
(e-mail: wwliu@cse.cuhk.edu.hk; pheng@cse.cuhk.edu.hk).
Yin Zhang, Jianling Wang, Yun He, and James Caverlee are with the Depart-
ment of Computer Science and Engineering, Texas A&M University, College
Station, TX 77843 USA (e-mail: zhan13679@tamu.edu; jlwang@tamu.edu;yunhe@tamu.edu; caverlee@tamu.edu).
Patrick P. K. Chan is with the School of Computer Science and Engineering,
South China University of Technology, Guangzhou 510006, China (e-mail:patrickchan@ieee.org).
Daniel S. Yeung, retired, was with the SMC Society o f IEEE, H ong Kong
(e-mail: danyeung@ieee.org).
Color versions of one or more ﬁgures in this article are available at

Digital Object Identiﬁer 10.1109/TNNLS.2021.3060872research on analyzing user–item interactions on the user–item
bipartite graphs – or s tudying social recommendation
via user–user social networks –, item relationships in
item graphs are only recently attracting attention. Discrim-
inating item relationships between products is an essential
task in e-commerce platforms by generating better and more
context-relevant recommendation, as well as improving the
explainability of the system .
On e-commerce platforms, such as Amazon, Shopify,
JD.com, and Taobao, items (or products) may participatein many heterogeneous relationships like being substitutableor complementary . Substitutable items are interchange-
able. Typical examples include items that are viewed by
the same user ( also viewed ) and items that a user who
viewed one eventually bought the other ( buy after viewing ).
Complementary items are usually purchased together by users.Examples include items that are bought together in the same
transaction and items that are purchased in different transac-
tions ( also bought ).
Understanding these item relationships can improve the
accuracy and explainability of recommendation by surfacingitems that are relevant to a given context. For example, when
a user is browsing for headphones in an online store, recom-
mending headphones that are cheaper or of better quality thanthe one she is currently viewing can better satisfy the user’s
needs. After she has bought a headphone, it is more reasonable
to recommend her with headphone cases or chargers, ratherthan continuing to recommend headphones.
Foundational work in item relationship prediction has shown
the potential of improving recommendation by uncovering
these relationships –. Most existing work simply relies
on item content information (reviews and descriptions) ofthe items to analyze the possible connections between them.
However, in addition to item content features, the multihop
relational information among items is also a critical part inpredicting item relationships, yet is ignored by previous work.
In fact, we ﬁnd that there is rich potential for extracting
item relationship information from multihop connections (the
neighborhood of a node with path length larger than 1). The
complex transitions among distant items can provide usefulclues not found in the direct item connections and thus can be
utilized for relationship recommendation.
Fig. 1 shows an example of predicting the relationship
between i
1and i2, with edges representing complementary
2162-237X © 2021 IEEE. Personal u se is perm itted, but republication/redistri bution requires IEEE permission.
See  for more information.
Authorized licensed use limited to: Texas A M University. Downloaded on April 06,2021 at 16:23:31 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
2 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
Fig. 1. Item relationship prediction. The red arrow means that the two items
are substitutable. The gray arrow means that the two items are complementary.
(a) Traditional item relationship predic tion. (b) Item relationship prediction
with multihop connections.
and substitutable relationshi ps. Traditional methods make
predictions purely according to the item content features.
Such prediction can be unreliable for the cold-start items
where there is no sufﬁcient review information to infer the
relationship. In contrast, rather than solely using the itemcontent information of the two items, multihop connections
exploit all the related items ( i
3,i4,a n d i5) and the transitive
relationship paths among them. In addition, multihop con-nections incorporate more item interactions so that the data
sparsity problem can be alleviated to some extent.
While intuitively useful to integrate multihop connections
into the item relationship prediction, it is nontrivial to preservethe complex relational depende ncies in product graphs. In one
direction, graph neural networks (GNNs) show promising
results in aggregating information over the multihop neigh-
borhood of nodes – by ﬁrst constructing message of
a node from its features and then propagating the message
between neighbors. However, they are far from optimal foritem relationship prediction. In particular, they typically focus
on node features and fail to capture complex edge features.
In fact, modeling edge features is one of the key tasks for itemrelationship prediction . Moreover, current models assume
that there is at most one relationship between two items, which
may not be the case in real-world e-commerce applications.
Two items can have multiple relationships depending on
different contexts and personal perspectives. For example, twophone cases can be connected by both the also bought andbuy
after viewing relationships, as they may be complementary in
color or texture while being substitutable in terms of function.
Therefore, in this work, we propose a novel GNN-based
framework item relationship graph neural network (IRGNN),to capture the multihop relationships in the item graph.
IRGNN is characterized by three unique features.
1) It incorporates multihop dependencies of relationships
by recursively aggregating the information, what wewrap up into messages, from an item’s neighbors.
2) We design an edge relational network to impart edge
features  to the multihop message propagation process.
The edge relational network learns edge-speciﬁc trans-
formation matrices with shar ed network parameters and
nonlinear mappings, which is more ﬂexible and can
exploit the full potentials of edge relational features.
3) We formulate the item relationship prediction problem
as a multilabel link prediction task and design an outerproduct layer to perform the multirelationship predictionsimultaneously.
The main contributions of this article are as follows.
1) We formulate item relationship prediction as a multilabel
link prediction task that allows multiple relationships
between items.
2) We propose a GNN-based framework, IRGNN, for
explaining and predicting item relationships. IRGNNcan better exploit the multihop relationships and the
topological structure in the item graph.
3) We are the ﬁrst to incorporate edge features for predict-
ing the item relationship. We design a novel edge rela-
tional network to model the local, structured relationalmessages. The edge relational network takes as input the
edge relational features and source and destination node
features and outputs an edge-speciﬁc transformationmatrix.
4) Extensive experiments show the effectiveness of
IRGNN, especially when the graph is sparse.
II. L
ITERATURE REVIEW
This work draws on the following research areas: 1) item
multirelationship-based recommendation and 2) graph-based
neural networks.
A. Item Relationship Prediction
Item relationships play a signiﬁcant role in user pur-
chase decisions. Recently, discovering item relationships has
received increased attention –, –. Most of the
existing work infers the item relationship simply from theitem content information  .
Sceptre  learns the content f eatures of items using latent
Dirichlet allocation (LDA) and ﬁts a logistic function over thedocument-topic features. Chen et al .  provided personal-
ized substitute recommendation with item-aware collaborative
ﬁltering from personalization and interpretability perspectives.
Item attributes are extracted fro m user reviews with sentiment
analysis. Linked variational autoencoder (LV AE)  extends
Sceptre by exploiting variational autoencoders (V AEs) to avoid
overﬁtting and producing noisy word clusters.
Another line of work seeks to use item images to infer
the items’ visual-level relationships. McAuley et al . a n d
He and McAuley  leveraged item images for style match-
ing to uncover relationships at the visual level. He et al. 
explored the visual information of items and proposed amixture-of-experts framework to deal with the complex andheterogeneous item relationships. Considering the differences
among item categories, Zhang et al.  aggregated both item
images and descriptions in order to capture different features
for heterogeneous item relationships.
Nevertheless, all the aforementioned methods consider the
heterogeneous relationships with direct neighbors (single-hop
neighbors), which limits the signal from multihop connec-
tions that may yield deeper insights into item relationships.SPEM  constructs an item copurchasing graph and predicts
the substitutable relationships with a deep autoencoder to pre-
serve ﬁrst- and second-order proximities, yet fails to generate
Authorized licensed use limited to: Texas A M University. Downloaded on April 06,2021 at 16:23:31 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
LIU et al. : IRGNNs FOR E-COMMERCE 3
predictions for complements and substitutes simultaneously.
Wang et al .  proposed a path-constrained framework by
involving two-step path constraints to infer item relationships.However, such a method may overlook useful dependenciespreserved in longer paths that can help infer item relationships
and is difﬁcult to generaliz e to scenarios with complex and
emerging relations, which limits the ﬁnal recommendation
performance.
Moreover, these methods assume only one single type
of relationship exists between items. In practice, we ﬁnd
that there could be multiple relationships between items.
For example, two phone cases can be connected by boththealso bought and buy after viewing relationships. Thus,
the assumption of only one relationship between two itemswould lose some of this pivotal item relationship informationand further affect the ﬁnal recommendation based on the
predicted item relationships.
B. Graph-Based Neural Networks
Many recent research efforts have demonstrated the power
of GNNs to model graph-structured data , –.
For example, graph convolutional neural networks (GCNs)have achieved the state of the art in node representation for
signed graphs , sentence classiﬁcation , , and
image recognition . Recently, Morris et al .  built
ak-dimensional GNN that can consider higher order graph
structures at multiple scales. Bresson and Laurent  pro-posed a residual GNN (ConvNets), which shows that residual-
ity can bring a signiﬁcant improvement in subgraph matching
and graph clustering, as they illustrated residuality could betterlearn multilayer architectures in complex graph-structured
data.
With the growing of GNN methods, GNNs are widely app-
lied for recommendation, such as social recommendation ,
, ,  and knowledge graph-based recommendation, , and are even adapted to traditional recommendation
methods such as collaborative ﬁltering –. Recently,
Fan et al .  provided a principled GNN approach with
social connections and user pur chase history to capture the
interactions between user and items for item recommendation.Song et al.  used a dynamic graph attention network and
incorporated recurrent neural networks for user behaviors insession-based social recommendation. Grad-Gyenge et al. 
built a graph embedding method that took advantage of the
knowledge graph to map users and items for recommendation.
Considering the user–item interaction, Wang et al .  con-
structed a user–item interaction bipartite graph and proposed
a graph-based collaborative ﬁltering method to capture higher
order connectivity in the user–item interactions. However, fewof these approaches consider graph-based neural networks for
item multirelationship-based recommendation.
This work is also related to link prediction (where item
relationships can be viewed as missing links). Traditional
link prediction methods mainly depend on heuristics to mea-sure the proximity between nodes and infer whether the
two nodes are linked, such as common neighbors and resis-
tance distance , . Weisfeiler–Lehman neural machine(WLNM)  encodes a subgraph by a fast hashing-basedWeisfeiler–Lehman algorithm to automatically learn suitable
heuristics for improved link prediction. Chen et al . 
proposed to learn separate projections with metric learning
to predict multiple relations.
Recently, GNN-based methods have shown great improve-
ment compared with these methods. Zhang and Chen developed a GNN to learn heuristics from local sub-
graphs for link prediction. Relational graph convolutional
networks (RGCNs)  extended previous work by consid-
ering multiple edge types, which have a large improvement
in knowledge-based data sets. Hamilton et al .  gave a
review of key advancements in learning on graphs with a
uniﬁed framework, including different GNN methods to embed
individual nodes or (sub)graphs for link prediction or othergraph tasks. HetGNN  is a GNN model proposed for
heterogeneous graphs with recurrent neural networks and the
attention mechanism. Regardin g the successful performance of
GNN-based models in link prediction, we build a GNN-basedframework with an edge relational network to learn the com-plex item multirelationships.
III. P
ROPOSED METHOD : IRGNN
In this work, we aim at discovering heterogeneous item
relationships simultaneously, such as complements or substi-tutes. We formulate the problem as a multilabel link prediction
task. Let G={V,E}be the directed multirelational item
graph, with item feature vector x
v∈Rdfor each item
v∈Vand edge relational vectors evw∈Rcfor the edge
 ∈Econnecting from item vto item w. Speciﬁcally, for
item multirelationship-based recommendation, edge relationalvectors are binary vectors, indicating the multiple relationships
between item pairs. The ith element of e
vw,e(i)
vw=1,i=
1,..., cindicates that the edge from vtowhas relationship i,
and e(i)
vw=0 otherwise. In many real-world e-commerce
applications, items are often connected by multiple types ofrelationships simultaneously, meaning that e
vwcan have more
than one element with value 1.
To discover item relationships, rather than having access to
the full set of edges E, we are given only a subset of edges ¯E.
O u rt a s ki st op r e d i c tˆ evwfromvtowto recover the unknown
edge relational vectors evwgiven any ordered pair of item
feature vectors [xv,xw].
We now present the architecture of IRGNN. The design of
IRGNN is built around three research questions.
1)RQ I: How can we encode the local substructure features
from direct neighbors into messages and aggregate the
messages to update the model so that we can better
preserve the topological information and the relationaldependencies in the single-hop neighborhood?
2)RQ II: How can we better exploit the local message
and propagate it to multihop neighbors to incorporatemultihop relationships?
3)RQ III: How can we predict item relationships with
the learned representation of multihop relationships in
directed multirelational item graphs?
Authorized licensed use limited to: Texas A M University. Downloaded on April 06,2021 at 16:23:31 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
4 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
A. RQ I: Single-Hop Message Propagation
Let us denote the current node embedding of item v∈Vby
hv∈Rd. For the ﬁrst question, we aim to design a single-hop
message propagation that can encode local graph structure and
relational features such as the edge type between the nodes
and hidden node features into the messages, so that we can
use these messages to effectively update the adjacent nodes’embeddings . Only if the messages from the single-hop
neighborhood are well-captured, can we further extend it to
multihop neighbors. We formulate the single-hop messagepropagation by a message construction step, a message aggre-
gation step, and a message update step. We propose an edge
relational network to construct relational messages.
1) Message Construction: Messages from neighbors should
carry information of the node embeddings of the connected
neighbors and the edge relational vectors. Therefore, we deﬁne
the message from item wtovas
m
v←w=fc
where fc(·)is the message construction function and hv,hw∈
Rdare the node embedding of the source and destination
items. This formulation is natural since an edge in a graph
is uniquely deﬁned by the source node, destination node, and
types of edges connecting them. The message constructionfunction f
cis typically chosen to be simply a linear transfor-
mation mv←w=Whwas in .
Linear transformation has been shown to be effective at
accumulating and encoding features from local, structured
neighborhoods . Based on this promising result, we design
a message-speciﬁc fcfor the item relationship discovering task
as
mv←w=g
where mv←whas a dimension of d, and the nonlinear function
g(·)is our edge relational network that takes as input both the
edge relational vector ewvand the previous node embeddings
of the source and destination nodes. It outputs a transformation
matrix of dimension d×dto capture the edge relational
features from hwtohvwith relationship ewv.
Edge Relational Network: In previous GNN-based mod-
els for heterogeneous graphs such as RGCN or HetGNN,
a separate set of parameters is learned for each type ofrelationship. Applying the same transformation to the same
type of relationship, regardless of which items are connected,
limits the expressive power and has high computational cost
when the number of types of relationships is large. Moreover,
such a discrete method fails to generalize to edge features incontinuous space and thus is less ﬂexible.
To resolve the problems, we propose the edge relational
network to generate an edge-speciﬁc transformation matrix.The edge relational network has the advantage of efﬁciently
encoding complex relational information into the transfor-
mation matrices. Instead of using edge type as the single
input, we propose to fully leverage the information from two
connected items and take as input the edge relational vectore
wvand the item features hwand hv
g=σ(
Wg·[ewv||hw⊙hv])
(3)where ||denotes the vector concatenation, ⊙is the element-
wise product, Wg∈Rd2×(c+d)is the trainable parameter, and
σ(·)is the ReLU function. We reshape the output to a matrix
with dimension d×d.
The proposed edge relational network gpreserves the edge
and node features. The edge relational vector ewvindicates
the multirelationship between a pair of items. Motivated bythe success of applying second- order feature interactions in
neural networks , , rather than using a linear con-catenation of h
wand hv, we adopt a multiplicative technique
(hw⊙hv) in the edge relational network to further capture the
second-order interactions of the node embeddings. Intuitively,the semantic similarity between a pair of items provides useful
evidence on the item relationships. For example, two function-
ally similar items are likely to be substitutable. The productoperation facilitates the measurement of the dependencies
between the connected items. We ﬁnd in experimental results
in Section IV-H2 that the elementwise product can actually
achieve better performance than a simple concatenation of the
node embeddings.
We concatenate the edge relational vector and the prod-
uct of the node embeddings to allow the edge relational
network to fully exploit the local structure and relationalinformation. Though multiple me ssage propagation explained
in Section III-B, messages captured by the edge relationalnetwork can be propagated along the paths and improve the
item relationship prediction.
2) Message Aggregation: Then, we aggregate messages
from the item’s local neighborhood with a mean aggregator
by averaging messages from neighbors
m
v=1
|Nv|∑
w∈Nvmv←w (4)
where Nvdenotes the set of direct neighbors of item v
and|Nv|is the number of neighbors of item v.T h i s mean
aggregator is nearly equivalent to the convolutional propaga-
tion rule used in the transductive GCN . One could use
more complex aggregators, such as LSTMs  or attention
mechanisms .
3) Message Update: After obtaining the messages mvfrom
the neighborhood, we use the previous node embedding of
itself and newly received neighborhood messages to update
the model.
In general, the updated h′
v∈Rdis expressed as
h′
v=fu
where fu(·)is a nonlinear function.
In experiments, we adopt a simple update scheme
h′
v=fr
=σ(W1·[σ(W2hv)||mv])+hv (7)
by ﬁrst mapping the previous node embedding into the mes-
sage space and then concatenate it with the current message
and feed to a second fully connected layer. The weightmatrices W
1∈Rd×2dand W2∈Rd×dare model parameters.
Authorized licensed use limited to: Texas A M University. Downloaded on April 06,2021 at 16:23:31 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
LIU et al. : IRGNNs FOR E-COMMERCE 5
Fig. 2. Illustration of the message propagation process.
graphs with fu=GRU, . Though the gated
mechanism can use information from the neighboring nodes
and the previous time step to update the node embedding,
it is computationally expensive, especially when the graph is
large. In our experiments (see Section IV-H2), we show that
two fully connected layers with a shortcut connection as in (7)
can achieve similar or even better performance as a GRU foritem relationship prediction. Details of the shortcut connection
will be discussed in the following.
B. RQ II: Multihop Relationships
With the node embedding representing messages from each
item’s single-hop neighbors, we are ready to propagate the
messages across the graph and model multihop relationships.
Multihop connections preserve relational dependencies amongdistant items and provide crucial evidence for inferring item
relationships.
We stack Lmessage propagation steps together and recur-
sively aggregate messages from neighboring nodes, as shown
in Fig. 2, so that the messages containing relational andstructured information from a node’s L-hop neighbors can
be explicitly encoded to the node embeddings. We initializeh
(0)
v=xv. Therefore, for l=1,..., L, our message propaga-
tion step is recursively expressed as follows :
m(l+1)
v=1
|Nv|∑
w∈Nvσ(
W(l)
g·[evw||h(l)
w⊙h(l)
v])
·h(l)
w (8)
h(l+1)
v=σ(
W(l)
1·[
σ(
W(l)
2h(l)
v)⏐⏐⏐⏐⏐⏐m
(l+1)
v])
+h(l)
v (9)
where h(l)
v∈Rdand m(l)
v∈Rddenote the node embedding
and the message of item vat the lth message propagation
step (layer). We use different parameters {W(l)
g,W(l)
1,W(l)
2}in
different layers to extract hierarchical latent features. As such,
multiple message propagation steps seamlessly inject multihop
information into the node embeddings.
a) Shortcut connections: To facilitate IRGNN to learn
deeper models to involve neighbors from farther away, we useshortcut connections  between hidden layers. Shortcut
connections enable the model to carry over information from
the previous iterations, with the added beneﬁt of not involvingany extra parameters or computational complexity.
Without a shortcut connection, the ﬁrst term f
r
in (6) should directly learn the representation of h′
v, but withan identity mapping hv,fronly needs to learn residual
mapping
h′
v−hv. (10)
With the increase of the number of iterations, if no new
residual mapping is needed, the network can bypass identitymappings, which could greatly simplify the training for learn-
ing multihop relationships.
C. RQ III: Item Multirelationship Prediction and
Optimization
After obtaining the embeddings, we propose an outer prod-
uct layer to reconstruct the given graph structure and performthe item multirelationship recommendation.
1) Outer Product: Given an ordered pair of learned node
embedding [h
(L)
v,h(L)
w]at the last step L, our prediction ˆ e(i)
vw
for relationship ifromvtow,i=1,..., cis
ˆe(i)
vw=μ(
W(i)
o·vec(h(L)
v⊗h(L)
w))
(11)
where μ(x)=(1/1+e−x)is the sigmoid function. Every
relationship iis associated with a weight vector W(i)
o∈Rd2.
We vectorize the outer product h(L)
v⊗h(L)
wto dimension
d2, allowing the model to fully exploit underlying feature
interactions. Moreover, the outer product operation is non-commutative so that [h
(L)
v,h(L)
w]and[h(L)
w,h(L)
v]yield different
results. As such, we can make predictions with directions.
2) Optimization: We optimize the sum of the cross-entropy
loss for each type of relationshi p separately, to push the model
to score observable relationships higher than the negative ones
L=∑
∈ Tc∑
i=1e(i)
vwlog(
ˆe(i)
vw)
+(
1−e(i)
vw)
log(
1−ˆe(i)
vw)
(12)
where Tis the training edge set. However, it is computa-
tionally consuming to train GNNs on large-scale graphs .We adopt the following two methods to advance the training
efﬁciency while preserving the accuracy.
1)Negative Sampling: Since it is impractical to use all
nonedges as negative samples, we train the model with
negative sampling. In the multigraph setting, item pairscan have multiple different relationships. Treating sam-
ples with one type of relationship as negative samples of
the ones with the other type as in  is not applicable.
Authorized licensed use limited to: Texas A M University. Downloaded on April 06,2021 at 16:23:31 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
6 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
Therefore, we randomly sample a set of nonedges ¯E
as many as there is at least one type of relationships
(|E|=| ¯E|).
2)Neighborhood Sampling: Most existing GNN-based
methods require that all nodes in the graph arepresent during the training of the embeddings, which
is time-consuming and not applicable when the graph islarge . Take the Amazon product graph for example,
and there are thousands of nodes and millions of edges,
making it infeasible to directly operate on the full graph.To scale up the model, we adopt a uniform neighborhood
sampling strategy ,  by sampling the neighbor-hood around a node and dynamically constructing a
computation graph containing all the nodes needed for
Literations from this sampled neighborhood. Therefore,
our model can be trained in a minibatch mode.
D. Discussion
We conclude our presentation of the IRGNN model with a
discussion of its relation to other models and time complexity
analysis.
1) Relations to Other Models: IRGNN is a general frame-
work for item relationship prediction and can be used to learnany type of item relationship. We can show that most existing
models can be viewed as special cases of IRGNN.
a) Relation to LVAE: LV AEs is the state-of-the-art model
for predicting relationships between products . SinceLV AE considers direct neighbors only, we can set the number
of message propagation iterations in IRGNN to L=1,
yielding the deterministic version of LV AE. The messageconstruction is
m
(1)
v←w=[
Wah(0)
v+sa⏐⏐⏐⏐W
bh(0)
w+sb]
by feeding the source and destination node features to a fully
connected neural networks separately, where {Wa,Wb,sa,sb}
are model parameters. We use a mapping m(1)
v(w)=m(1)
v←w
as the aggregator. The message update function is a simpleelementwise ReLU function σ(·)
h
(1)
v(w)=σ(
m(1)
v(w))
.
LV AE uses a fully connected layer to predict the relationships
ˆe(i)
vw=Wc·h(1)
v(w)+sc
where {Wc,sc}are parameters to be learned.
b) Relation to RGCN: RGCNs is the state-of-the-art
GNN-based model developed speciﬁcally to deal with the
multirelational data . We can view the RGCN message
construction function as
m(l+1)
v←w=c∑
i=11
cv,ie(i)
wvW(l)
ih(l)
w
where cv,ris a predeﬁned normalization constant. The trans-
formation matrix W(l)
iremains the same for relationship i
at layer l, which is less expressive than our designed edge
relational network. Then, the message is aggregated by a Sum
aggregator m(l+1)
v=∑
w∈Nvm(l+1)
v←w. The node embeddings areupdated by the message update function h(l+1)
v=σ(m(l+1)
v+
W(l)
0h(l)
v).
In sum, IRGNN is a ﬂexible framework for item relation-
ship prediction. IRGNN generalizes the existing methods byincorporating multihop relationships with an edge-dependentedge relational network.
2) Time Complexity Analysis: One limitation of GNN-based
models is scalability. As we use the neighborhood samplingto break down the graph into minibatches of subgraphs,
the complexity of a single step of the message propagation for
a dense graph is reduced from O(n
2d2)toO(m2d2),w h e r e
nis the number of nodes of the full graph, m≪nis the
number of nodes in each batch, and dis the dimension of
the node embedding. Therefore, the overall time complexityisO(m
2d2L)with Lmessage propagation steps.
Empirically, IRGNN trains a graph with thousands of nodes
and millions of edges within 300 s per epoch on a TITAN Xp
Graphics Card, following the setting in Section IV-B. As for
prediction, IRGNN predicts millions of edges at the same scalewithin 30 s.
IV . E
XPERIMENTS
In this section, we evaluate the proposed IRGNN on several
Amazon-based data sets and a Taobao data set. We aim toanswer the following research questions.
1)RQ1: How does IRGNN perform as compared with the
state-of-the-art item relationship prediction models?
2)RQ2: How well does IRGNN perform on sparse data as
compared with the state-of-the-art model?
3)RQ3: How do different hyperparameter settings (the
number of message propagation iterations and thedimension of node embeddings) inﬂuence the perfor-
mance of IRGNN?
4)RQ4: How does each designed component (the shortcut
connection, the edge relational network design, the mes-
sage update scheme, the outer product, the neighborhoodsampling, and the negative sampling) inﬂuence the per-
formance of IRGNN?
5)RQ5: How does IRGNN beneﬁt the recommendation
in e-commerce platforms with improved accuracy andexplainability?
A. Data Set Description
1) Amazon Data: We use the Amazon data set from ,
. The complete data set contains over 1 million products
and 42 million copurchase relati onships across around 20 top-
level product categories. We focus on ﬁve main categories
that display different complementary aspects: Video Games,
Musical Instruments, Movies and TV , Electronics, and Cloth-ing, Shoes, and Jewelry. Each category forms a separate
item graph. The data set contains four different types ofrelationships and all of them are asymmetric.
1) Also Bought (AB): Users bought xalso bought y.
2)Also Viewed (AV): Users viewed xalso viewed y.
3)Bought Together (BT): Users frequently bought xand y
(xand ywere purchased as part of a single basket).
Authorized licensed use limited to: Texas A M University. Downloaded on April 06,2021 at 16:23:31 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
LIU et al. : IRGNNs FOR E-COMMERCE 7
TABLE I
STATISTICS OF THE AMAZON DATA
4)Buy After Viewing (BV): Users who viewed xeventually
bought y.
We remove the isolated nodes and use the reviews as the node
feature, following the preprocessing steps in . The detailedstatistics of the data are shown in Table I.
1
a) Mutigraph and simple graph setting: As discussed in
Section II, existing work on item relationship recommendationassumes only one single type of relationship exists between
items –, –. From Table I, we can observe
that items are actually connected by multiple relationships
(multiedges). Following the previous setting would lose some
pivotal item relationships and inﬂuence the recommendationresults. Though one could formulate the problem as a simple
graph setting with 2
4=16 different types of edges, the size of
the parameters of the model will increase exponentially, and
the limited training data for each type of edge will result in
poor prediction performances.
Moreover, also bought relationship dominates the graph
in Video Games, Musical Instruments, and Movies & TV ,
whereas also viewed relationship is the dominant one in
Clothing, Shoes, and Jewelry. Discriminating relationships on
the imbalanced data with one dominant relationship is easier
in the simple graph setting since the goal is to differentiate
between edge types. In contrast, the setting of the multigraph
is more challenging due to the more relaxed assumption.
2) Taobao Data: Taobao data is a data set consisting
of user behavior data retrieved from Taobao,2one of the
biggest e-commerce platforms in China, with 987 994 users,
4 162 042 items, and 100 150 807 interactions. It contains user
behaviors from November 25 to December 3, 2017, with
several behavior types, including click, purchase, adding tocart, and item favoring. We assume that an item relationship
exists if more than 50 users perform the same behaviors toward
a pair of items, and thereby, the Taobao product graph has thefollowing four types of relationships.
1)Coclick: Users clicked xalso clicked y.
2)Copurchase: Users bought xalso bought y.
3)Add-both-to-cart: Users added xto the shopping cart
also added y.
4)Favor-both: Users favored both xand y.
Node features are item ID and the corresponding category ID.
Only items that have at least 50 interactions are kept. After
1Note that the number of BV of clothing, Shoes, and Jewelry in Table I
is much smaller. It may be due to the crawling strategy of the data set
that products with BV are rarely sampled . The preprocessing procedure also
removed some products with no review information.
2 the isolated nodes, the retained item graph contains
224 654 nodes and 2 410 056 multiedges.
B. Experimental Setting
Our code is available at 
IRGNN_TNNLS_2021. We randomly assign paired item sam-ples into the training, validation, and test sets with an8/1/1 ratio. The number of relationships c=4. We use grid
search to select the hyperparameter for all the methods on thevalidation set: the node embedding size d∈{8,16,64,128} ,
learning rate in {0.001, 0.01,0.1}, the batch size is set to 512,
and the optimization method is Adam . For our proposedIRGNN, the message propagation iterations L∈{2,3,4,5}
and d∈{8,16,32}, and we select d=16 for all the
experiments. All models are trained 300 epochs to ensureconvergence. We conduct ﬁve independent runs and use early
stopping. For a fair comparison, the input node features x
v
are the same, followed by a fully connected embedding layermapping raw node features from the original dimension to the
dimension of the node embeddings d.
C. Baselines
1)Logistic Regression (LR): Our ﬁrst baseline is a straight-
forward application of LR. We ﬁrst feed the node feature
into an embedding layer to obtain the node embeddings
and then concatenate the pair of node embeddings[h
v||hw]as the input of four logistic models, one for
predicting one type of relationship.
2)Sceptre: Sceptre ﬁts a logistic classiﬁer over the
topic space of LDA, which not only learns the rela-tionship between items but also the direction of therelationship .
3)PME: PME  incorporates multirelations by embed-
ding separate different node and edge types into differentlatent spaces and uses metric learning to capture the ﬁrst-
and second-order proximities.
4)PMSC: PMSC is a path-constrained method to dis-
criminate substitutes and complements (PMSC) .Speciﬁcally, PMSC incorporates 2-hop path constraints
with t-norm fuzzy logics.
5)RGCN: RGCNs is a GNN-based model developed
speciﬁcally to deal with the multirelational data .RGCN handles different types of relationships and direc-
tions separately and uses a weighted sum to aggregate
them.
Authorized licensed use limited to: Texas A M University. Downloaded on April 06,2021 at 16:23:31 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
8 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
TABLE II
EXPERIMENTAL RESULTS ON AMAZON DATA
6)HetGNN: Heterogeneous GNN (HetGNN)  is the
state-of-the-art GNN-based model designed for hetero-
geneous graphs with recurrent neural networks and the
attention mechanism.
7)LVAE: LV AEs is the state-of-the-art model for pre-
dicting relationships between products . LV AE isa generative deep learning model that links two V AEs
using a connector neural network.
As the aforementioned models all assume that only one singlerelationship exists between items except RGCN, we replace
theSoftmax function before the output by a Sigmoid function
to allow them to predict multiple relationships at the same
time.
D. Evaluation Metrics
In discovering item relationships, the model should not only
predict the existing relationships for a given pair of items
but also the direction of the link. Given an ordered pair
of items (each order represent s one direction), we compute
the area under the ROC curve  on the test set . Note that we mea-
sure AUC, precision, and recal l separately for each type of
relationship and report the weighted average value. We take
label imbalance into account so that the results are weighted
by support (the number of true instances for each type ofrelationship). For the ACC score, all four types of relationships
must be correctly predicted simultaneously.
E. Overall Performance (RQ1)
Table II shows the overall performance on Amazon data.
Bold numbers are the best results. From the table, we can
observe that LR has the lowest accuracy, indicating that theexpressive power of the logistic model is insufﬁcient to capture
complex relationships among items. Sceptre improves LR by
learning topic models to discover topics from the reviews.It not only learns the relationship between items but also the
direction as well. However, LR and Sceptre only consider
the information from direct neighbors. The connectivity of
the neighbors a few hops away also contains rich semantics
that could be used for discovering item relationships.
PME and PMSC outperform LR and Sceptre since they con-
sider 2-hop connections in the item graph. PME models 1-hopand 2-hop proximities with metric learning approach. PMSCincorporates 2-hop path constraints by maximizing the
co-occurrence patterns of the type of edges. However, both
PME and PMSC require manually setting the dependencies
in the graph and have limited expressive power. RGCN and
HetGNN generally have better performance than PME andPMSC. It may be because RG CN and HetGNN can automati-
cally model multihop relationships using GNNs. In particular,HetGNN aggregates different types of information with recur-rent neural networks and the attention mechanism.
LV AE consists of two V AEs with a connector neural net-
work, designed speciﬁcally to discriminate item relationships
(substitutes and complements). LV AE is the strongest baseline
since the pair of V AEs can capture meaningful item featuresexplaining the relationship between items.
IRGNN consistently yields the best performance on all ﬁve
data sets, even on the highly sparse data set such as Clothing,Shoes, and Jewelry. In particular, IRGNN improves over the
strongest baseline LV AE by 3% in ACC, 9.7% in recall,
and 1.9% in AUC on average, and the average precision is
almost the same. We attribute this improvement due to IRGNN
incorporating multihop relationships and better utilizing theedge relational features along the path. The edge relational
features contain collaborative information such as substitute
or complement and can be captured by the designed edgerelational network in IRGNN.
In Table III, path-based methods, such as PME and PMSC,
do not generalize well and fail to effectively identify item
relationships. This might suggest that the four relationships
Authorized licensed use limited to: Texas A M University. Downloaded on April 06,2021 at 16:23:31 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
LIU et al. : IRGNNs FOR E-COMMERCE 9
TABLE III
EXPERIMENTAL RESULTS ON TAOBAO DATA
in Taobao data such as coclick/copurchase/favor-both share
similar semantic meanings and i s a more difﬁcult scenario to
predict the item relationships and differentiate between them.
Hence, the simple path constraints as in PME or PMSC may
not be able to characterize the underlying item dependencies.
Yet, IRGNN achieves signiﬁcant improvements over all base-
lines under various metrics, showing its high effectivenessand generalization ability to complex product relationships
and scenarios. Speciﬁcally, its relative improvement over the
strongest baseline with respect to Precision is 3.77%.
F . Data Sparsity Problem (RQ2)
As discussed in Section III, our proposed model is capable
of naturally utilizing multihop re lationships of products, which
can provide more information when data are sparse. In this
section, we further evaluate the effectiveness of IRGNN as thedegree of data sparsity increases on Amazon data. We decrease
the ratio of the training set to increase the data sparsity and
generate four versions of data sets. The data split ratios arevaried from 8/1/1 (i.e., the ratio for training, validation, and
test sets, respectively) to 4/3/3, 6/7/7, and 1/2/2.
The comparison between our proposed model and the
strongest baseline LV AE on these four versions of the data setsis presented in Fig. 3. We observe that IRGNN consistently
outperforms the strongest baseline in terms of accuracy on
all sparse data sets. Moreover, the sparser the data set is,
the larger improvement can be achieved by IRGNN uponLV AE. For example, from data set 8/1/1 to the sparsest data
set 1/2/2, improvement of IRGNN upon LV AE increases from
2.8% to 3.2% in Video Games, from 2.5% to 4.5% in MusicalInstruments, from 3.2% to 4.6% in Movies and TV , from 1%
to 2.3% in Electronics, and from 5.5% to 8.4% in Clothing,
Shoes, and Jewelry. This result demonstrates that IRGNN
is superior to other methods in sparse data sets and shows
that utilizing multihop relationships of products is effectiveto alleviate the data sparsity problem for item relationship
prediction.
G. Hyperparameter Study (RQ3)
In this section, we study the impact of the hyperparameters
on the performance of IRGNN.
1) Number of Message Propagation Iterations: As dis-
cussed in Section IV-F, multihop relationships of products
can provide additional information in sparse data sets andhence beneﬁt the prediction of item relationships. Naturally,
the next question would be how many hops are most helpful
for item relationship prediction? Hence, we study the impactTABLE IV
IRGNN W ITHDIFFERENT NUMBERS OF MESSAGE
PROPAGATION ITERATIONS
of the number of message propagation iterations, which cor-
responds to the maximum number of hops of the connections.
In Table IV, we present the performance in terms of ACCand AUC of IRGNN by varying the number of message
propagation iterations (taking Video Games, Musical Instru-
ments, and Movies and TV as examples due to limited places,similar observations can be obtained on the other two data
sets). We observe that generally, a large number of iterations
are more effective to predict item relationships. For example,
IRGNN achieves the best performance when the number is
5 at Video Games and Movies and TV , and the best resultswhen the number is 4 at Musical Instruments. This result
demonstrates that the proposed model beneﬁts from a large
number of message propagation iterations.
2) Dimension of the Node Embeddings: Another impactful
hyperparameter is the dimension of the node embeddings.We evaluate the proposed model with the dimension varyingfrom 8 to 16 and 32 and present the results in Table V.
We observe that IRGNN achieves the best performance when
the dimension of the node embeddings is 16 for all data sets(results on the other two data sets are similar and omitted for
simplicity). For example, IRGNN achieves the best accuracy
of 0.8403 when the dimension is 16 on the Video Games data
set, which is superior to 0.7836 when the dimension is 8 and
slightly better than 0.8398 when the dimension is 32. On theone hand, IRGNN suffers from a small dimension  due
to its limited ﬁtting capability. On the other hand, it is also
worth noting that a large dimension may cause overﬁtting anddegrade the performance.
H. Ablation Study (RQ4)
1) Shortcut Connections: As discussed in Section III,
the goal of shortcut connections is to simplify the learning of
multihop relationships. The useful information from the pre-
vious message propagation iteration could be directly utilized
through this shortcut connection. In this section, we further
Authorized licensed use limited to: Texas A M University. Downloaded on April 06,2021 at 16:23:31 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
10 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
Fig. 3. Comparison between IRGNN and the strongest baseline in terms of ACC w ith different data sparsities. (a) Video games. (b) Musical instruments.
(c) Movies and TV . (d) Electronics. (e) Clothing.
Fig. 4. IRGNN with and without shortcut connections. (a) Vi deo games. (b) Musical instruments. (c) Movies and TV .
TABLE V
IRGNN W ITHDIFFERENT NODE EMBEDDING DIMENSIONS
evaluate the effectiveness of the design of the shortcut connec-
tions in IRGNN. We vary the number of message propagation
iterations and report the accuracy for the proposed modelwith and without shortcut connections in Fig. 4. We observe
that the shortcut connection is truly helpful to improve the
performance. For example, IRGNN with the shortcut connec-
tions outperforms the variant without the shortcut connections
on Video Games for all numbers of message propagation,where the largest improvement is obtained when the number
of message propagation is 5. A similar observation is also
obtained on the other data sets except for Musical Instrumentswhere IRGNN with and without the shortcut connections have
a close performance. A possible reason behind this result is
that the graph of Musical Instruments has many circles of
length 4, and the message propagation is trapped by the circles,
so that increasing the number of iterations will not improvethe performance.
2) Other Variants of IRGNN: We also test several variants
of IRGNN to validate the design of our edge relational
network. Each time, we replace one key component and
compare the result with the full-ﬂedged IRGNN, where theresults are presented in Table VI. Experimental results demon-
strate that all the components working together yield the best
performance.TABLE VI
VARIANTS OF IRGNN
a) Node embeddings in message propagation: We ﬁrst
evaluate the performance of IRGNN with a simple edgerelational network that removes node embeddings and only
takes as input the edge relational vector (IRGNN-SIMPLE-E),
as adopted in , and namely, the edge relational network is
reduced to
g(
h
(l)
w,evw,h(l)
v)
=σ(W(l)
g′evw).
In this setting, the transformation matrix gonly depends on
the edge relational vectors so that edges with the same edge
relational vectors have the same transformation matrix. Theresults in Table VI indicate that the performance of IRGNN-
SIMPLE-E is downgraded compared to IRGNN, which shows
the effectiveness of node embeddings in message propagation.
b) Second-order feature interactions: To validate the
usage of the elementwise product in the edge relational net-
work, we compare IRGNN with the edge relational network
that uses the linear concatenatio n of the source and destination
node embeddings (IRGNN-LIN-E)
g(
h
(l)
w,evw,h(l)
v)
=σ(
W(l)
g′′·[
evw||h(l)
w||h(l)
v])
.
Authorized licensed use limited to: Texas A M University. Downloaded on April 06,2021 at 16:23:31 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
LIU et al. : IRGNNs FOR E-COMMERCE 11
Fig. 5. Illustration of item relationship predic tion. (a) Musical instruments. (b) Electronics.
As shown in Table VI, by using the elementwise product, ACC
increases by 0.2%, 0.2%, and 0.8% for Video Games, MusicalInstruments, and Movies and TV , respectively. This illustrates
the importance of the second-order feature interactions.
c) GRU to replace shortcut connections: For the message
update function, we propose to use two fully connected layerswith a shortcut connection to facilitate the training with mul-
tihop relationships, which is more efﬁcient than GRU and is
capable of incorporating the message from the previous layer.
We replace our designed f
in  (IRGNN-GRU) and evaluate the performance of it.
Experimental results demonstrated the effectiveness of the
shortcut connection, as IRGNN outperforms IRGNN-GRU onall the three data sets. It may be because the IRGNN-GRU
overﬁts the training data.
d) Outer product: Although the simple concatenation is
a widely adopted method for multir elationship prediction ,
we observe that the outer product may be more feasible for
the item multirelationship prediction task. We replace the outerproduct in IRGNN with a simple concatenation of the source
and destination node embeddings and propose IRGNN-LIN-D,
which is equivalent to LR
ˆe
(i)
vw=μ(
W(i)
o′·[
h(L)
v||h(L)
w])
where μis the sigmoid function. Table VI demonstrates that
the outer product enhances the p erformance of the multi-item
relationship prediction, with ACC increasing by 2.9%, 0.7%,and 2.4%, AUC increasing by 3.5%, 0.7%, and 2.9% on Video
Games, Musical Instruments, and Movies and TV , respectively.
3) Neighborhood Sampling: As discussed in Section III-C2,
without neighborhood sampling, the memory and expected
runtime of a single batch is unpredictable and in the worst case
isO(n),w h e r e nis the number of nodes in the product graph.
Therefore, we study the effectiveness and efﬁciency of ourproposed IRGNN with and without neighborhood sampling on
Video Games, as shown in Table VII. The runtime in Table VIIrecords the average running time of a message propagation
step for one node. The results suggest that neighborhood
sampling could help IRGNN achieve higher performance withTABLE VII
IRGNN W ITH AND WITHOUT NEIGHBORHOOD SAMPLING
TABLE VIII
IRGNN W ITHDIFFERENT RATI OS OF POSITIVE TO NEGATIVE SAMPLES
lower space and time complexity since message propagationwithout neighborhood sampling has to aggregate messagesfrom all neighbors, which is less efﬁcient and may involve
noisy neighbors.
4) Negative Sampling: Directly treating all unobserved
edges as negative samples is extremely computationally expen-sive, as the number of the unobserved edges is huge and power
to the number of nodes . To this end, we uniformly samplenegative samples as in  and study the effect of the ratio of
positive to negative samples. As shown in Table VIII, IRGNN
is generally not sensitive to the ratio of positive to negative
samples—different settings yield similar results. The perfor-
mance is getting slightly better when adding more negativeexamples. Yet, more negativ e samples require more time and
space for training the model, and we use the 1:1 balancedsetting in our experiments.
I. Case Study (RQ5)
1) Item Relationship-Based Recommendation: To perform
recommendation according to the l earned item relationships,
we randomly select a query item. We rank the remaining
Authorized licensed use limited to: Texas A M University. Downloaded on April 06,2021 at 16:23:31 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
12 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
TABLE IX
EXAMPLES OF PREDICTIONS ON MULTIHOP CONNECTIONS (TWOTYPES OF RELATIONSHIPS :SUB/COMP )
TABLE X
PREDICTIONS ON MULTIHOP CONNECTIONS (FOUR TYPES
OFRELATIONSHIPS : AB/A V/BT/BV)
items by the predicted score for each type of relationship with
the query item. Illustrations of the top-3 recommendation are
shown in Fig. 5. For the convenience of visualization, we use
the category of Musical Instruments and Electronics. For the
ground-truth relationships, since the data are sparse and are
hard to ﬁnd an item in the test set that contains all fourrelationships, we need to densify the data set. We perform
a content-based technique  and create pseudoitems by
grouping the ten-nearest neighbors to one pseudoitem accord-ing to the item features. Ground-truth relationships of the
pseudoitems are presented in the second row.
Fig. 5 shows that compared to the ground truth, IRGNN can
perform reasonable recommendations according to multiple
complicated relationships. For example, IRGNN is capable tolearn the pattern that users also viewed other electronic guitars
after viewing the queried guitar and bought guitar picks and
shoulder straps together with the guitars. Such results may
also be useful for understanding item relationships and user
behavior prediction.
2) Multihop Prediction: To investigate whether our pro-
posed IRGNN is capable of learning meaningful knowledge
from multihop connections, we use IRGNN to predict itemrelationships where items are connected by a 3-hop path.
We count predictions made on th ree most frequent 3-hop paths
as an illustration. To start with, we consider edges with single
relationships (either complements or substitutes) for easier
understanding. We refer to AB and BT as complements and
A V and BV as substitutes. Table IX computes the ratio of
the number of the given patterns to all the predictions made
by IRGNN. We also show examples of positive and negativecases. Four products are connected sequentially by the given
relationships and we let that IRGNN predicts the item relation-
ship between the ﬁrst and the last item. Positive cases representthe cases that the item relationship predicted by IRGNN is inaccordance with the given pattern such as [ ⇒
sub] on the left, whereas negative cases represent that IRGNN
predicts the opposite .
Results show that IRGNN can discover useful dependencies ofsubstitutes and complements among 3-hop paths. For example,
IRGNN learns that if i
0sub− → i1sub− → i2sub− → i3, then likely
i0sub− → i3, which matches our intuition. As shown in Table IX,
in Musical Instruments, i0,i1,i2,a n d i3are four keyboard
controllers but with different brands, and then i0and i3are
also substitutable. Negative cases suggest that apart from the
edge relational information, IRGNN also considers the item
content information to make predictions.
We further display the multihop dependencies with multiple
relationships learned by IRGNN in Table X. We computethe ratio of the frequency of the given multihop pattern to
all the predictions made by IRGNN. Table X presents thethree most frequent patterns and reveals that IRGNN can
discover multihop patterns such as [ ⇒AB].
In sum, IRGNN is able to preserve useful path dependenciesand thereby improves the performance of item relationship
prediction.
V. C
ONCLUSION AND FUTURE WORK
In this work, we propose a GNN-based framework, IRGNN,
with multihop relationships to discover item multirelation-
ships. Rather than solely relying on the item content infor-
mation, IRGNN automatically learns topological features and
relational dependencies in multihop neighborhoods to improve
the quality of item relationship prediction. The edge rela-tional network is carefully designed to impart both node and
edge features to the message propagation process. Moreover,
we study a more generalized setting—multigraph—for dis-covering item relationships. Extensive experiments on large
real-world item graphs demonstrate the rationality and effec-
tiveness of IRGNN.
This work exploits multihop connections in inferring item
relationships. Multihop connections offer the potential of pre-serving complex relational dependencies that can help identify
item relationships. This is the ﬁrst work that emphasizes on
edge relational features for the item relationship prediction.We hope that it can provide insights on how to better model
the complex item relationships in real-world e-commerce
scenarios.
Authorized licensed use limited to: Texas A M University. Downloaded on April 06,2021 at 16:23:31 UTC from IEEE Xplore.  Restrictions apply. This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.
LIU et al. : IRGNNs FOR E-COMMERCE 13
In practice, item relationships may change dynamically and
evolve over time due to daily transactions, which may not be
handled well by our current model due to the assumption of
static training data –. An online variant of IRGNNon how to efﬁciently update the model can be investigated
in the future. The temporal evolutionary patterns can also be
incorporated to improve the representation quality of items.In addition, as item relationships may vary for individuals,
we plan to personalize the message propagation step and
thereby generate personalized recommendation in our contin-
uing work.
RAuthorized licensed use limited to: Texas A M University. Downloaded on April 06,2021 at 16:23:31 UTC from IEEE Xplore.  Restrictions apply. This paper connects equal opportunity to popularity bias in implicit93Knowledge of a disease includes information
ofvariousaspectsofthedisease,suchassigns
and symptoms, diagnosis and treatment. This
disease knowledge is critical for many health-
related and biomedical tasks, including con-
sumer health question answering, medical lan-
guage inference and disease name recognition.
While pre-trained language models like BERT
have shown success in capturing syntactic, se-
mantic, and world knowledge from text, we
ﬁnd they can be further complemented by spe-
ciﬁcinformationlikeknowledgeofsymptoms,
diagnoses, treatments, and other disease as-
pects. Hence, we integrate BERT with dis-
ease knowledge for improving these important
tasks. Speciﬁcally, we propose a new dis-
ease knowledge infusion training procedure
and evaluate it on a suite of BERT models in-
cluding BERT, BioBERT, SciBERT, Clinical-
BERT,BlueBERT,andALBERT.Experiments
overthethreetasksshowthatthesemodelscan
be enhanced in nearly all cases, demonstrat-
ingtheviabilityofdiseaseknowledgeinfusion.
For example, accuracy of BioBERT on con-
sumer health question answering is improved
from 68.29% to 72.09%, while new SOTA re-
sults are observed in two datasets. We make
our data and code freely available. /one.sup
Humandiseaseis“adisorderofstructureorfunction
in a human that produces speciﬁc signs or symp-
toms” . Dis-
ease is one of the fundamental biological enti-
ties in biomedical research and consequently it is
frequently searched for in the scientiﬁc literature
 and on the internet
.
Knowledge of a disease includes information
about various aspects of the disease, like the signs
/one.sup 1: Disease knowledge of COVID-19 is presented
fromthreeaspects: symptoms,diagnosisandtreatment
(based on Wikipedia).
Disease Aspect Information
COVID-19 symptomsFever is the most common symptom,
but highly variable in severity and
presentation, with some older...
COVID-19 diagnosisThe standard method of testing is
real-time reverse transcription poly-
merase chain reaction (rRT-PCR)...
COVID-19 treatmentPeople are managed with supportive
care, which may include ﬂuid therapy,
oxygen support, and supporting...
and symptoms, diagnosis, and treatment (Saleem
etal.,2012;Urnesetal.,2008;DuJeongetal.,2017).
Asanexample,Table1highlightsseveralaspects
for COVID-19. Specialized disease knowledge
iscriticalformanyhealth-relatedandbiomedical
naturallanguageprocessing(NLP)tasks,including:
•Consumerhealthquestionanswering (Abacha
et al., 2019) - the goal is to rank candidate
passagesforansweringquestionslike“What
isthediagnosisof COVID-19 ?” asshownin
Figure 1a;
•Medical language inference (Romanov and
Shivade, 2018) - the goal is to predict if a
givenhypothesis(descriptionofapatient)can
be inferred from a given premise (another
description of the patient);
•Diseasenamerecognition 
- the goal is to detect disease concepts in text.
For these tasks, it is critical for NLP models
tocapturediseaseknowledge,thatisthesemantic
relations between a disease-descriptive text and its
corresponding aspect and disease:arXiv:2010.03746v1  [cs.CL]  8 Oct 2020•AsshowninFigure1a,ifmodelscanseman-
tically relate “...real-time reverse transcrip-
tion polymerase chain reaction...” (disease-
descriptive text) to the diagnosis (aspect) of
COVID-19 (disease), it is easier for them to
pick up the most relevant answer among the
candidates.
•Likewise, as shown in Figure 1b, if models
knowthatthepremiseisthesymptoms(aspect)
ofAphasia(disease) in the hypothesis, they
can easily predict that it is entailment not
contradiction.
•AnotherexampleisshowninFigure1c,ifmod-
els can semantically relate “CTG expansion’
to the cause (aspect) of Myotonic dystrophy
(disease), it is easier for them to detect this
disease.
Inanutshell,NLPmodelsrequirethediseaseknowl-
edge for these disease-related tasks.
Recently, a new style of knowledge learning and
leveraginghasshakenNLPﬁeldwithdramaticsuc-
cesses, enabled by BERT  and
its variants (Yang et al., 2019; Liu et al., 2019b;
Raﬀel et al., 2019; Lan et al., 2020). These mod-
els capture language and world knowledge (Qiu
et al., 2020; Rogers et al., 2020) in their parame-
tersviaself-supervisedpre-trainingoverlarge-scale
unannotated data and then leverage these knowl-
edgeinfurtherﬁne-tuningoverdownstreamtasks.
Moreover, many biomedical BERT models such as
BioBERTareproposed,whichare
pre-trainedoverbiomedicalcorporaviaamasked
language model (MLM) that predicts randomly
masked tokens given their context. This MLM
strategy is designed to capture the semantic re-
lations between random masked tokens and their
context, but not the disease knowledge. Because
thecorrespondingdiseaseand aspect mightnotbe
randomly masked or might not be mentioned at all
in the disease-descriptive text, the semantic rela-
tions betweenthem cannot beeﬀectively captured
via MLM. Therefore, a new training strategy is
required to capture this disease knowledge.
In this paper, we propose a new disease knowl-
edgeinfusion trainingproceduretoexplicitlyaug-
ment BERT-like models with the disease knowl-
edge. The core idea is to train BERT to infer the
corresponding disease and aspect from a disease-
descriptive text, enabled by weakly-supervised sig-
nals from Wikipedia. Given a passage extracted
Question:…keen to learn how to get COVID-19diagnosed, many thanksAnswer 1: ...real-time reverse transcription polymerase chain reaction...Answer 2: ... diagnosis of vipomarequires demonstration of diarrhea...Answer 3: ...affected by this disorder are not able to make lipoproteins…Label: Answer 1 is themost relevantDiseaseKnowledge:Answer1isthediagnosis ofCOVID-19(a) Consumer Health Question Answering
Premise: She was not able to speak, but appeared to comprehend wellHypothesis: Patient had aphasiaLabel: entailmentDiseaseKnowledge:Premisedescribesthesymptomsofaphasia
(b) Medical Language Inference
Text:Myotonic dystrophy (DM) is caused by a CTG expansion in the 3 untranslated region of the DM gene.Label: Myotonic dystrophy DiseaseKnowledge:thetextcontainsthecauseofMyotonic dystrophy 
(c) Disease Name Recognition
Figure 1: Examples of tasks that can beneﬁt from dis-
ease knowledge.
fromasection (normallydescribesanaspect) ofa
disease’sWikipediaarticle,BERTistrainedtoinfer
thetitleofthecorrespondingsection(aspectname)
and the title of the corresponding article (disease
name). For example, in Table 1, given “...testing
is real-time reverse transcription polymerase chain
reaction (rRT-PCR)...”, BERT is trained to infer
that this passage is from the section “diagnosis" of
the article “COVID-19”. Moreover, because some
passages do not mention the disease and aspect,
we construct auxiliary sentences that contain the
disease and aspect, suchas “What is the diagnosis
ofCOVID-19?"andinsertthissentenceatthebe-
ginning of the corresponding passage. After that,
we mask the disease and aspect in the auxiliary
sentence and then let BERT-like models infer them
giventhepassage. Inthisway,BERTlearnshowto
semanticallyrelateadisease-descriptivetextwith
its corresponding aspect and disease.
To evaluate the quality of disease knowledge in-
fusion,weconductexperimentsonasuiteofBERT
models – including BERT, BlueBERT, Clinical-
BERT,SciBERT,BioBERT,andALBERT–over
consumer health question (CHQ) answering, med-
ical language inference, and disease name recog-
nition. We ﬁnd that (1) these models can be en-
hancedinnearlyallcases. Forexample,accuracy
of BioBERT on CHQ answering is improved from
68.29% to 72.09%; and (2) our method is supe-
rior to MLM for infusing the disease knowledge.
Moreover,newSOTAresultsareobservedintwo
datasets. These results demonstrate the potentialof disease knowledge infusion into pre-trained lan-
guage models like BERT.
2 Related Work
Knowledge-EnrichedBERT: Incorporatingexter-
nal knowledge into BERT has been shown to be
eﬀective. Such external knowledge includes world
(factual) knowledge for tasks such as entity typ-
ingandrelationclassiﬁcation(Zhangetal.,2019;
Peters et al., 2019; Liu et al., 2019a; Xiong et al.,
2019), sentiment knowledge for sentiment analysis
, word sense
knowledge for word sense disambiguation (Levine
et al., 2019), commonsense knowledge for com-
monsense reasoning  and
sarcasmgeneration,le-
gal knowledge for legal element extraction (Zhong
et al., 2020), numerical skills for numerical reason-
ing,andcodingknowledgefor
code generation .
Biomedical BERT: BERT can also be enriched
with biomedical knowledge via pre-training over
biomedicalcorporalikePubMed,asinBioBERT
,
ClinicalBERT  and Blue-
BERT. ThesebiomedicalBERT
modelsreportnewSOTAperformanceonseveral
biomedical tasks. Disease knowledge, of course, is
a subset of biomedical knowledge. However, there
aretwokeydiﬀerencesbetweenthesebiomedical
BERT models and our work: (1) Many biomedical
BERTmodelsarepre-trainedviaBERT’sdefault
MLMthatpredicts15%randomlymaskedtokens.
Incontrast,weproposeanewtrainingtask: disease
knowledge infusion, which infers the disease and
aspect from the corresponding disease-descriptive
text; (2)BiomedicalBERTmodelscapturethegen-
eralsyntacticandsemanticknowledgeofbiomed-
ical language, while our work is speciﬁcally de-
signedforcapturingthesemanticrelationsbetween
a disease-descriptive text and its corresponding as-
pectanddisease. ExperimentsreportedinSection4
show that our proposed method can improve the
performance of each of these biomedical BERT
models, demonstrating the importance of disease
knowledge infusion.
Biomedical Knowledge Integration Methods
withUMLS: Previousnon-BERTmethodsconnect
data of downstream tasks with knowledge bases
like UMLS (Sharma et al., 2019; Romanov and
Shivade, 2018). For example, they map medicalconceptsandsemanticrelationshipsinthedatato
UMLS.Afterthat,theseconceptsandrelationships
areencodedintoembeddingsandincorporatedinto
models . The advantage is
that they can explicitly incorporate knowledge into
models. However, these methods have been out-
performed by biomedical BERT models such as
BioBERT in most cases.
Table 2: Eight aspects of knowledge of a disease that
are considered in this work.
Aspect Name Deﬁnition
Information The general information of a disease.
Causes The causes of a disease.
Symptoms The signs and symptoms of a disease.
Diagnosis How to test and diagnose a disease.
Treatment How to treat and manage a disease.
Prevention How to prevent a disease.
Pathophysiology The physiological processes of a disease.
Transmission The means by which a disease spread.
3 Proposed Method: Disease Knowledge
Infusion Training
Inthissection,weproposeanewtrainingtask: Dis-
easeKnowledgeInfusionTraining. Ourgoalisto
integrateBERT-likepre-trainedlanguagemodels
with disease knowledge to achieve better perfor-
manceonavarietyofmedicaldomaintasksinclud-
ingansweringhealthquestions,medicallanguage
inference, and disease name recognition. Our ap-
proachisguidedbythreequestions: Whichdiseases
andaspectsshouldwefocuson? Howdoweinfuse
diseaseknowledgeintoBERT-likemodels? What
is the objective function of this training task?
3.1 Targeting Diseases and Aspects
First, we seek a disease vocabulary that provides
diseaseterms. SeveralresourcesincludeMedical
Subject Headings /two.sup, the
National Cancer Institute thesaurus (De Coronado
et al., 2004), SNOMED CT , and
Uniﬁed Medical Language System (UMLS) (Bo-
denreider, 2004). Each has a diﬀerent scope and
design purpose, and it is an open question into
which is most appropriate here. As a ﬁrst step,
we select MeSH, which is a comprehensive con-
trolledvocabularyproposedbytheNationalLibrary
of Medicine (NLM) to index journal articles and
booksinthelifesciences,composedof16branches
likeanatomy,organisms, anddiseases. We collect
/two.sup
treenumberC01-C26)andMentalDisorderbranch
(MeSHtreenumberF01),resultingin5,853total
disease terms.
Knowledge of a disease involves information
about various aspects of the disease (Saleem et al.,
2012; Urnes et al., 2008; Du Jeong et al., 2017).
For each aspect, we focus on text alone (excluding
images or other media). Following Abacha and
Demner-Fushman(2019),weconsidereightdisease
aspects as shown in Table 2.
3.2 Weakly Supervised Knowledge Infusion
from Wikipedia
Given the target set of diseases and aspects, the
next challenge is how to infuse knowledge of the
aspects of these diseases into BERT-like models.
We propose to train BERT to infer the correspond-
ing disease and aspect from a disease-descriptive
text. By minimizing the loss between the predicted
disease and aspect and the original disease and
aspect, the model should memorize the semantic
relationsbetweenthedisease-descriptivetextand
its corresponding disease and aspect.
Astraightforwardapproachistomaskandpre-
dictthediseaseandaspectinthedisease-descriptive
text. However, this strategy faces two problems:
(1) Given a passage extracted from disease-related
papers,clinicalnotes,orbiomedicalwebsites,the
ground-truth of its topic 
is diﬃcult to identify. Medical expert annotation
is time-consuming and expensive; while automatic
annotation can suﬀer from large errors. For ex-
ample,weneedtorecognizediseasenamesinthe
passage, which is yet another challenging and still
open problem in biomedical text mining (Doğan
etal.,2014);(2)Diseasesandaspectsmentionedin
a passage are not necessarily the topic words. Mul-
tiplediseasenamesoraspectnamesmightappear,
makingitdiﬃculttodeterminewhichisthecorrect
topic. For example, in Table 1, the symptoms of
COVID-19 alsomentions fever /three.sup,whilethecorrect
topic isCOVID-19 .
Weakly-Supervised Knowledge Source: Instead
ofannotatinganarbitrarydisease-relatedpassage,
weexploitthestructureofWikipediaasaweakly-
supervised signal. In many cases, each disease’s
Wikipediaarticleconsistsofseveralsectionswhere
agnosis). For example, step 2 in Figure 2 shows
/three.supFever is included in the disease branch of MeSH.several aspects onthe Wikipedia pagefor COVID-
19. By extracting the passage from each section,
thetitleofthesectionisthetopic
aspect of the passage and the title of the article is
the topic disease . Speciﬁcally,
we search Wikipedia to obtain the articles for the
5,853 target disease terms from MeSH and apply
regularexpressionstoextractthetextofthesections
correspondingtotheappropriateaspects. Intotal,
we collect a disease knowledge resource consisting
of 14,617 passages. /four.supIn fact, there are other online
resources /five.supwiththesimilarstructure. Asaﬁrststep,
we start with Wikipedia.
AuxiliarySentencesforDiseaseandAspectPre-
diction:Thesecondproblemisthattheextracted
passagesdonotnecessarilymentionthecorrespond-
ingdiseaseandtheaspect. Forexample,inTable
1,thediseasename“COVID-19”doesnotappear
intheinformationofitssymptoms. Inthedisease
knowledge resource, we ﬁnd that only 51.4% of
passages mention both the corresponding diseases
and aspects. Hence, we cannot simply mask-and-
predictthe diseaseand aspectbecause thepassage
does not mention them at all.
A remedy for this problem is an auxiliary sen-
tencethatcontainsthecorrespondingdiseaseand
aspect for each passage. We use a template of
questionstyle: “Whatisthe[ Aspect]of[Disease]?”
to automatically generate auxiliary sentences as
shown in step 5 in Figure 2. Some examples are
showninTable3. Theadvantageofthisquestion
style template is that the cloze statement of the
auxiliary sentences for all aspects (except for the
“information” aspect) are the same (What is the
[MASK]of[MASK]?). Hence,theauxiliarysen-
tences provide no clues  for predicting
the corresponding aspect.
Table 3: Examples of auxiliary sentences
Aspect Name Auxiliary Sentence
Diagnosis What is the diagnosis of COVID-19?
Treatment What is the treatment of COVID-19?
Prevention What is the prevention of COVID-19?
Transmission What is the transmission of COVID-19?
Cloze Statement What is the [MASK] of [MASK]?
After that, we replace the corresponding disease
and aspect with the special token [MASK] in the
/four.supNote that each disease article does not necessarily have
all eight target aspects.
/five.sup WHO has published several testing protocols for the disease. The standard method of testing is real-time reverse transcription polymerase chain reaction (rRT-PCR)...
NewPassageforMLM:Whatisthe[MASK]of[MASK]?The WHO has published several testing protocols for the disease. The standard method of testing is real-time reverse transcription polymerase chain reaction (rRT-PCR)...
Auxiliary Sentence: WhatisthediagnosisofCOVID-19?5.Constructanauxiliary sentencethatmentionsthesubjectdiseaseandaspect.3.Extracttextfromasectionasthepassage.
6.Concatenatethepassageandtheauxiliary Sentence.BERTistrainedtoinferthediseaseandaspect.1.ObtaindiseasetermsfromMeSH
2.ObtainArticlesofdiseasesfromWikipediaDisease:COVID-19(titleoftheWikipediaarticle)4.Extracttheweakly-supervisedtopicdiseaseandaspectforthepassage.Aspect:Diagnosis(titleofthesection)Figure 2: Disease Knowledge Infusion Training: An example with COVID-19.
auxiliary sentences. Then, we insert the auxil-
iary sentence at the beginning of its corresponding
passagetoformanewpassagewithaquestion-and-
answer style as shown in Figure 2, where BERT is
trained to predict the original tokens of the masked
disease and aspect.
3.3 Training Objective and Details
Finally,weshowtheobjectivefunctionofdisease
infusiontraining. Sincemostdiseasenamesareout
ofBERTvocabulary,theWordPiecetokenizer(Wu
et al., 2016) will split these terms into sub-word
tokens that exist in the vocabulary. For example,
“COVID-19"willbesplitinto4tokens: “co",“vid",
“-" and “19". Formally, let 𝑋=¹𝑥1  𝑥 𝑇ºdenote
a sequence of 𝑇tokens that are split from a disease
namewhere 𝑥𝑡isthe 𝑡-thtoken. Theoriginalcross-
entropy loss is to get the conditional probability of
a masked token as close as possible to the 1-hot
vector of the token:
L𝑑𝑖𝑠𝑒𝑎𝑠𝑒 = 𝑇∑︁
𝑡=1𝑙𝑜𝑔 𝑝¹𝑥𝑡j𝑝𝑎𝑠𝑠𝑎𝑔𝑒º(1)
where 𝑝¹𝑥𝑡j𝑐𝑜𝑛𝑡𝑒𝑥𝑡ºis a conditional probability
over𝑥𝑡giventhecorrespondingpassage,whichcan
be deﬁned as:
𝑝¹𝑥𝑡j𝑝𝑎𝑠𝑠𝑎𝑔𝑒º=𝑒𝑥𝑝¹𝑧𝑡ºÍ
𝑧2V𝑒𝑥𝑝¹𝑧º(2)
whereVis the vocabulary and 𝑧𝑡is the unnor-
malized log probability of 𝑥𝑡. Lety𝑡denote the
embedding of token 𝑥𝑡from the output layer of
BERT. We can estimate 𝑧𝑡via:
𝑧𝑡=wy𝑡¸𝑏 (3)
wheretheweight wandbias 𝑏arelearnablevectors.
NotethatthevocabularysizeofBERTisaround
30,000 which means masked language modeling
task is a 30,000 multi-class problem. The logits(like 𝑧𝑡)afterthenormalizationofsoftmax(Equa-
tion2)willbeprettysmall(theexpectationofmean
should be around 1/30,000=3.3*e-5), which might
cause some obstacles for the learning. Therefore,
we also maximize the raw logits (like 𝑧𝑡) before
softmaxnormalizationwhichmightkeepmoreuse-
ful information. Empirically, we add the reciprocal
of the logits to the cross-entropy loss:
L𝑑𝑖𝑠𝑒𝑎𝑠𝑒 = 𝑇∑︁
𝑡=1𝑙𝑜𝑔𝑝¹𝑥𝑡j𝑝𝑎𝑠𝑠𝑎𝑔𝑒º¸𝛽
Í𝑇
𝑡=1𝑧𝑡
(4)
where 𝛽balances the two parts of the loss. The
ﬁnalobjectivefunctioniscombinedwiththeloss
ofthediseaseandaspect: L=L𝑑𝑖𝑠𝑒𝑎𝑠𝑒¸L𝑎𝑠𝑝𝑒𝑐𝑡
whereL𝑎𝑠 𝑝𝑒𝑐𝑡 = 𝑙𝑜𝑔 𝑝¹𝑎j𝑝𝑎𝑠𝑠𝑎𝑔𝑒ºand𝑎isthe
token of the aspect name. By minimizing this loss
function, BERT can update its parameters to store
the disease knowledge.
4 Experiments
In this section, we examine disease knowledge
infusion into six BERT variants over three disease-
relatedtasks: healthquestionanswering,medical
language inference, and disease name recognition.
Reproducibility: Thecodeanddatainthispaper
is released. /six.supA modelis ﬁrstly initializedwith the
pre-trainedparametersfromBERToritsvariants
and then is further trained by disease knowledge
infusion to capture the disease knowledge. We use
a widely used Pytorch implementation /seven.supof BERT
and Adam as the optimizer. We empirically set
learning rate as 1e-5, batch size as 16 and 𝛽as
10. BecauseMeSHischosen
as the disease vocabulary in our experiments, as
a smaller vocabulary compared with others like
/six.sup
/seven.sup
transformersUMLS , we obtain a rel-
atively small dataset of 14,617 passages. Hence,
the training of disease knowledge infusion is as
fastasﬁne-tuningBERToverdownstreamdatasets,
which takes 2-4 epochs to enhance BERT for a
better performance on downstream tasks, which
will be discussed in Section 4.5. The training is
performed on one single NVIDIA V100 GPU and
ittakesabout10minutestocompleteonetraining
epoch using BERT-base architecture. The repro-
ducibility for ﬁne-tuning over downstream tasks
will be detailed in Section 4.2.
4.1 BERT and its Biomedical Variants
WeconsidersixBERTmodels: twopre-trainedover
general language corpora (BERT and ALBERT)
and four pre-trained over biomedical corpora (Clin-
ical BERT, BioBERT, BlueBERT and SciBERT).
BERTisamulti-layerbidirec-
tional Transformer encoder. Since the following
biomedicalversionsofBERTareoftenbasedonthe
BERT-base architecture (12 layers and 768 hidden
embeddingsizewith108Mparameters),wechoose
BERT-base here for fair comparison.
ALBERT /eight.sup compresses the ar-
chitecture of BERT by factorized embedding pa-
rameterization and cross-layer parameter sharing.
Via this compression, ALBERT can have a sub-
stantiallyhighercapacitythanBERT,withstronger
performanceonmanytasks. Wechoosethemaxi-
mumversionALBERT-xxlarge(12layersand4096
hidden embedding size with 235M parameters).
BioBERT /nine.sup is the ﬁrst BERT
pre-trained onbiomedical corpora. Itis initialized
with BERT’s pre-trained parameters (108M) and
then further trained over PubMed abstracts (4.5B
words)andPubMedCentralfull-textarticles(13.5B
words). We choose the best version BioBERT v1.1.
ClinicalBERT /one.sup/zero.supisaBERT
model initialized from BioBERT v1.0 (Lee et al.,
2020) and further pre-trained over approximately
2 millionnotes in the MIMIC-IIIv1.4 database of
patientnotes. Weadoptthe
bestperformingversionofClinicalBERT(108Mpa-
rameters)basedondischargesummariesofclinical
notes: Bio-Discharge Summary BERT.
BlueBERT /one.sup/one.sup is ﬁrstly initial-
ized from BERT (108M parameters) and further
/eight.sup
/nine.sup
/one.sup/zero.sup
/one.sup/one.sup 4: Summary of Tasks and Datasets.
Datasets Train Dev Test
MEDIQA-2019 208 ¹1701º125 
TRECQA-2017 254 
MEDNLI 1123221,395 1,422
BC5CDR-disease 418234,244 4,424
NCBI 5,145 787 960
1, Questions with associated answers; 2, Pairs of premise and
hypothesis; 3, Disease name mentions.
pre-trained over a biomedical corpus of PubMed
abstracts and clinical notes .
SciBERT /one.sup/two.supisaBERT-base
(108M parameters) model pre-trained on a random
sample of the full text of 1.14M papers from Se-
mantic Scholar , with 18% of
papersfromthecomputersciencedomainand82%
from the biomedical domain.
4.2 Tasks
We test disease knowledge infusion over three
biomedical NLP tasks. The dataset statistics are in
Table4. Forﬁne-tuningofBERTanditsvariants,
thebatchsizeisselectedfrom[16,32]andlearning
rate is selected from [1e-5, 2e-5, 3e-5, 4e-5, 5e-5].
Task1: ConsumerHealthQuestionAnswering.
The objective of this task is to rank candidate
answers for consumer health questions.
Datasets. We consider two datasets: MEDIQA-
2019andTRECQA-2017
. /one.sup/three.supMEDIQA-2019 is based
on questions submitted to the consumer health
QA system CHiQA /one.sup/four.sup. TRECQA-2017 is based
onquestionssubmittedtotheNationalLibraryof
Medicine. Medical experts manually re-ranked thearXiv:2004.12158 .We present a new benchmark dataset called
PARADE for paraphrase identiﬁcation that re-
quires specialized domain knowledge. PA-
RADE contains paraphrases that overlap very
little at the lexical and syntactic level but
are semantically equivalent based on computer
science domain knowledge, as well as non-
paraphrases that overlap greatly at the lexical
and syntactic level but are not semantically
equivalent based on this domain knowledge.
Experiments show that both state-of-the-art
neural models and non-expert human annota-
tors have poor performance on PARADE. For
example, BERT after ﬁne-tuning achieves an
F1 score of 0.709, which is much lower than its
performance on other paraphrase identiﬁcation
datasets. PARADE can serve as a resource for
researchers interested in testing models that in-
corporate domain knowledge. We make our
data and code freely available.1
Paraphrases are sentences that express the same (or
similar) meaning by using different wording (Bha-
gat and Hovy, 2013). Automatically identifying
paraphrases and non-paraphrases has proven useful
for a wide range of natural language processing
(NLP) applications, including question answering,
semantic parsing, information extraction, machine
translation, textual entailment, and semantic textual
similarity.
Paraphrase identiﬁcation (PI) is typically formal-
ized as a binary classiﬁcation problem: given two
sentences, determine if they roughly express the
same meaning. Traditional paraphrase identiﬁca-
tion approaches (Mihalcea et al., 2006; Kozareva
and Montoyo, 2006; Wan et al., 2006; Das and
Smith, 2009; Xu et al., 2014) mainly rely on lex-
ical and syntactic overlap features to measure the
1
datasetsemantic similarity between the two sentences. Ex-
amples include string-based features (e.g., whether
two sentences share the same words), part-of-
speech features (e.g., whether shared words have
the same POS tags), and dependency-based fea-
tures (e.g., whether two sentences have similar de-
pendency trees).
s1: the lowest level of code made up of 0s and 1s.
s2: binary instructions used by the cpu.
Label: paraphrase (both describe “Machine Code”)
s3: a graph representation that uses a 2d array such that if
arr[i][j] == 1, there is an edge between vertices i and j
s4: a matrix which records the number of direct links between
vertices
Label: paraphrase (both describe “Adjacency Matrix”)
s5: how the optimal solution toa linear programming
problem changes as the problem data are modiﬁed.
s6: how changes in the coefﬁcients ofa linear programming
problem affect the optimal solution
Label: non-paraphrase
Table 1: Examples of paraphrases and non-paraphrases
from the computer science domain. Judgments are
made based on domain knowledge rather than lexical
or syntactic features. Overlapping words (other than
stop-words) are in bold and key different words are un-
derlined.
However, these shallow lexical and syntactic
overlap features may not effectively capture the
domain-speciﬁc semantics of the two sentences.
A typical situation where models based on these
overlap features may fail is a pair of sentences that
overlap very little at the lexical and syntactic level
but are semantically equivalent based on domain
knowledge . Consider the two paraphrases s1 and
s2 in Table 1. Both describe machine code though
they have very little overlap. In order to correctly
identify paraphrases like this pair, it is necessary to
have specialized domain knowledge that a proces-
sor (CPU) can only understand binary instructions
made up of 0s and 1s. On the other hand, a pairarXiv:2010.03725v1  [cs.CL]  8 Oct 2020of sentences that overlap greatly at the lexical and
syntactic level but are not semantically equivalent
based on domain knowledge can also confuse both
non-expert annotators and NLP models. Consider
the non-paraphrase of s5 and s6 in Table 1 as an
example. Sentence s5 is about a sensitivity analysis
between the problem data and the optimal solution
while s6 is about a sensitivity analysis between
the coefﬁcients and the optimal solution; these two
cases are fundamentally different, requiring spe-
cialized domain knowledge of linear programming
to distinguish the two. These examples highlight
the importance of specialized domain knowledge
for identifying paraphrases and non-paraphrases
correctly.
Recent neural models (Nie and Bansal, 2017;
Parikh et al., 2016; Chen et al., 2017) that go be-
yond traditional approaches based on lexical and
syntactic features have demonstrated state-of-the-
art performance on paraphrase identiﬁcation. For
example, BERT and its variants (Devlin et al., 2018;
Liu et al., 2019; Yang et al., 2019; Lan et al., 2019;
Raffel et al., 2019) have achieved the best results
on the General Language Understanding Evalua-
tion  on two
paraphrase identiﬁcation datasets: the Microsoft
Research Paraphrase Corpus (MRPC) and Quora
Question Pairs (QQP). Using massive pre-training
data and a ﬂexible bidirectional self-attention mech-
anism, BERT and its variants are able to better
model the semantic relationship between sentences.
Moreover, two recent studies (Petroni et al., 2019;
Davison et al., 2019) observe that BERT without
ﬁne-tuning can even capture world knowledge and
can answer factual questions like “place of birth”
and “who developed the theory of relativity.” Natu-
rally, we are curious to know if these neural mod-
els can correctly identify paraphrases that require
specialized domain knowledge like the examples
shown in Table 1.
Hence, our overarching research goal is to cre-
ate new datasets and enable new models for high-
quality paraphrase identiﬁcation based on domain
knowledge . Because previous paraphrase datasets
(Dolan and Brockett, 2005; Dolan et al., 2004; Xu
et al., 2014; Lan et al., 2017; Iyer et al., 2017;
Zhang et al., 2019) were not originally designed
and constructed from the perspective of domain
knowledge, to date there is no such dataset that re-
quires specialized domain knowledge to discern
the quality of two candidate sentences as para-phrases. As a ﬁrst step, we focus in this paper
on the computer science domain. Speciﬁcally, we
require a dataset of paraphrases that overlap very
little but are semantically equivalent, and of non-
paraphrases that have overlap greatly but are not
semantically equivalent based on computer science
domain knowledge. Correspondingly, there is a re-
search gap in understanding if modern neural mod-
els can achieve exemplary performance on such
a dataset, especially in comparison with existing
paraphrase identiﬁcation datasets (that lack such
specialized domain knowledge). In sum, this paper
makes four contributions:
•First, we propose a novel extensible frame-
work for inexpensively collecting domain-
speciﬁc sentential candidate paraphrases that
are characterized by specialized knowledge.
The key idea is to leverage large-scale online
collections of user-generated ﬂashcards . We
treat deﬁnitions on each ﬂashcard’s back side
that correspond to a common entity on the
front side  as candidate
paraphrases.
•Due to the noise in user-generated ﬂashcards
and heterogeneity in the aspects in the can-
didate paraphrases, our second contribution
is a reﬁnement strategy coupled with annota-
tion by domain experts to create a new gold
dataset called PARADE ( PARA phrase iden-
tiﬁcation based on Domain knowledg E). PA-
RADE contains 4,778 (46.9%) paraphrases
and 5,404 (53.1%) non-paraphrases that de-
scribe 788 distinct entities from the computer
science domain and is the ﬁrst publicly avail-
able benchmark for paraphrase identiﬁcation
based on domain knowledge.
•Third, we evaluate the quality of state-of-the-
art paraphrase identiﬁcation models on PA-
RADE and existing paraphrase identiﬁcation
datasets like MRPC and QQP. We ﬁnd that
both state-of-the-art neural models (which
have shown strong performance on existing
PI datasets) and non-expert human annotators
have poor performance on PARADE. For ex-
ample, BERT after ﬁne-tuning only achieves
0.709 in terms of F1 on PARADE compared
to 0.893 on MRPC and 0.877 on QQP. Such a
gap indicates the need for new models that can
better exploit specialized domain knowledge.•Finally, we show that incorporating external
domain knowledge into the training of models
like BERT offers the potential for improve-
ments on PARADE. Concretely, we ﬁnd that
SciBERT – a BERT variant pre-trained on a
corpus of computer science papers – improves
the accuracy from 0.729 to 0.741. This im-
provement is encouraging, and suggests the
need for further enhancements in incorporat-
ing domain knowledge into NLP models.
2 Related Work
Framework for Collecting Paraphrases: The ba-
sic idea of collecting a paraphrase dataset is to
connect parallel data that are related to the same27.Modern recommenders usually consider both collaborative fea-
tures from user behavior data  and content information
about the users and items  for im-
proved recommendations. While encouraging, the uncovered user52This paper focuses on how to generate unbiased recommendations
based on biased implicit user-item interactions. We propose a com-
binational joint learning framework to simultaneously learn unbi-
ased user-item relevance and unbiased propensity. More specifically,
we first present a new unbiased objective function for estimating
propensity. We then show how a naïve joint learning approach
faces an estimation-training overlap problem. Hence, we propose
to jointly train multiple sub-models from different parts of the
training dataset to avoid this problem. Finally, we show how to
incorporate residual components trained by the complete train-
ing data to complement the relevance and propensity sub-models.
Extensive experiments on two public datasets demonstrate the ef-
fectiveness of the proposed model with an improvement of 4%on
average over the best alternatives.
CCS CONCEPTS
•Information systems →Recommender systems .556There is an increasing attention on next-item recommendation sys-1110Online reviews play a critical role in persuading or dissuading users
when making purchase decisions. And yet very few users take the
time to write helpful reviews. Encouragingly, recent advances in
deep neural networks offer good potential to produce review-like
natural language content. However, there is a lack of large, high-
quality labeled data at both the aspect and sentiment level for
training. Hence, toward enabling a writing assistant framework
to help users post online reviews, this paper proposes a scalable
labeling method for bootstrapping aspect and sentiment labels.
Concretely, the proposed approach – Aspect Dependent Online RE-
views (ADORE) – leverages the underlying distribution of reviews
and a small seed set of labeled data through carefully designed
review segmentation and label assignment. We then show how
these labels can inform a generative model to produce aspect and
sentiment-aware reviews. We study the effectiveness of ADORE
under various scenarios such as how end-users perceive the quality
of the labels and aspect-aware generated reviews. Our experiments
indicate that the proposed effective labeling process along with
a regularized joint generative model lead to high quality reviews
with∼90% accuracy.
CCS CONCEPTS
•Computing methodologies →Natural language processing.1030Recommendation algorithms typically build models based on user-
item interactions  to provide a person-
alized ranked list of items. These interactions are often distributed
unevenly over different groups of items due to varying user pref-
erences. However, we show that recommendation algorithms can
inherit or even amplify this imbalanced distribution, leading to item
under-recommendation bias. Concretely, we formalize the concepts
of ranking-based statistical parity and equal opportunity as two
measures of item under-recommendation bias. Then, we empirically
show that one of the most widely adopted algorithms – Bayesian
Personalized Ranking – produces biased recommendations, which
motivates our effort to propose the novel debiased personalized
ranking model. The debiased model is able to improve the two pro-
posed bias metrics while preserving recommendation performance.
Experiments on three public datasets show strong bias reduction
of the proposed model versus state-of-the-art alternatives.
CCS CONCEPTS
•Information systems →Recommender systems.
KEYWORDS
recommender systems; statistical parity; equal opportunity; recom-
mendation bias458The cold start problem is a long-standing challenge in recommender
systems. That is, how to recommend for new users and new items
without any historical interaction record? Recent ML-based ap-
proaches have made promising strides versus traditional methods.
These ML approaches typically combine both user-item interac-
tion data of existing warm start users and items (as in CF-based
methods) with auxiliary information of users and items such as
user profiles and item content information (as in content-based
methods). However, such approaches face key drawbacks including
the error superimposition issue that the auxiliary-to-CF transforma-
tion error increases the final recommendation error; the ineffective
learning issue that long distance from transformation functions
to model output layer leads to ineffective model learning; and the
unified transformation issue that applying the same transformation
function for different users and items results in poor transformation.
Hence, this paper proposes a novel model designed to overcome
these drawbacks while delivering strong cold start performance.
Three unique features are: (i) a combined separate-training and
joint-training framework to overcome the error superimposition
issue and improve model quality; (ii) a Randomized Training mech-
anism to promote the effectiveness of model learning; and (iii) a
Mixture-of-Experts Transformation mechanism to provide ‘person-
alized’ transformation functions. Extensive experiments on three
datasets show the effectiveness of the proposed model over state-
of-the-art alternatives.
CCS CONCEPTS
•Information systems →Recommender systems.
KEYWORDS
collaborative filtering; cold start recommendation; randomized
training; mixture-of-experts
∗Part of this work performed while interning at Comcast Applied AI Research Lab.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’20, July 25–30, 2020, Virtual Event, China
©2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-8016-4/20/07. . . $15.00

Figure 1: (a) setup of cold start recommendation problem,
where both warm and cold users and items have auxiliary
representations (such as user profiles and item content); and
(b) the main idea of existing cold start recommendation algo-
rithms [7, 17, 28, 35, 36]: learn transformation functions to
transform auxiliary representations to CF representations.1130We propose an adaptive hierarchical translation-based sequential
recommendation called HierTrans that first extends traditional
item-level relations to the category-level, to help capture dynamic
sequence patterns that can generalize across users and time . Then
unlike item-level based methods, we build a novel hierarchical tem-
poral graph that contains item multi-relations at the category-level
and user dynamic sequences at the item-level. Based on the graph,
HierTrans adaptively aggregates the high-order multi-relationson Recommender Systems . 77–85.descriptions of cars, but both have physical
characteristics that are associated with them as we discussed in
Section 3.
5.2 TF-IDF
So far we have compiled the most relevant terms in from the reviews.
We now need to weight these terms for each review, so that we know
the car-speak terms are most associated with a car. Using TF-IDF
2(Term Frequency-Inverse Document Frequency) has been used as a
reliable metric for finding the relevant terms in a document .
We represent each review as a vector of TF-IDF scores for each
word in the review. The length of this vector is 10 ,867. We label
each review vector with the car it reviews. We ignore the year of the
car being reviewed and focus specifically on the model (i.e Acura
ILX, not 2013 Acura ILX). This is because there a single model of
car generally retains the same characteristics over time [9, 14].
5.3 Classification Experiments
We train a series of classifiers in order to classify car-speak. We
train three classifiers on the review vectors that we prepared in
Section 5.2. The classifiers we use are K Nearest Neighbors (KNN),
Random Forest , and Multi-
layer Perceptron (MLP) .
Table 2: Evaluation metrics for all classifiers.
KNN RF SVM MLP
Precision Macro 0.6133 0.5968 0.6080 0.6094
Recall Macro 0.6086 0.5947 0.605 0.6059
F1 Macro 0.5808 0.5733 0.5801 0.5795
F1 Micro 0.6762 0.6687 0.6712 0.6778
In order to evaluate our classifiers, we perform 4-fold cross val-
idation on a shuffled data set. Table 2 shows the F1 micro and F1
macro scores for all the classifiers. The KNN classifier seem to per-
form the best across all four metrics. This is probably due to the
multi-class nature of the data set.
6 CONCLUSION & FUTURE WORK
car-speak and a way to automate car dealers at dealerships. We first
provide a definition of “car-speak” in Section 3. We explore what
constitutes car-speak and how to identify car-speak.
We also gather a data set of car-speak to use for exploration and
training purposes. This data set id full of vehicle reviews from U.S.
News . These reviews provide a reasonable set of car-speak
data that we can study.
Finally, we create and test several classifiers that are trained
on the data we gathered. While these classifiers did not perform
particularly well, they provide a good starting point for future work
on this subject.
In the future we plan to use more complex models to attempt
to understand car-speak. We also would like to test our classifiers
on user-provided natural language queries. This would be a more
practical evaluation of our classification. It would also satisfy the
need for a computer system that understands car-speak.3User-generated item lists are popular on many platforms. Examples
include video-based playlists on YouTube, image-based lists (or
“boards”) on Pinterest, book-based lists on Goodreads, and answer-
based lists on question-answer forums like Zhihu. As users create
these lists, a common challenge is in identifying what items to
curate next. Some lists are organized around particular genres or
topics, while others are seemingly incoherent, reflecting individ-258The importance of the distribution of ratings on recommender sys-
tems (RS) is well-recognized. And yet, recommendation approaches
ants  optimize for the head of these distributions, poten-
tially leading to large estimation errors for tail ratings. These errors
in tail ratings that are far from the mean predicted rating fall out
of a uni-modal assumption underlying these popular models, as
we show in this paper. We propose to improve the estimation of
tail ratings by extending traditional single latent representations
 with new
multi-latent representations for better modeling these tail ratings.
We show how to incorporate these multi-latent representations in
an end-to-end neural prediction model that is designed to better
reflect the underlying ratings distributions of items. Through ex-
periments over six datasets, we find the proposed model leads to
a significant improvement in RMSE versus a suite of benchmark
methods. We also find that the predictions for the most polarized
items are improved by more than 15%.
CCS CONCEPTS
•Information systems →Recommender systems.
KEYWORDS
Recommendation system; Latent representation; Polarization; Rat-
ing distribution770We propose a personalized user recommendation framework for635Recommendation systems typically rely on the interactions be-
tween a crowd of ordinary users and items, ignoring the fact that
many real-world communities are notably influenced by a small
group of key opinion leaders, whose feedback on items wields
outsize influence. With important positions in the community (e.g.
have a large number of followers), their elite opinions are able to dif-
fuse to the community and further impact what items we buy, what
media we consume, and how we interact with online platforms.
Hence, this paper investigates how to develop a novel recommenda-
tion system by explicitly capturing the influence from key opinion
leaders to the whole community. Centering around opinion elicita-
tion and diffusion, we propose an end-to-end Graph-based neural
model - GoRec. Specifically, to preserve the multi-relations between
key opinion leaders and items, GoRec elicits the opinions from key
opinion leaders with a translation-based embedding method. More-
over, GoRec adopts the idea of Graph Neural Networks to model
the elite opinion diffusion process for improved recommendation.
Through experiments on Goodreads and Epinions, the proposed
model outperforms state-of-the-art approaches by 10.75% and 9.28%
on average in Top-K item recommendation.
KEYWORDS
Recommendation; Key Opinion Leaders; Graph Neural Networks644Currently, most sequence-based recommendation models aim to
predict a user’s next actions (e.g. next purchase) based on their pastWeb Search and Data Mining . ACM, 582–590. We propose a framework for personalized music curator rec-
ommendation to connect users with curators who have matching curation
style. Three unique features of the proposed framework are: (i) models
of curation style to capture the coverage of music and curator’s individ-
ual style in assigning tracks to playlists; (ii) a curation-based embedding
approach to capture inter-track agreement, beyond the audio features,
resulting in models of music tracks that pair well together; and (iii) a
novel neural pairwise ranking model for personalized music curator rec-ommendation that naturally incorporates both curator style models and
track embeddings. Experiments over a Spotify dataset show signiﬁcant
improvements in precision, recall, and F1 versus state-of-the-art.
Music streaming platforms provide access to a diverse, incredibly large, and ever
growing collection of music tracks. To make sense of the millions of availabletracks , playlists
have become an essential feature of many music streaming platforms for orga-
nizing music, mediating how users experience the service. Across platforms,most playlists are manually curated and managed by a group of music curators ,
which consists of both “regular” users and expert tastemakers. To beneﬁt from
the power of human curation , many platforms enable users to follow these
music curators to receive updates of their listening activities, e.g., to discover
new tracks, albums, or playlists (as illustrated in Fig. 1(a)).
While recommendation systems have been widely deployed in many music
streaming platforms for tasks like recommending individual music tracks
[3,19,23] or playlists [ 2,14], they are not well-suited for real-world scenarios
like (i) discovering new tracks with little or no feedback; (ii) ﬁnding relevant
playlists that are frequently updated (and hence, out-of-sync with respect to a
learned recommendation model); and (iii) recommending playlist creators them-selves who can provide direct access to new tracks, albums, or playlists. As a
step toward supporting these scenarios, we focus on the task of curator recom-
mendation to create a personalization layer to help users discover vast amounts
of new tracks, fresh playlists, and interesting curators.
While some services highlighting highly-rated or popular curators [ 7,16,22]
,
c⃝Springer Nature Switzerland AG 2020
J. M. Jose et al. (Eds.): ECIR 2020, LNCS 12035, pp. 191–204, 2020.
_13192 J. Wang and J. Caverlee
Fig. 1. (a) Users follow music curators with matching curation style to receive updates.
(b) We randomly sample curators with IDs containing the keywords (“workout”,
“sport”, “run”, “ﬁt”, “gym”, “country”, “piano”, “instrument”, “classic”, “jazz”). Weshow the 2D visualization (with t-SNE ) of selected curators based on average audio
features of music tracks they curate.
identifying personally-relevant music curators is a daunting task due to the fol-
lowing challenges. First, music curators themselves are complex amalgamationsof the playlists they create, the tracks they select, and their unique style.F o r
example, some curators may focus on speciﬁc emotions (like happy or excited),
eras . Hence the
ﬁrst challenge is: How can we build models that capture these stylistic diﬀerences
across music curators taking both their curating coverage and individual styleinto consideration? To illustrate, we conduct an initial exploration of Spotify
curators whose areas of interest can be inferred from their user IDs. We repre-
sent each of the music curators using the average of audio features (see Sect. 3.2
for details) for all the tracks in playlists they curate and plot the 2D t-SNE dis-
tribution in Fig. 1(b). We see that there are clear patterns of curator coverage :
country music curators cluster together in the top-right, curators focusing on
active music for sports and workouts cluster in the top-left, while classical and
instrumental music curators dominate the lower portion. We see that curatorsSteering Committee (2015)